{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification on the Hypnogram Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lightning.pytorch as pl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = Path.cwd().parent\n",
    "input_path = project_path / 'data' / 'hypnogram' / 'hypnogram_input.csv'\n",
    "output_path = project_path / 'data' / 'hypnogram' / 'hypnogram_output.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hypnogram(Dataset):\n",
    "\n",
    "    def __init__(self, input_path: Path, output_path: Path, train=True, scale=False):\n",
    "        x = np.loadtxt(input_path, dtype=int, delimiter=',')\n",
    "        y = np.loadtxt(output_path, dtype=int, delimiter=',')\n",
    "        if scale:\n",
    "            x = (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "        split_index = int(x.shape[0] * 0.8)\n",
    "        if train:\n",
    "            self.x = x[:split_index, :]\n",
    "            self.y = y[:split_index]\n",
    "        else:\n",
    "            self.x = x[split_index:, :]\n",
    "            self.y = y[split_index:]\n",
    "        assert self.x.shape[0] == self.y.shape[0]\n",
    "        self.x = torch.tensor(self.x, dtype=torch.float)\n",
    "        self.y = torch.tensor(self.y, dtype=torch.float)\n",
    "        self.y = torch.unsqueeze(self.y, dim=1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def print_info(self):\n",
    "        print(f\"{self.x.shape = }\")\n",
    "        print(f\"{self.y.shape = }\")\n",
    "        print(f\"{torch.max(self.x) = }\")\n",
    "        print(f\"{torch.min(self.x) = }\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchLinearModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(TorchLinearModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_size, output_size)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        y_pred = self.sigmoid(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "epoch = 500 \n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, save_weight_path = None):\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    best_test_acc = 0\n",
    "\n",
    "    for e in range(epoch):\n",
    "        print(f\"--------- epoch: {e+1} ---------\")\n",
    "        # training\n",
    "        train_loss = 0.0\n",
    "        corrects = 0\n",
    "        total_examples = 0\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()  # zero the gradients\n",
    "            # prepare data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).to(torch.float32)\n",
    "            # the forward pass\n",
    "            y_pred = model(x)\n",
    "            y_pred = y_pred.reshape(y.shape)\n",
    "            # the backward pass\n",
    "            loss = criterion(y_pred, y)  # calculate the loss\n",
    "            loss.backward()  # get the gradients\n",
    "            optimizer.step()  # update the params based on the gradients\n",
    "            # collect training results\n",
    "            train_loss += loss.item()\n",
    "            corrects += torch.sum((y_pred.round() == y))\n",
    "            total_examples += len(y)\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        train_accuracies.append(corrects / total_examples)\n",
    "        print(f\"num_corrects / total_examples = {corrects.item()} / {total_examples}\")\n",
    "        print(f\"training loss = {train_losses[-1]:.4f}\")\n",
    "        print(f\"training accuracy = {train_accuracies[-1]:.4f}\")\n",
    "        # print(total_examples)\n",
    "\n",
    "        # testing\n",
    "        test_corrects = 0\n",
    "        test_total_examples = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(test_loader):\n",
    "                # prepare data\n",
    "                x = x.to(device)\n",
    "                y = y.to(device).to(torch.float32)\n",
    "                # the forward pass\n",
    "                y_pred = model(x)\n",
    "                y_pred = y_pred.reshape(y.shape)\n",
    "                # collect testing results\n",
    "                test_corrects += torch.sum((y_pred.round() == y))\n",
    "                test_total_examples += len(y)\n",
    "\n",
    "        test_acc = test_corrects.item() / test_total_examples\n",
    "        test_accuracies.append(test_acc)\n",
    "        print(f\"num_test_corrects / test_total_examples = {test_corrects.item()} / {test_total_examples}\")\n",
    "        print(f\"testing accuracy = {test_accuracies[-1]:.4f}\")\n",
    "        if save_weight_path is not None:\n",
    "            if test_acc > best_test_acc:\n",
    "                # save the model\n",
    "                print(f\"found best test accuracy at epoch {e+1}\")\n",
    "                print(f\"save the model checkpoint to {save_weight_path}\")\n",
    "                torch.save({\n",
    "                    'epoch': e+1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'test_acc': test_acc,\n",
    "                }, save_weight_path)\n",
    "                best_test_acc = test_acc\n",
    "\n",
    "    return train_losses, train_accuracies, test_accuracies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Pytorch - Integer Data (No Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the simple nn on CPU is much faster\n",
    "lr_model = TorchLinearModel(300, 1)\n",
    "lr_model.to(device)\n",
    "lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.x.shape = torch.Size([36865, 300])\n",
      "self.y.shape = torch.Size([36865, 1])\n",
      "torch.max(self.x) = tensor(31.)\n",
      "torch.min(self.x) = tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Hypnogram(input_path=input_path, \n",
    "                          output_path=output_path, \n",
    "                          train=True,\n",
    "                          scale=False)\n",
    "train_dataset.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.x.shape = torch.Size([9217, 300])\n",
      "self.y.shape = torch.Size([9217, 1])\n",
      "torch.max(self.x) = tensor(31.)\n",
      "torch.min(self.x) = tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = Hypnogram(input_path=input_path, output_path=output_path, train=False)\n",
    "test_dataset.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcf0fc01ee0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_dataset.x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- epoch: 1 ---------\n",
      "num_corrects / total_examples = 22724 / 36865\n",
      "training loss = 0.9803\n",
      "training accuracy = 0.6164\n",
      "num_test_corrects / test_total_examples = 6432 / 9217\n",
      "testing accuracy = 0.6978\n",
      "--------- epoch: 2 ---------\n",
      "num_corrects / total_examples = 22878 / 36865\n",
      "training loss = 0.9448\n",
      "training accuracy = 0.6206\n",
      "num_test_corrects / test_total_examples = 6435 / 9217\n",
      "testing accuracy = 0.6982\n",
      "--------- epoch: 3 ---------\n",
      "num_corrects / total_examples = 23060 / 36865\n",
      "training loss = 0.9273\n",
      "training accuracy = 0.6255\n",
      "num_test_corrects / test_total_examples = 6438 / 9217\n",
      "testing accuracy = 0.6985\n",
      "--------- epoch: 4 ---------\n",
      "num_corrects / total_examples = 23190 / 36865\n",
      "training loss = 0.9140\n",
      "training accuracy = 0.6291\n",
      "num_test_corrects / test_total_examples = 6443 / 9217\n",
      "testing accuracy = 0.6990\n",
      "--------- epoch: 5 ---------\n",
      "num_corrects / total_examples = 23374 / 36865\n",
      "training loss = 0.9042\n",
      "training accuracy = 0.6340\n",
      "num_test_corrects / test_total_examples = 6443 / 9217\n",
      "testing accuracy = 0.6990\n",
      "--------- epoch: 6 ---------\n",
      "num_corrects / total_examples = 23483 / 36865\n",
      "training loss = 0.8959\n",
      "training accuracy = 0.6370\n",
      "num_test_corrects / test_total_examples = 6445 / 9217\n",
      "testing accuracy = 0.6993\n",
      "--------- epoch: 7 ---------\n",
      "num_corrects / total_examples = 23573 / 36865\n",
      "training loss = 0.8889\n",
      "training accuracy = 0.6394\n",
      "num_test_corrects / test_total_examples = 6453 / 9217\n",
      "testing accuracy = 0.7001\n",
      "--------- epoch: 8 ---------\n",
      "num_corrects / total_examples = 23678 / 36865\n",
      "training loss = 0.8828\n",
      "training accuracy = 0.6423\n",
      "num_test_corrects / test_total_examples = 6458 / 9217\n",
      "testing accuracy = 0.7007\n",
      "--------- epoch: 9 ---------\n",
      "num_corrects / total_examples = 23747 / 36865\n",
      "training loss = 0.8777\n",
      "training accuracy = 0.6442\n",
      "num_test_corrects / test_total_examples = 6461 / 9217\n",
      "testing accuracy = 0.7010\n",
      "--------- epoch: 10 ---------\n",
      "num_corrects / total_examples = 23839 / 36865\n",
      "training loss = 0.8721\n",
      "training accuracy = 0.6467\n",
      "num_test_corrects / test_total_examples = 6464 / 9217\n",
      "testing accuracy = 0.7013\n",
      "--------- epoch: 11 ---------\n",
      "num_corrects / total_examples = 23894 / 36865\n",
      "training loss = 0.8668\n",
      "training accuracy = 0.6481\n",
      "num_test_corrects / test_total_examples = 6468 / 9217\n",
      "testing accuracy = 0.7017\n",
      "--------- epoch: 12 ---------\n",
      "num_corrects / total_examples = 23985 / 36865\n",
      "training loss = 0.8624\n",
      "training accuracy = 0.6506\n",
      "num_test_corrects / test_total_examples = 6473 / 9217\n",
      "testing accuracy = 0.7023\n",
      "--------- epoch: 13 ---------\n",
      "num_corrects / total_examples = 24045 / 36865\n",
      "training loss = 0.8587\n",
      "training accuracy = 0.6522\n",
      "num_test_corrects / test_total_examples = 6475 / 9217\n",
      "testing accuracy = 0.7025\n",
      "--------- epoch: 14 ---------\n",
      "num_corrects / total_examples = 24126 / 36865\n",
      "training loss = 0.8551\n",
      "training accuracy = 0.6544\n",
      "num_test_corrects / test_total_examples = 6481 / 9217\n",
      "testing accuracy = 0.7032\n",
      "--------- epoch: 15 ---------\n",
      "num_corrects / total_examples = 24172 / 36865\n",
      "training loss = 0.8522\n",
      "training accuracy = 0.6557\n",
      "num_test_corrects / test_total_examples = 6490 / 9217\n",
      "testing accuracy = 0.7041\n",
      "--------- epoch: 16 ---------\n",
      "num_corrects / total_examples = 24219 / 36865\n",
      "training loss = 0.8498\n",
      "training accuracy = 0.6570\n",
      "num_test_corrects / test_total_examples = 6497 / 9217\n",
      "testing accuracy = 0.7049\n",
      "--------- epoch: 17 ---------\n",
      "num_corrects / total_examples = 24266 / 36865\n",
      "training loss = 0.8476\n",
      "training accuracy = 0.6582\n",
      "num_test_corrects / test_total_examples = 6508 / 9217\n",
      "testing accuracy = 0.7061\n",
      "--------- epoch: 18 ---------\n",
      "num_corrects / total_examples = 24317 / 36865\n",
      "training loss = 0.8456\n",
      "training accuracy = 0.6596\n",
      "num_test_corrects / test_total_examples = 6512 / 9217\n",
      "testing accuracy = 0.7065\n",
      "--------- epoch: 19 ---------\n",
      "num_corrects / total_examples = 24356 / 36865\n",
      "training loss = 0.8438\n",
      "training accuracy = 0.6607\n",
      "num_test_corrects / test_total_examples = 6517 / 9217\n",
      "testing accuracy = 0.7071\n",
      "--------- epoch: 20 ---------\n",
      "num_corrects / total_examples = 24374 / 36865\n",
      "training loss = 0.8420\n",
      "training accuracy = 0.6612\n",
      "num_test_corrects / test_total_examples = 6523 / 9217\n",
      "testing accuracy = 0.7077\n",
      "--------- epoch: 21 ---------\n",
      "num_corrects / total_examples = 24412 / 36865\n",
      "training loss = 0.8405\n",
      "training accuracy = 0.6622\n",
      "num_test_corrects / test_total_examples = 6525 / 9217\n",
      "testing accuracy = 0.7079\n",
      "--------- epoch: 22 ---------\n",
      "num_corrects / total_examples = 24470 / 36865\n",
      "training loss = 0.8391\n",
      "training accuracy = 0.6638\n",
      "num_test_corrects / test_total_examples = 6532 / 9217\n",
      "testing accuracy = 0.7087\n",
      "--------- epoch: 23 ---------\n",
      "num_corrects / total_examples = 24485 / 36865\n",
      "training loss = 0.8378\n",
      "training accuracy = 0.6642\n",
      "num_test_corrects / test_total_examples = 6534 / 9217\n",
      "testing accuracy = 0.7089\n",
      "--------- epoch: 24 ---------\n",
      "num_corrects / total_examples = 24520 / 36865\n",
      "training loss = 0.8367\n",
      "training accuracy = 0.6651\n",
      "num_test_corrects / test_total_examples = 6537 / 9217\n",
      "testing accuracy = 0.7092\n",
      "--------- epoch: 25 ---------\n",
      "num_corrects / total_examples = 24561 / 36865\n",
      "training loss = 0.8357\n",
      "training accuracy = 0.6662\n",
      "num_test_corrects / test_total_examples = 6541 / 9217\n",
      "testing accuracy = 0.7097\n",
      "--------- epoch: 26 ---------\n",
      "num_corrects / total_examples = 24584 / 36865\n",
      "training loss = 0.8348\n",
      "training accuracy = 0.6669\n",
      "num_test_corrects / test_total_examples = 6544 / 9217\n",
      "testing accuracy = 0.7100\n",
      "--------- epoch: 27 ---------\n",
      "num_corrects / total_examples = 24620 / 36865\n",
      "training loss = 0.8340\n",
      "training accuracy = 0.6678\n",
      "num_test_corrects / test_total_examples = 6553 / 9217\n",
      "testing accuracy = 0.7110\n",
      "--------- epoch: 28 ---------\n",
      "num_corrects / total_examples = 24646 / 36865\n",
      "training loss = 0.8333\n",
      "training accuracy = 0.6685\n",
      "num_test_corrects / test_total_examples = 6560 / 9217\n",
      "testing accuracy = 0.7117\n",
      "--------- epoch: 29 ---------\n",
      "num_corrects / total_examples = 24659 / 36865\n",
      "training loss = 0.8327\n",
      "training accuracy = 0.6689\n",
      "num_test_corrects / test_total_examples = 6564 / 9217\n",
      "testing accuracy = 0.7122\n",
      "--------- epoch: 30 ---------\n",
      "num_corrects / total_examples = 24678 / 36865\n",
      "training loss = 0.8321\n",
      "training accuracy = 0.6694\n",
      "num_test_corrects / test_total_examples = 6562 / 9217\n",
      "testing accuracy = 0.7119\n",
      "--------- epoch: 31 ---------\n",
      "num_corrects / total_examples = 24706 / 36865\n",
      "training loss = 0.8315\n",
      "training accuracy = 0.6702\n",
      "num_test_corrects / test_total_examples = 6569 / 9217\n",
      "testing accuracy = 0.7127\n",
      "--------- epoch: 32 ---------\n",
      "num_corrects / total_examples = 24732 / 36865\n",
      "training loss = 0.8310\n",
      "training accuracy = 0.6709\n",
      "num_test_corrects / test_total_examples = 6573 / 9217\n",
      "testing accuracy = 0.7131\n",
      "--------- epoch: 33 ---------\n",
      "num_corrects / total_examples = 24758 / 36865\n",
      "training loss = 0.8305\n",
      "training accuracy = 0.6716\n",
      "num_test_corrects / test_total_examples = 6577 / 9217\n",
      "testing accuracy = 0.7136\n",
      "--------- epoch: 34 ---------\n",
      "num_corrects / total_examples = 24778 / 36865\n",
      "training loss = 0.8301\n",
      "training accuracy = 0.6721\n",
      "num_test_corrects / test_total_examples = 6578 / 9217\n",
      "testing accuracy = 0.7137\n",
      "--------- epoch: 35 ---------\n",
      "num_corrects / total_examples = 24803 / 36865\n",
      "training loss = 0.8297\n",
      "training accuracy = 0.6728\n",
      "num_test_corrects / test_total_examples = 6583 / 9217\n",
      "testing accuracy = 0.7142\n",
      "--------- epoch: 36 ---------\n",
      "num_corrects / total_examples = 24817 / 36865\n",
      "training loss = 0.8293\n",
      "training accuracy = 0.6732\n",
      "num_test_corrects / test_total_examples = 6585 / 9217\n",
      "testing accuracy = 0.7144\n",
      "--------- epoch: 37 ---------\n",
      "num_corrects / total_examples = 24842 / 36865\n",
      "training loss = 0.8290\n",
      "training accuracy = 0.6739\n",
      "num_test_corrects / test_total_examples = 6587 / 9217\n",
      "testing accuracy = 0.7147\n",
      "--------- epoch: 38 ---------\n",
      "num_corrects / total_examples = 24839 / 36865\n",
      "training loss = 0.8287\n",
      "training accuracy = 0.6738\n",
      "num_test_corrects / test_total_examples = 6588 / 9217\n",
      "testing accuracy = 0.7148\n",
      "--------- epoch: 39 ---------\n",
      "num_corrects / total_examples = 24843 / 36865\n",
      "training loss = 0.8284\n",
      "training accuracy = 0.6739\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 40 ---------\n",
      "num_corrects / total_examples = 24844 / 36865\n",
      "training loss = 0.8281\n",
      "training accuracy = 0.6739\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 41 ---------\n",
      "num_corrects / total_examples = 24848 / 36865\n",
      "training loss = 0.8279\n",
      "training accuracy = 0.6740\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 42 ---------\n",
      "num_corrects / total_examples = 24880 / 36865\n",
      "training loss = 0.8277\n",
      "training accuracy = 0.6749\n",
      "num_test_corrects / test_total_examples = 6597 / 9217\n",
      "testing accuracy = 0.7157\n",
      "--------- epoch: 43 ---------\n",
      "num_corrects / total_examples = 24888 / 36865\n",
      "training loss = 0.8276\n",
      "training accuracy = 0.6751\n",
      "num_test_corrects / test_total_examples = 6599 / 9217\n",
      "testing accuracy = 0.7160\n",
      "--------- epoch: 44 ---------\n",
      "num_corrects / total_examples = 24916 / 36865\n",
      "training loss = 0.8275\n",
      "training accuracy = 0.6759\n",
      "num_test_corrects / test_total_examples = 6602 / 9217\n",
      "testing accuracy = 0.7163\n",
      "--------- epoch: 45 ---------\n",
      "num_corrects / total_examples = 24941 / 36865\n",
      "training loss = 0.8274\n",
      "training accuracy = 0.6765\n",
      "num_test_corrects / test_total_examples = 6603 / 9217\n",
      "testing accuracy = 0.7164\n",
      "--------- epoch: 46 ---------\n",
      "num_corrects / total_examples = 24958 / 36865\n",
      "training loss = 0.8273\n",
      "training accuracy = 0.6770\n",
      "num_test_corrects / test_total_examples = 6605 / 9217\n",
      "testing accuracy = 0.7166\n",
      "--------- epoch: 47 ---------\n",
      "num_corrects / total_examples = 24984 / 36865\n",
      "training loss = 0.8273\n",
      "training accuracy = 0.6777\n",
      "num_test_corrects / test_total_examples = 6605 / 9217\n",
      "testing accuracy = 0.7166\n",
      "--------- epoch: 48 ---------\n",
      "num_corrects / total_examples = 24998 / 36865\n",
      "training loss = 0.8273\n",
      "training accuracy = 0.6781\n",
      "num_test_corrects / test_total_examples = 6608 / 9217\n",
      "testing accuracy = 0.7169\n",
      "--------- epoch: 49 ---------\n",
      "num_corrects / total_examples = 25020 / 36865\n",
      "training loss = 0.8273\n",
      "training accuracy = 0.6787\n",
      "num_test_corrects / test_total_examples = 6608 / 9217\n",
      "testing accuracy = 0.7169\n",
      "--------- epoch: 50 ---------\n",
      "num_corrects / total_examples = 25023 / 36865\n",
      "training loss = 0.8273\n",
      "training accuracy = 0.6788\n",
      "num_test_corrects / test_total_examples = 6608 / 9217\n",
      "testing accuracy = 0.7169\n",
      "--------- epoch: 51 ---------\n",
      "num_corrects / total_examples = 25045 / 36865\n",
      "training loss = 0.8274\n",
      "training accuracy = 0.6794\n",
      "num_test_corrects / test_total_examples = 6609 / 9217\n",
      "testing accuracy = 0.7170\n",
      "--------- epoch: 52 ---------\n",
      "num_corrects / total_examples = 25057 / 36865\n",
      "training loss = 0.8275\n",
      "training accuracy = 0.6797\n",
      "num_test_corrects / test_total_examples = 6609 / 9217\n",
      "testing accuracy = 0.7170\n",
      "--------- epoch: 53 ---------\n",
      "num_corrects / total_examples = 25056 / 36865\n",
      "training loss = 0.8277\n",
      "training accuracy = 0.6797\n",
      "num_test_corrects / test_total_examples = 6609 / 9217\n",
      "testing accuracy = 0.7170\n",
      "--------- epoch: 54 ---------\n",
      "num_corrects / total_examples = 25076 / 36865\n",
      "training loss = 0.8278\n",
      "training accuracy = 0.6802\n",
      "num_test_corrects / test_total_examples = 6610 / 9217\n",
      "testing accuracy = 0.7172\n",
      "--------- epoch: 55 ---------\n",
      "num_corrects / total_examples = 25073 / 36865\n",
      "training loss = 0.8303\n",
      "training accuracy = 0.6801\n",
      "num_test_corrects / test_total_examples = 6611 / 9217\n",
      "testing accuracy = 0.7173\n",
      "--------- epoch: 56 ---------\n",
      "num_corrects / total_examples = 25093 / 36865\n",
      "training loss = 0.8305\n",
      "training accuracy = 0.6807\n",
      "num_test_corrects / test_total_examples = 6612 / 9217\n",
      "testing accuracy = 0.7174\n",
      "--------- epoch: 57 ---------\n",
      "num_corrects / total_examples = 25105 / 36865\n",
      "training loss = 0.8308\n",
      "training accuracy = 0.6810\n",
      "num_test_corrects / test_total_examples = 6613 / 9217\n",
      "testing accuracy = 0.7175\n",
      "--------- epoch: 58 ---------\n",
      "num_corrects / total_examples = 25110 / 36865\n",
      "training loss = 0.8312\n",
      "training accuracy = 0.6811\n",
      "num_test_corrects / test_total_examples = 6614 / 9217\n",
      "testing accuracy = 0.7176\n",
      "--------- epoch: 59 ---------\n",
      "num_corrects / total_examples = 25146 / 36865\n",
      "training loss = 0.8318\n",
      "training accuracy = 0.6821\n",
      "num_test_corrects / test_total_examples = 6609 / 9217\n",
      "testing accuracy = 0.7170\n",
      "--------- epoch: 60 ---------\n",
      "num_corrects / total_examples = 25150 / 36865\n",
      "training loss = 0.8328\n",
      "training accuracy = 0.6822\n",
      "num_test_corrects / test_total_examples = 6602 / 9217\n",
      "testing accuracy = 0.7163\n",
      "--------- epoch: 61 ---------\n",
      "num_corrects / total_examples = 25150 / 36865\n",
      "training loss = 0.8331\n",
      "training accuracy = 0.6822\n",
      "num_test_corrects / test_total_examples = 6602 / 9217\n",
      "testing accuracy = 0.7163\n",
      "--------- epoch: 62 ---------\n",
      "num_corrects / total_examples = 25154 / 36865\n",
      "training loss = 0.8334\n",
      "training accuracy = 0.6823\n",
      "num_test_corrects / test_total_examples = 6602 / 9217\n",
      "testing accuracy = 0.7163\n",
      "--------- epoch: 63 ---------\n",
      "num_corrects / total_examples = 25153 / 36865\n",
      "training loss = 0.8337\n",
      "training accuracy = 0.6823\n",
      "num_test_corrects / test_total_examples = 6601 / 9217\n",
      "testing accuracy = 0.7162\n",
      "--------- epoch: 64 ---------\n",
      "num_corrects / total_examples = 25160 / 36865\n",
      "training loss = 0.8340\n",
      "training accuracy = 0.6825\n",
      "num_test_corrects / test_total_examples = 6600 / 9217\n",
      "testing accuracy = 0.7161\n",
      "--------- epoch: 65 ---------\n",
      "num_corrects / total_examples = 25164 / 36865\n",
      "training loss = 0.8344\n",
      "training accuracy = 0.6826\n",
      "num_test_corrects / test_total_examples = 6600 / 9217\n",
      "testing accuracy = 0.7161\n",
      "--------- epoch: 66 ---------\n",
      "num_corrects / total_examples = 25164 / 36865\n",
      "training loss = 0.8347\n",
      "training accuracy = 0.6826\n",
      "num_test_corrects / test_total_examples = 6600 / 9217\n",
      "testing accuracy = 0.7161\n",
      "--------- epoch: 67 ---------\n",
      "num_corrects / total_examples = 25176 / 36865\n",
      "training loss = 0.8351\n",
      "training accuracy = 0.6829\n",
      "num_test_corrects / test_total_examples = 6600 / 9217\n",
      "testing accuracy = 0.7161\n",
      "--------- epoch: 68 ---------\n",
      "num_corrects / total_examples = 25197 / 36865\n",
      "training loss = 0.8354\n",
      "training accuracy = 0.6835\n",
      "num_test_corrects / test_total_examples = 6598 / 9217\n",
      "testing accuracy = 0.7159\n",
      "--------- epoch: 69 ---------\n",
      "num_corrects / total_examples = 25209 / 36865\n",
      "training loss = 0.8358\n",
      "training accuracy = 0.6838\n",
      "num_test_corrects / test_total_examples = 6601 / 9217\n",
      "testing accuracy = 0.7162\n",
      "--------- epoch: 70 ---------\n",
      "num_corrects / total_examples = 25218 / 36865\n",
      "training loss = 0.8361\n",
      "training accuracy = 0.6841\n",
      "num_test_corrects / test_total_examples = 6599 / 9217\n",
      "testing accuracy = 0.7160\n",
      "--------- epoch: 71 ---------\n",
      "num_corrects / total_examples = 25218 / 36865\n",
      "training loss = 0.8364\n",
      "training accuracy = 0.6841\n",
      "num_test_corrects / test_total_examples = 6598 / 9217\n",
      "testing accuracy = 0.7159\n",
      "--------- epoch: 72 ---------\n",
      "num_corrects / total_examples = 25226 / 36865\n",
      "training loss = 0.8367\n",
      "training accuracy = 0.6843\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 73 ---------\n",
      "num_corrects / total_examples = 25232 / 36865\n",
      "training loss = 0.8370\n",
      "training accuracy = 0.6844\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 74 ---------\n",
      "num_corrects / total_examples = 25236 / 36865\n",
      "training loss = 0.8373\n",
      "training accuracy = 0.6846\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 75 ---------\n",
      "num_corrects / total_examples = 25241 / 36865\n",
      "training loss = 0.8376\n",
      "training accuracy = 0.6847\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 76 ---------\n",
      "num_corrects / total_examples = 25238 / 36865\n",
      "training loss = 0.8380\n",
      "training accuracy = 0.6846\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 77 ---------\n",
      "num_corrects / total_examples = 25241 / 36865\n",
      "training loss = 0.8384\n",
      "training accuracy = 0.6847\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 78 ---------\n",
      "num_corrects / total_examples = 25231 / 36865\n",
      "training loss = 0.8389\n",
      "training accuracy = 0.6844\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 79 ---------\n",
      "num_corrects / total_examples = 25248 / 36865\n",
      "training loss = 0.8393\n",
      "training accuracy = 0.6849\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 80 ---------\n",
      "num_corrects / total_examples = 25249 / 36865\n",
      "training loss = 0.8398\n",
      "training accuracy = 0.6849\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 81 ---------\n",
      "num_corrects / total_examples = 25247 / 36865\n",
      "training loss = 0.8402\n",
      "training accuracy = 0.6849\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 82 ---------\n",
      "num_corrects / total_examples = 25260 / 36865\n",
      "training loss = 0.8430\n",
      "training accuracy = 0.6852\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 83 ---------\n",
      "num_corrects / total_examples = 25257 / 36865\n",
      "training loss = 0.8435\n",
      "training accuracy = 0.6851\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 84 ---------\n",
      "num_corrects / total_examples = 25259 / 36865\n",
      "training loss = 0.8440\n",
      "training accuracy = 0.6852\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 85 ---------\n",
      "num_corrects / total_examples = 25261 / 36865\n",
      "training loss = 0.8444\n",
      "training accuracy = 0.6852\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 86 ---------\n",
      "num_corrects / total_examples = 25255 / 36865\n",
      "training loss = 0.8449\n",
      "training accuracy = 0.6851\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 87 ---------\n",
      "num_corrects / total_examples = 25266 / 36865\n",
      "training loss = 0.8477\n",
      "training accuracy = 0.6854\n",
      "num_test_corrects / test_total_examples = 6597 / 9217\n",
      "testing accuracy = 0.7157\n",
      "--------- epoch: 88 ---------\n",
      "num_corrects / total_examples = 25266 / 36865\n",
      "training loss = 0.8484\n",
      "training accuracy = 0.6854\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 89 ---------\n",
      "num_corrects / total_examples = 25266 / 36865\n",
      "training loss = 0.8490\n",
      "training accuracy = 0.6854\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 90 ---------\n",
      "num_corrects / total_examples = 25269 / 36865\n",
      "training loss = 0.8496\n",
      "training accuracy = 0.6854\n",
      "num_test_corrects / test_total_examples = 6597 / 9217\n",
      "testing accuracy = 0.7157\n",
      "--------- epoch: 91 ---------\n",
      "num_corrects / total_examples = 25265 / 36865\n",
      "training loss = 0.8502\n",
      "training accuracy = 0.6853\n",
      "num_test_corrects / test_total_examples = 6599 / 9217\n",
      "testing accuracy = 0.7160\n",
      "--------- epoch: 92 ---------\n",
      "num_corrects / total_examples = 25276 / 36865\n",
      "training loss = 0.8507\n",
      "training accuracy = 0.6856\n",
      "num_test_corrects / test_total_examples = 6598 / 9217\n",
      "testing accuracy = 0.7159\n",
      "--------- epoch: 93 ---------\n",
      "num_corrects / total_examples = 25274 / 36865\n",
      "training loss = 0.8512\n",
      "training accuracy = 0.6856\n",
      "num_test_corrects / test_total_examples = 6597 / 9217\n",
      "testing accuracy = 0.7157\n",
      "--------- epoch: 94 ---------\n",
      "num_corrects / total_examples = 25279 / 36865\n",
      "training loss = 0.8516\n",
      "training accuracy = 0.6857\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 95 ---------\n",
      "num_corrects / total_examples = 25280 / 36865\n",
      "training loss = 0.8521\n",
      "training accuracy = 0.6857\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 96 ---------\n",
      "num_corrects / total_examples = 25265 / 36865\n",
      "training loss = 0.8524\n",
      "training accuracy = 0.6853\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 97 ---------\n",
      "num_corrects / total_examples = 25273 / 36865\n",
      "training loss = 0.8527\n",
      "training accuracy = 0.6856\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 98 ---------\n",
      "num_corrects / total_examples = 25279 / 36865\n",
      "training loss = 0.8530\n",
      "training accuracy = 0.6857\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 99 ---------\n",
      "num_corrects / total_examples = 25295 / 36865\n",
      "training loss = 0.8533\n",
      "training accuracy = 0.6862\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 100 ---------\n",
      "num_corrects / total_examples = 25301 / 36865\n",
      "training loss = 0.8535\n",
      "training accuracy = 0.6863\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 101 ---------\n",
      "num_corrects / total_examples = 25303 / 36865\n",
      "training loss = 0.8537\n",
      "training accuracy = 0.6864\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 102 ---------\n",
      "num_corrects / total_examples = 25301 / 36865\n",
      "training loss = 0.8539\n",
      "training accuracy = 0.6863\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 103 ---------\n",
      "num_corrects / total_examples = 25301 / 36865\n",
      "training loss = 0.8564\n",
      "training accuracy = 0.6863\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 104 ---------\n",
      "num_corrects / total_examples = 25303 / 36865\n",
      "training loss = 0.8566\n",
      "training accuracy = 0.6864\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 105 ---------\n",
      "num_corrects / total_examples = 25308 / 36865\n",
      "training loss = 0.8567\n",
      "training accuracy = 0.6865\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 106 ---------\n",
      "num_corrects / total_examples = 25309 / 36865\n",
      "training loss = 0.8569\n",
      "training accuracy = 0.6865\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 107 ---------\n",
      "num_corrects / total_examples = 25316 / 36865\n",
      "training loss = 0.8571\n",
      "training accuracy = 0.6867\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 108 ---------\n",
      "num_corrects / total_examples = 25328 / 36865\n",
      "training loss = 0.8572\n",
      "training accuracy = 0.6870\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 109 ---------\n",
      "num_corrects / total_examples = 25333 / 36865\n",
      "training loss = 0.8574\n",
      "training accuracy = 0.6872\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 110 ---------\n",
      "num_corrects / total_examples = 25333 / 36865\n",
      "training loss = 0.8576\n",
      "training accuracy = 0.6872\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 111 ---------\n",
      "num_corrects / total_examples = 25336 / 36865\n",
      "training loss = 0.8578\n",
      "training accuracy = 0.6873\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 112 ---------\n",
      "num_corrects / total_examples = 25333 / 36865\n",
      "training loss = 0.8579\n",
      "training accuracy = 0.6872\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 113 ---------\n",
      "num_corrects / total_examples = 25340 / 36865\n",
      "training loss = 0.8581\n",
      "training accuracy = 0.6874\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 114 ---------\n",
      "num_corrects / total_examples = 25342 / 36865\n",
      "training loss = 0.8582\n",
      "training accuracy = 0.6874\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 115 ---------\n",
      "num_corrects / total_examples = 25350 / 36865\n",
      "training loss = 0.8584\n",
      "training accuracy = 0.6876\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 116 ---------\n",
      "num_corrects / total_examples = 25353 / 36865\n",
      "training loss = 0.8585\n",
      "training accuracy = 0.6877\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 117 ---------\n",
      "num_corrects / total_examples = 25353 / 36865\n",
      "training loss = 0.8586\n",
      "training accuracy = 0.6877\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 118 ---------\n",
      "num_corrects / total_examples = 25353 / 36865\n",
      "training loss = 0.8588\n",
      "training accuracy = 0.6877\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 119 ---------\n",
      "num_corrects / total_examples = 25362 / 36865\n",
      "training loss = 0.8589\n",
      "training accuracy = 0.6880\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 120 ---------\n",
      "num_corrects / total_examples = 25360 / 36865\n",
      "training loss = 0.8590\n",
      "training accuracy = 0.6879\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 121 ---------\n",
      "num_corrects / total_examples = 25365 / 36865\n",
      "training loss = 0.8592\n",
      "training accuracy = 0.6881\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 122 ---------\n",
      "num_corrects / total_examples = 25372 / 36865\n",
      "training loss = 0.8593\n",
      "training accuracy = 0.6882\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 123 ---------\n",
      "num_corrects / total_examples = 25375 / 36865\n",
      "training loss = 0.8594\n",
      "training accuracy = 0.6883\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 124 ---------\n",
      "num_corrects / total_examples = 25379 / 36865\n",
      "training loss = 0.8596\n",
      "training accuracy = 0.6884\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 125 ---------\n",
      "num_corrects / total_examples = 25384 / 36865\n",
      "training loss = 0.8597\n",
      "training accuracy = 0.6886\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 126 ---------\n",
      "num_corrects / total_examples = 25380 / 36865\n",
      "training loss = 0.8599\n",
      "training accuracy = 0.6885\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 127 ---------\n",
      "num_corrects / total_examples = 25388 / 36865\n",
      "training loss = 0.8600\n",
      "training accuracy = 0.6887\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 128 ---------\n",
      "num_corrects / total_examples = 25395 / 36865\n",
      "training loss = 0.8602\n",
      "training accuracy = 0.6889\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 129 ---------\n",
      "num_corrects / total_examples = 25399 / 36865\n",
      "training loss = 0.8603\n",
      "training accuracy = 0.6890\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 130 ---------\n",
      "num_corrects / total_examples = 25405 / 36865\n",
      "training loss = 0.8604\n",
      "training accuracy = 0.6891\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 131 ---------\n",
      "num_corrects / total_examples = 25408 / 36865\n",
      "training loss = 0.8605\n",
      "training accuracy = 0.6892\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 132 ---------\n",
      "num_corrects / total_examples = 25406 / 36865\n",
      "training loss = 0.8606\n",
      "training accuracy = 0.6892\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 133 ---------\n",
      "num_corrects / total_examples = 25411 / 36865\n",
      "training loss = 0.8607\n",
      "training accuracy = 0.6893\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 134 ---------\n",
      "num_corrects / total_examples = 25415 / 36865\n",
      "training loss = 0.8608\n",
      "training accuracy = 0.6894\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 135 ---------\n",
      "num_corrects / total_examples = 25413 / 36865\n",
      "training loss = 0.8609\n",
      "training accuracy = 0.6894\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 136 ---------\n",
      "num_corrects / total_examples = 25417 / 36865\n",
      "training loss = 0.8610\n",
      "training accuracy = 0.6895\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 137 ---------\n",
      "num_corrects / total_examples = 25419 / 36865\n",
      "training loss = 0.8611\n",
      "training accuracy = 0.6895\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 138 ---------\n",
      "num_corrects / total_examples = 25422 / 36865\n",
      "training loss = 0.8612\n",
      "training accuracy = 0.6896\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 139 ---------\n",
      "num_corrects / total_examples = 25422 / 36865\n",
      "training loss = 0.8613\n",
      "training accuracy = 0.6896\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 140 ---------\n",
      "num_corrects / total_examples = 25428 / 36865\n",
      "training loss = 0.8614\n",
      "training accuracy = 0.6898\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 141 ---------\n",
      "num_corrects / total_examples = 25424 / 36865\n",
      "training loss = 0.8615\n",
      "training accuracy = 0.6897\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 142 ---------\n",
      "num_corrects / total_examples = 25422 / 36865\n",
      "training loss = 0.8616\n",
      "training accuracy = 0.6896\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 143 ---------\n",
      "num_corrects / total_examples = 25428 / 36865\n",
      "training loss = 0.8617\n",
      "training accuracy = 0.6898\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 144 ---------\n",
      "num_corrects / total_examples = 25432 / 36865\n",
      "training loss = 0.8618\n",
      "training accuracy = 0.6899\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 145 ---------\n",
      "num_corrects / total_examples = 25436 / 36865\n",
      "training loss = 0.8619\n",
      "training accuracy = 0.6900\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 146 ---------\n",
      "num_corrects / total_examples = 25434 / 36865\n",
      "training loss = 0.8619\n",
      "training accuracy = 0.6899\n",
      "num_test_corrects / test_total_examples = 6591 / 9217\n",
      "testing accuracy = 0.7151\n",
      "--------- epoch: 147 ---------\n",
      "num_corrects / total_examples = 25431 / 36865\n",
      "training loss = 0.8620\n",
      "training accuracy = 0.6898\n",
      "num_test_corrects / test_total_examples = 6591 / 9217\n",
      "testing accuracy = 0.7151\n",
      "--------- epoch: 148 ---------\n",
      "num_corrects / total_examples = 25427 / 36865\n",
      "training loss = 0.8621\n",
      "training accuracy = 0.6897\n",
      "num_test_corrects / test_total_examples = 6591 / 9217\n",
      "testing accuracy = 0.7151\n",
      "--------- epoch: 149 ---------\n",
      "num_corrects / total_examples = 25433 / 36865\n",
      "training loss = 0.8645\n",
      "training accuracy = 0.6899\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 150 ---------\n",
      "num_corrects / total_examples = 25431 / 36865\n",
      "training loss = 0.8646\n",
      "training accuracy = 0.6898\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 151 ---------\n",
      "num_corrects / total_examples = 25432 / 36865\n",
      "training loss = 0.8647\n",
      "training accuracy = 0.6899\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 152 ---------\n",
      "num_corrects / total_examples = 25438 / 36865\n",
      "training loss = 0.8670\n",
      "training accuracy = 0.6900\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 153 ---------\n",
      "num_corrects / total_examples = 25440 / 36865\n",
      "training loss = 0.8671\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6591 / 9217\n",
      "testing accuracy = 0.7151\n",
      "--------- epoch: 154 ---------\n",
      "num_corrects / total_examples = 25442 / 36865\n",
      "training loss = 0.8672\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6591 / 9217\n",
      "testing accuracy = 0.7151\n",
      "--------- epoch: 155 ---------\n",
      "num_corrects / total_examples = 25438 / 36865\n",
      "training loss = 0.8673\n",
      "training accuracy = 0.6900\n",
      "num_test_corrects / test_total_examples = 6591 / 9217\n",
      "testing accuracy = 0.7151\n",
      "--------- epoch: 156 ---------\n",
      "num_corrects / total_examples = 25438 / 36865\n",
      "training loss = 0.8673\n",
      "training accuracy = 0.6900\n",
      "num_test_corrects / test_total_examples = 6591 / 9217\n",
      "testing accuracy = 0.7151\n",
      "--------- epoch: 157 ---------\n",
      "num_corrects / total_examples = 25440 / 36865\n",
      "training loss = 0.8674\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6591 / 9217\n",
      "testing accuracy = 0.7151\n",
      "--------- epoch: 158 ---------\n",
      "num_corrects / total_examples = 25438 / 36865\n",
      "training loss = 0.8675\n",
      "training accuracy = 0.6900\n",
      "num_test_corrects / test_total_examples = 6591 / 9217\n",
      "testing accuracy = 0.7151\n",
      "--------- epoch: 159 ---------\n",
      "num_corrects / total_examples = 25441 / 36865\n",
      "training loss = 0.8676\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6591 / 9217\n",
      "testing accuracy = 0.7151\n",
      "--------- epoch: 160 ---------\n",
      "num_corrects / total_examples = 25441 / 36865\n",
      "training loss = 0.8676\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 161 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.8677\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 162 ---------\n",
      "num_corrects / total_examples = 25444 / 36865\n",
      "training loss = 0.8678\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 163 ---------\n",
      "num_corrects / total_examples = 25445 / 36865\n",
      "training loss = 0.8678\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 164 ---------\n",
      "num_corrects / total_examples = 25445 / 36865\n",
      "training loss = 0.8679\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 165 ---------\n",
      "num_corrects / total_examples = 25443 / 36865\n",
      "training loss = 0.8680\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 166 ---------\n",
      "num_corrects / total_examples = 25441 / 36865\n",
      "training loss = 0.8680\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 167 ---------\n",
      "num_corrects / total_examples = 25443 / 36865\n",
      "training loss = 0.8681\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 168 ---------\n",
      "num_corrects / total_examples = 25440 / 36865\n",
      "training loss = 0.8682\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 169 ---------\n",
      "num_corrects / total_examples = 25441 / 36865\n",
      "training loss = 0.8682\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 170 ---------\n",
      "num_corrects / total_examples = 25440 / 36865\n",
      "training loss = 0.8683\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 171 ---------\n",
      "num_corrects / total_examples = 25437 / 36865\n",
      "training loss = 0.8683\n",
      "training accuracy = 0.6900\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 172 ---------\n",
      "num_corrects / total_examples = 25435 / 36865\n",
      "training loss = 0.8684\n",
      "training accuracy = 0.6899\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 173 ---------\n",
      "num_corrects / total_examples = 25438 / 36865\n",
      "training loss = 0.8684\n",
      "training accuracy = 0.6900\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 174 ---------\n",
      "num_corrects / total_examples = 25434 / 36865\n",
      "training loss = 0.8685\n",
      "training accuracy = 0.6899\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 175 ---------\n",
      "num_corrects / total_examples = 25436 / 36865\n",
      "training loss = 0.8685\n",
      "training accuracy = 0.6900\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 176 ---------\n",
      "num_corrects / total_examples = 25439 / 36865\n",
      "training loss = 0.8686\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6590 / 9217\n",
      "testing accuracy = 0.7150\n",
      "--------- epoch: 177 ---------\n",
      "num_corrects / total_examples = 25440 / 36865\n",
      "training loss = 0.8686\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6591 / 9217\n",
      "testing accuracy = 0.7151\n",
      "--------- epoch: 178 ---------\n",
      "num_corrects / total_examples = 25439 / 36865\n",
      "training loss = 0.8687\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6591 / 9217\n",
      "testing accuracy = 0.7151\n",
      "--------- epoch: 179 ---------\n",
      "num_corrects / total_examples = 25442 / 36865\n",
      "training loss = 0.8687\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 180 ---------\n",
      "num_corrects / total_examples = 25440 / 36865\n",
      "training loss = 0.8687\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 181 ---------\n",
      "num_corrects / total_examples = 25441 / 36865\n",
      "training loss = 0.8688\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 182 ---------\n",
      "num_corrects / total_examples = 25443 / 36865\n",
      "training loss = 0.8688\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6592 / 9217\n",
      "testing accuracy = 0.7152\n",
      "--------- epoch: 183 ---------\n",
      "num_corrects / total_examples = 25442 / 36865\n",
      "training loss = 0.8688\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 184 ---------\n",
      "num_corrects / total_examples = 25441 / 36865\n",
      "training loss = 0.8689\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 185 ---------\n",
      "num_corrects / total_examples = 25440 / 36865\n",
      "training loss = 0.8689\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 186 ---------\n",
      "num_corrects / total_examples = 25440 / 36865\n",
      "training loss = 0.8689\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 187 ---------\n",
      "num_corrects / total_examples = 25440 / 36865\n",
      "training loss = 0.8690\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 188 ---------\n",
      "num_corrects / total_examples = 25443 / 36865\n",
      "training loss = 0.8690\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 189 ---------\n",
      "num_corrects / total_examples = 25449 / 36865\n",
      "training loss = 0.8690\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 190 ---------\n",
      "num_corrects / total_examples = 25453 / 36865\n",
      "training loss = 0.8713\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 191 ---------\n",
      "num_corrects / total_examples = 25447 / 36865\n",
      "training loss = 0.8713\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 192 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.8714\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 193 ---------\n",
      "num_corrects / total_examples = 25442 / 36865\n",
      "training loss = 0.8714\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 194 ---------\n",
      "num_corrects / total_examples = 25442 / 36865\n",
      "training loss = 0.8715\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 195 ---------\n",
      "num_corrects / total_examples = 25442 / 36865\n",
      "training loss = 0.8715\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6593 / 9217\n",
      "testing accuracy = 0.7153\n",
      "--------- epoch: 196 ---------\n",
      "num_corrects / total_examples = 25443 / 36865\n",
      "training loss = 0.8716\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 197 ---------\n",
      "num_corrects / total_examples = 25443 / 36865\n",
      "training loss = 0.8716\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 198 ---------\n",
      "num_corrects / total_examples = 25442 / 36865\n",
      "training loss = 0.8717\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 199 ---------\n",
      "num_corrects / total_examples = 25444 / 36865\n",
      "training loss = 0.8717\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 200 ---------\n",
      "num_corrects / total_examples = 25442 / 36865\n",
      "training loss = 0.8741\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 201 ---------\n",
      "num_corrects / total_examples = 25442 / 36865\n",
      "training loss = 0.8741\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 202 ---------\n",
      "num_corrects / total_examples = 25445 / 36865\n",
      "training loss = 0.8742\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 203 ---------\n",
      "num_corrects / total_examples = 25444 / 36865\n",
      "training loss = 0.8743\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 204 ---------\n",
      "num_corrects / total_examples = 25441 / 36865\n",
      "training loss = 0.8743\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 205 ---------\n",
      "num_corrects / total_examples = 25442 / 36865\n",
      "training loss = 0.8743\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 206 ---------\n",
      "num_corrects / total_examples = 25439 / 36865\n",
      "training loss = 0.8744\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 207 ---------\n",
      "num_corrects / total_examples = 25437 / 36865\n",
      "training loss = 0.8744\n",
      "training accuracy = 0.6900\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 208 ---------\n",
      "num_corrects / total_examples = 25442 / 36865\n",
      "training loss = 0.8745\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 209 ---------\n",
      "num_corrects / total_examples = 25443 / 36865\n",
      "training loss = 0.8745\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 210 ---------\n",
      "num_corrects / total_examples = 25443 / 36865\n",
      "training loss = 0.8745\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 211 ---------\n",
      "num_corrects / total_examples = 25440 / 36865\n",
      "training loss = 0.8746\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 212 ---------\n",
      "num_corrects / total_examples = 25442 / 36865\n",
      "training loss = 0.8746\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 213 ---------\n",
      "num_corrects / total_examples = 25445 / 36865\n",
      "training loss = 0.8746\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 214 ---------\n",
      "num_corrects / total_examples = 25445 / 36865\n",
      "training loss = 0.8747\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 215 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.8770\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 216 ---------\n",
      "num_corrects / total_examples = 25444 / 36865\n",
      "training loss = 0.8770\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 217 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.8770\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 218 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.8771\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 219 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.8771\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 220 ---------\n",
      "num_corrects / total_examples = 25447 / 36865\n",
      "training loss = 0.8771\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 221 ---------\n",
      "num_corrects / total_examples = 25447 / 36865\n",
      "training loss = 0.8771\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 222 ---------\n",
      "num_corrects / total_examples = 25441 / 36865\n",
      "training loss = 0.8793\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 223 ---------\n",
      "num_corrects / total_examples = 25442 / 36865\n",
      "training loss = 0.8793\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 224 ---------\n",
      "num_corrects / total_examples = 25441 / 36865\n",
      "training loss = 0.8793\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 225 ---------\n",
      "num_corrects / total_examples = 25438 / 36865\n",
      "training loss = 0.8793\n",
      "training accuracy = 0.6900\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 226 ---------\n",
      "num_corrects / total_examples = 25440 / 36865\n",
      "training loss = 0.8793\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 227 ---------\n",
      "num_corrects / total_examples = 25440 / 36865\n",
      "training loss = 0.8793\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 228 ---------\n",
      "num_corrects / total_examples = 25437 / 36865\n",
      "training loss = 0.8793\n",
      "training accuracy = 0.6900\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 229 ---------\n",
      "num_corrects / total_examples = 25437 / 36865\n",
      "training loss = 0.8793\n",
      "training accuracy = 0.6900\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 230 ---------\n",
      "num_corrects / total_examples = 25440 / 36865\n",
      "training loss = 0.8793\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 231 ---------\n",
      "num_corrects / total_examples = 25435 / 36865\n",
      "training loss = 0.8793\n",
      "training accuracy = 0.6899\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 232 ---------\n",
      "num_corrects / total_examples = 25434 / 36865\n",
      "training loss = 0.8793\n",
      "training accuracy = 0.6899\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 233 ---------\n",
      "num_corrects / total_examples = 25433 / 36865\n",
      "training loss = 0.8793\n",
      "training accuracy = 0.6899\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 234 ---------\n",
      "num_corrects / total_examples = 25433 / 36865\n",
      "training loss = 0.8793\n",
      "training accuracy = 0.6899\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 235 ---------\n",
      "num_corrects / total_examples = 25433 / 36865\n",
      "training loss = 0.8793\n",
      "training accuracy = 0.6899\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 236 ---------\n",
      "num_corrects / total_examples = 25431 / 36865\n",
      "training loss = 0.8793\n",
      "training accuracy = 0.6898\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 237 ---------\n",
      "num_corrects / total_examples = 25432 / 36865\n",
      "training loss = 0.8815\n",
      "training accuracy = 0.6899\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 238 ---------\n",
      "num_corrects / total_examples = 25436 / 36865\n",
      "training loss = 0.8815\n",
      "training accuracy = 0.6900\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 239 ---------\n",
      "num_corrects / total_examples = 25439 / 36865\n",
      "training loss = 0.8815\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 240 ---------\n",
      "num_corrects / total_examples = 25439 / 36865\n",
      "training loss = 0.8815\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 241 ---------\n",
      "num_corrects / total_examples = 25439 / 36865\n",
      "training loss = 0.8837\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 242 ---------\n",
      "num_corrects / total_examples = 25440 / 36865\n",
      "training loss = 0.8837\n",
      "training accuracy = 0.6901\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 243 ---------\n",
      "num_corrects / total_examples = 25443 / 36865\n",
      "training loss = 0.8837\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 244 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.8837\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 245 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.8837\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 246 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.8837\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 247 ---------\n",
      "num_corrects / total_examples = 25447 / 36865\n",
      "training loss = 0.8837\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 248 ---------\n",
      "num_corrects / total_examples = 25450 / 36865\n",
      "training loss = 0.8837\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 249 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.8837\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 250 ---------\n",
      "num_corrects / total_examples = 25447 / 36865\n",
      "training loss = 0.8837\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 251 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.8837\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 252 ---------\n",
      "num_corrects / total_examples = 25449 / 36865\n",
      "training loss = 0.8838\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 253 ---------\n",
      "num_corrects / total_examples = 25451 / 36865\n",
      "training loss = 0.8838\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6597 / 9217\n",
      "testing accuracy = 0.7157\n",
      "--------- epoch: 254 ---------\n",
      "num_corrects / total_examples = 25451 / 36865\n",
      "training loss = 0.8838\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6597 / 9217\n",
      "testing accuracy = 0.7157\n",
      "--------- epoch: 255 ---------\n",
      "num_corrects / total_examples = 25449 / 36865\n",
      "training loss = 0.8838\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6597 / 9217\n",
      "testing accuracy = 0.7157\n",
      "--------- epoch: 256 ---------\n",
      "num_corrects / total_examples = 25451 / 36865\n",
      "training loss = 0.8838\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 257 ---------\n",
      "num_corrects / total_examples = 25451 / 36865\n",
      "training loss = 0.8838\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 258 ---------\n",
      "num_corrects / total_examples = 25454 / 36865\n",
      "training loss = 0.8838\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 259 ---------\n",
      "num_corrects / total_examples = 25457 / 36865\n",
      "training loss = 0.8838\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 260 ---------\n",
      "num_corrects / total_examples = 25456 / 36865\n",
      "training loss = 0.8838\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 261 ---------\n",
      "num_corrects / total_examples = 25455 / 36865\n",
      "training loss = 0.8838\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 262 ---------\n",
      "num_corrects / total_examples = 25456 / 36865\n",
      "training loss = 0.8839\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 263 ---------\n",
      "num_corrects / total_examples = 25456 / 36865\n",
      "training loss = 0.8839\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 264 ---------\n",
      "num_corrects / total_examples = 25455 / 36865\n",
      "training loss = 0.8839\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 265 ---------\n",
      "num_corrects / total_examples = 25455 / 36865\n",
      "training loss = 0.8839\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 266 ---------\n",
      "num_corrects / total_examples = 25455 / 36865\n",
      "training loss = 0.8839\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 267 ---------\n",
      "num_corrects / total_examples = 25453 / 36865\n",
      "training loss = 0.8839\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 268 ---------\n",
      "num_corrects / total_examples = 25452 / 36865\n",
      "training loss = 0.8839\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 269 ---------\n",
      "num_corrects / total_examples = 25451 / 36865\n",
      "training loss = 0.8839\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 270 ---------\n",
      "num_corrects / total_examples = 25451 / 36865\n",
      "training loss = 0.8839\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 271 ---------\n",
      "num_corrects / total_examples = 25451 / 36865\n",
      "training loss = 0.8840\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 272 ---------\n",
      "num_corrects / total_examples = 25451 / 36865\n",
      "training loss = 0.8840\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 273 ---------\n",
      "num_corrects / total_examples = 25451 / 36865\n",
      "training loss = 0.8840\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 274 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.8840\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 275 ---------\n",
      "num_corrects / total_examples = 25447 / 36865\n",
      "training loss = 0.8840\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 276 ---------\n",
      "num_corrects / total_examples = 25447 / 36865\n",
      "training loss = 0.8840\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 277 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.8840\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 278 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.8840\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 279 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.8841\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 280 ---------\n",
      "num_corrects / total_examples = 25450 / 36865\n",
      "training loss = 0.8841\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 281 ---------\n",
      "num_corrects / total_examples = 25451 / 36865\n",
      "training loss = 0.8841\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 282 ---------\n",
      "num_corrects / total_examples = 25451 / 36865\n",
      "training loss = 0.8863\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 283 ---------\n",
      "num_corrects / total_examples = 25452 / 36865\n",
      "training loss = 0.8863\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 284 ---------\n",
      "num_corrects / total_examples = 25451 / 36865\n",
      "training loss = 0.8864\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 285 ---------\n",
      "num_corrects / total_examples = 25453 / 36865\n",
      "training loss = 0.8864\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 286 ---------\n",
      "num_corrects / total_examples = 25450 / 36865\n",
      "training loss = 0.8864\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 287 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.8864\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 288 ---------\n",
      "num_corrects / total_examples = 25449 / 36865\n",
      "training loss = 0.8864\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 289 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.8864\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 290 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.8864\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 291 ---------\n",
      "num_corrects / total_examples = 25447 / 36865\n",
      "training loss = 0.8864\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 292 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.8865\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 293 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.8865\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 294 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.8865\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 295 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.8865\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 296 ---------\n",
      "num_corrects / total_examples = 25449 / 36865\n",
      "training loss = 0.8865\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 297 ---------\n",
      "num_corrects / total_examples = 25449 / 36865\n",
      "training loss = 0.8865\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 298 ---------\n",
      "num_corrects / total_examples = 25450 / 36865\n",
      "training loss = 0.8888\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 299 ---------\n",
      "num_corrects / total_examples = 25447 / 36865\n",
      "training loss = 0.8911\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 300 ---------\n",
      "num_corrects / total_examples = 25445 / 36865\n",
      "training loss = 0.8911\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 301 ---------\n",
      "num_corrects / total_examples = 25447 / 36865\n",
      "training loss = 0.8911\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 302 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.8911\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 303 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.8911\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 304 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.8912\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 305 ---------\n",
      "num_corrects / total_examples = 25447 / 36865\n",
      "training loss = 0.8912\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 306 ---------\n",
      "num_corrects / total_examples = 25447 / 36865\n",
      "training loss = 0.8912\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 307 ---------\n",
      "num_corrects / total_examples = 25447 / 36865\n",
      "training loss = 0.8912\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 308 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.8912\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 309 ---------\n",
      "num_corrects / total_examples = 25445 / 36865\n",
      "training loss = 0.8912\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 310 ---------\n",
      "num_corrects / total_examples = 25443 / 36865\n",
      "training loss = 0.8912\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 311 ---------\n",
      "num_corrects / total_examples = 25443 / 36865\n",
      "training loss = 0.8912\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 312 ---------\n",
      "num_corrects / total_examples = 25444 / 36865\n",
      "training loss = 0.8913\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 313 ---------\n",
      "num_corrects / total_examples = 25447 / 36865\n",
      "training loss = 0.8913\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 314 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.8913\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 315 ---------\n",
      "num_corrects / total_examples = 25452 / 36865\n",
      "training loss = 0.8913\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 316 ---------\n",
      "num_corrects / total_examples = 25454 / 36865\n",
      "training loss = 0.8913\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 317 ---------\n",
      "num_corrects / total_examples = 25455 / 36865\n",
      "training loss = 0.8913\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 318 ---------\n",
      "num_corrects / total_examples = 25455 / 36865\n",
      "training loss = 0.8913\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 319 ---------\n",
      "num_corrects / total_examples = 25455 / 36865\n",
      "training loss = 0.8913\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 320 ---------\n",
      "num_corrects / total_examples = 25453 / 36865\n",
      "training loss = 0.8913\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 321 ---------\n",
      "num_corrects / total_examples = 25457 / 36865\n",
      "training loss = 0.8913\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 322 ---------\n",
      "num_corrects / total_examples = 25456 / 36865\n",
      "training loss = 0.8913\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 323 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.8913\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 324 ---------\n",
      "num_corrects / total_examples = 25457 / 36865\n",
      "training loss = 0.8913\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 325 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.8913\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 326 ---------\n",
      "num_corrects / total_examples = 25457 / 36865\n",
      "training loss = 0.8914\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 327 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.8914\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 328 ---------\n",
      "num_corrects / total_examples = 25465 / 36865\n",
      "training loss = 0.8956\n",
      "training accuracy = 0.6908\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 329 ---------\n",
      "num_corrects / total_examples = 25467 / 36865\n",
      "training loss = 0.8956\n",
      "training accuracy = 0.6908\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 330 ---------\n",
      "num_corrects / total_examples = 25472 / 36865\n",
      "training loss = 0.8956\n",
      "training accuracy = 0.6910\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 331 ---------\n",
      "num_corrects / total_examples = 25467 / 36865\n",
      "training loss = 0.8956\n",
      "training accuracy = 0.6908\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 332 ---------\n",
      "num_corrects / total_examples = 25461 / 36865\n",
      "training loss = 0.8979\n",
      "training accuracy = 0.6907\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 333 ---------\n",
      "num_corrects / total_examples = 25460 / 36865\n",
      "training loss = 0.8979\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 334 ---------\n",
      "num_corrects / total_examples = 25462 / 36865\n",
      "training loss = 0.8980\n",
      "training accuracy = 0.6907\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 335 ---------\n",
      "num_corrects / total_examples = 25462 / 36865\n",
      "training loss = 0.8980\n",
      "training accuracy = 0.6907\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 336 ---------\n",
      "num_corrects / total_examples = 25461 / 36865\n",
      "training loss = 0.8980\n",
      "training accuracy = 0.6907\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 337 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.8981\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 338 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.8981\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 339 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.8981\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 340 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.8981\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 341 ---------\n",
      "num_corrects / total_examples = 25461 / 36865\n",
      "training loss = 0.8981\n",
      "training accuracy = 0.6907\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 342 ---------\n",
      "num_corrects / total_examples = 25460 / 36865\n",
      "training loss = 0.8982\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 343 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.8982\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 344 ---------\n",
      "num_corrects / total_examples = 25460 / 36865\n",
      "training loss = 0.8982\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 345 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.8982\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 346 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.8982\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 347 ---------\n",
      "num_corrects / total_examples = 25460 / 36865\n",
      "training loss = 0.8982\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 348 ---------\n",
      "num_corrects / total_examples = 25460 / 36865\n",
      "training loss = 0.8983\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 349 ---------\n",
      "num_corrects / total_examples = 25460 / 36865\n",
      "training loss = 0.8983\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 350 ---------\n",
      "num_corrects / total_examples = 25461 / 36865\n",
      "training loss = 0.8983\n",
      "training accuracy = 0.6907\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 351 ---------\n",
      "num_corrects / total_examples = 25462 / 36865\n",
      "training loss = 0.8983\n",
      "training accuracy = 0.6907\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 352 ---------\n",
      "num_corrects / total_examples = 25464 / 36865\n",
      "training loss = 0.8983\n",
      "training accuracy = 0.6907\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 353 ---------\n",
      "num_corrects / total_examples = 25464 / 36865\n",
      "training loss = 0.8984\n",
      "training accuracy = 0.6907\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 354 ---------\n",
      "num_corrects / total_examples = 25463 / 36865\n",
      "training loss = 0.8984\n",
      "training accuracy = 0.6907\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 355 ---------\n",
      "num_corrects / total_examples = 25463 / 36865\n",
      "training loss = 0.8984\n",
      "training accuracy = 0.6907\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 356 ---------\n",
      "num_corrects / total_examples = 25464 / 36865\n",
      "training loss = 0.8984\n",
      "training accuracy = 0.6907\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 357 ---------\n",
      "num_corrects / total_examples = 25467 / 36865\n",
      "training loss = 0.8984\n",
      "training accuracy = 0.6908\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 358 ---------\n",
      "num_corrects / total_examples = 25464 / 36865\n",
      "training loss = 0.9008\n",
      "training accuracy = 0.6907\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 359 ---------\n",
      "num_corrects / total_examples = 25465 / 36865\n",
      "training loss = 0.9009\n",
      "training accuracy = 0.6908\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 360 ---------\n",
      "num_corrects / total_examples = 25461 / 36865\n",
      "training loss = 0.9009\n",
      "training accuracy = 0.6907\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 361 ---------\n",
      "num_corrects / total_examples = 25457 / 36865\n",
      "training loss = 0.9010\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 362 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9011\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 363 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9011\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 364 ---------\n",
      "num_corrects / total_examples = 25456 / 36865\n",
      "training loss = 0.9012\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 365 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9012\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 366 ---------\n",
      "num_corrects / total_examples = 25456 / 36865\n",
      "training loss = 0.9013\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 367 ---------\n",
      "num_corrects / total_examples = 25457 / 36865\n",
      "training loss = 0.9013\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 368 ---------\n",
      "num_corrects / total_examples = 25456 / 36865\n",
      "training loss = 0.9013\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 369 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.9014\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 370 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.9014\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 371 ---------\n",
      "num_corrects / total_examples = 25454 / 36865\n",
      "training loss = 0.9014\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 372 ---------\n",
      "num_corrects / total_examples = 25450 / 36865\n",
      "training loss = 0.9015\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 373 ---------\n",
      "num_corrects / total_examples = 25451 / 36865\n",
      "training loss = 0.9015\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 374 ---------\n",
      "num_corrects / total_examples = 25449 / 36865\n",
      "training loss = 0.9015\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 375 ---------\n",
      "num_corrects / total_examples = 25451 / 36865\n",
      "training loss = 0.9015\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 376 ---------\n",
      "num_corrects / total_examples = 25449 / 36865\n",
      "training loss = 0.9015\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 377 ---------\n",
      "num_corrects / total_examples = 25450 / 36865\n",
      "training loss = 0.9016\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 378 ---------\n",
      "num_corrects / total_examples = 25449 / 36865\n",
      "training loss = 0.9016\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 379 ---------\n",
      "num_corrects / total_examples = 25450 / 36865\n",
      "training loss = 0.9016\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 380 ---------\n",
      "num_corrects / total_examples = 25449 / 36865\n",
      "training loss = 0.9016\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 381 ---------\n",
      "num_corrects / total_examples = 25451 / 36865\n",
      "training loss = 0.9016\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 382 ---------\n",
      "num_corrects / total_examples = 25451 / 36865\n",
      "training loss = 0.9016\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 383 ---------\n",
      "num_corrects / total_examples = 25451 / 36865\n",
      "training loss = 0.9016\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 384 ---------\n",
      "num_corrects / total_examples = 25449 / 36865\n",
      "training loss = 0.9016\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 385 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.9017\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 386 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.9017\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 387 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.9017\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 388 ---------\n",
      "num_corrects / total_examples = 25445 / 36865\n",
      "training loss = 0.9017\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 389 ---------\n",
      "num_corrects / total_examples = 25445 / 36865\n",
      "training loss = 0.9017\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 390 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.9017\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 391 ---------\n",
      "num_corrects / total_examples = 25445 / 36865\n",
      "training loss = 0.9017\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 392 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 393 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 394 ---------\n",
      "num_corrects / total_examples = 25445 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 395 ---------\n",
      "num_corrects / total_examples = 25443 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 396 ---------\n",
      "num_corrects / total_examples = 25443 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 397 ---------\n",
      "num_corrects / total_examples = 25444 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 398 ---------\n",
      "num_corrects / total_examples = 25445 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 399 ---------\n",
      "num_corrects / total_examples = 25447 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 400 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 401 ---------\n",
      "num_corrects / total_examples = 25447 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 402 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 403 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 404 ---------\n",
      "num_corrects / total_examples = 25444 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 405 ---------\n",
      "num_corrects / total_examples = 25444 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 406 ---------\n",
      "num_corrects / total_examples = 25444 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 407 ---------\n",
      "num_corrects / total_examples = 25445 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 408 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 409 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 410 ---------\n",
      "num_corrects / total_examples = 25449 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 411 ---------\n",
      "num_corrects / total_examples = 25449 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 412 ---------\n",
      "num_corrects / total_examples = 25451 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 413 ---------\n",
      "num_corrects / total_examples = 25450 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 414 ---------\n",
      "num_corrects / total_examples = 25450 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 415 ---------\n",
      "num_corrects / total_examples = 25450 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 416 ---------\n",
      "num_corrects / total_examples = 25447 / 36865\n",
      "training loss = 0.9018\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 417 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 418 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 419 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 420 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 421 ---------\n",
      "num_corrects / total_examples = 25446 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6902\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 422 ---------\n",
      "num_corrects / total_examples = 25447 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 423 ---------\n",
      "num_corrects / total_examples = 25447 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 424 ---------\n",
      "num_corrects / total_examples = 25449 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 425 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 426 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 427 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 428 ---------\n",
      "num_corrects / total_examples = 25447 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 429 ---------\n",
      "num_corrects / total_examples = 25447 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 430 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 431 ---------\n",
      "num_corrects / total_examples = 25449 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 432 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 433 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 434 ---------\n",
      "num_corrects / total_examples = 25449 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 435 ---------\n",
      "num_corrects / total_examples = 25450 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 436 ---------\n",
      "num_corrects / total_examples = 25449 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 437 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 438 ---------\n",
      "num_corrects / total_examples = 25449 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 439 ---------\n",
      "num_corrects / total_examples = 25449 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 440 ---------\n",
      "num_corrects / total_examples = 25448 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 441 ---------\n",
      "num_corrects / total_examples = 25450 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 442 ---------\n",
      "num_corrects / total_examples = 25450 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 443 ---------\n",
      "num_corrects / total_examples = 25450 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 444 ---------\n",
      "num_corrects / total_examples = 25449 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 445 ---------\n",
      "num_corrects / total_examples = 25449 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6903\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 446 ---------\n",
      "num_corrects / total_examples = 25450 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 447 ---------\n",
      "num_corrects / total_examples = 25450 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "--------- epoch: 448 ---------\n",
      "num_corrects / total_examples = 25451 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 449 ---------\n",
      "num_corrects / total_examples = 25451 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 450 ---------\n",
      "num_corrects / total_examples = 25452 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 451 ---------\n",
      "num_corrects / total_examples = 25453 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 452 ---------\n",
      "num_corrects / total_examples = 25454 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 453 ---------\n",
      "num_corrects / total_examples = 25454 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 454 ---------\n",
      "num_corrects / total_examples = 25454 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 455 ---------\n",
      "num_corrects / total_examples = 25454 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 456 ---------\n",
      "num_corrects / total_examples = 25455 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 457 ---------\n",
      "num_corrects / total_examples = 25455 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 458 ---------\n",
      "num_corrects / total_examples = 25455 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 459 ---------\n",
      "num_corrects / total_examples = 25455 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 460 ---------\n",
      "num_corrects / total_examples = 25456 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 461 ---------\n",
      "num_corrects / total_examples = 25456 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 462 ---------\n",
      "num_corrects / total_examples = 25457 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 463 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 464 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 465 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 466 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 467 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 468 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 469 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 470 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 471 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 472 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 473 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9019\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 474 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9020\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 475 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9020\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 476 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9020\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 477 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9020\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 478 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9020\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 479 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9020\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 480 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9020\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 481 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.9020\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 482 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.9020\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 483 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.9020\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 484 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.9020\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 485 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.9020\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 486 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.9020\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 487 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.9020\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 488 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.9020\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 489 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.9020\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 490 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.9020\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 491 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.9020\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 492 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.9020\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 493 ---------\n",
      "num_corrects / total_examples = 25458 / 36865\n",
      "training loss = 0.9020\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6595 / 9217\n",
      "testing accuracy = 0.7155\n",
      "--------- epoch: 494 ---------\n",
      "num_corrects / total_examples = 25455 / 36865\n",
      "training loss = 0.9043\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 495 ---------\n",
      "num_corrects / total_examples = 25459 / 36865\n",
      "training loss = 0.9043\n",
      "training accuracy = 0.6906\n",
      "num_test_corrects / test_total_examples = 6596 / 9217\n",
      "testing accuracy = 0.7156\n",
      "--------- epoch: 496 ---------\n",
      "num_corrects / total_examples = 25457 / 36865\n",
      "training loss = 0.9066\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6597 / 9217\n",
      "testing accuracy = 0.7157\n",
      "--------- epoch: 497 ---------\n",
      "num_corrects / total_examples = 25456 / 36865\n",
      "training loss = 0.9066\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6597 / 9217\n",
      "testing accuracy = 0.7157\n",
      "--------- epoch: 498 ---------\n",
      "num_corrects / total_examples = 25457 / 36865\n",
      "training loss = 0.9066\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6597 / 9217\n",
      "testing accuracy = 0.7157\n",
      "--------- epoch: 499 ---------\n",
      "num_corrects / total_examples = 25454 / 36865\n",
      "training loss = 0.9066\n",
      "training accuracy = 0.6905\n",
      "num_test_corrects / test_total_examples = 6597 / 9217\n",
      "testing accuracy = 0.7157\n",
      "--------- epoch: 500 ---------\n",
      "num_corrects / total_examples = 25453 / 36865\n",
      "training loss = 0.9066\n",
      "training accuracy = 0.6904\n",
      "num_test_corrects / test_total_examples = 6597 / 9217\n",
      "testing accuracy = 0.7157\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accuracies, test_accuracies = train(lr_model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcf068cfb20>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcf0db8f520>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if device.type == 'cuda':\n",
    "    train_accuracies = [acc.cpu() for acc in train_accuracies]\n",
    "plt.plot(train_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcf057fb790>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max train accuracy = 0.6909534931182861\n",
      "max test accuracy = 0.7175870673755018\n"
     ]
    }
   ],
   "source": [
    "print(f\"max train accuracy = {max(train_accuracies)}\")\n",
    "print(f\"max test accuracy = {max(test_accuracies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0475,  0.1035, -0.0447, -0.0250, -0.1009,  0.1327, -0.0243,  0.0446,\n",
      "         -0.0705,  0.0117,  0.0269, -0.0262,  0.0708, -0.1628,  0.0670, -0.0204,\n",
      "          0.0022,  0.1177, -0.0419,  0.0290, -0.1351,  0.0856,  0.0096, -0.0676,\n",
      "         -0.0191,  0.0309, -0.0240,  0.0377, -0.0050,  0.0314, -0.0844,  0.0493,\n",
      "          0.0681, -0.0734, -0.0164, -0.0241,  0.0482, -0.0055, -0.0927,  0.0939,\n",
      "          0.0143, -0.0523,  0.0284, -0.0237, -0.0709,  0.0867, -0.0470,  0.0380,\n",
      "         -0.1166,  0.0613,  0.0170, -0.0462,  0.0557, -0.0159,  0.0035, -0.0027,\n",
      "         -0.0035, -0.0178, -0.0328,  0.0420,  0.0055, -0.0318, -0.0012,  0.0088,\n",
      "         -0.0286,  0.1226, -0.0468, -0.1034,  0.0386,  0.0867, -0.0821, -0.0181,\n",
      "         -0.0529,  0.0987, -0.1463,  0.0284, -0.0308,  0.1270, -0.1194,  0.0476,\n",
      "         -0.0543,  0.0328,  0.0013,  0.0659, -0.0322, -0.0188, -0.0099, -0.0390,\n",
      "         -0.0092,  0.0735, -0.0243, -0.0333, -0.0325,  0.0263,  0.0452, -0.0362,\n",
      "         -0.0584,  0.0588, -0.0064, -0.0262, -0.0339,  0.0843, -0.0810,  0.0461,\n",
      "          0.0221, -0.0302,  0.0112, -0.0171,  0.0334,  0.0499, -0.0764, -0.0188,\n",
      "         -0.0597,  0.1065, -0.0143, -0.0409, -0.0046, -0.0251,  0.0148,  0.0549,\n",
      "         -0.0793, -0.0119,  0.1042, -0.0516,  0.0584, -0.0317, -0.0590, -0.0005,\n",
      "          0.0190, -0.0189,  0.0484, -0.0451,  0.0057,  0.0454, -0.1059,  0.0645,\n",
      "         -0.0176,  0.0386, -0.0004,  0.0095, -0.0507,  0.0178,  0.0177, -0.0190,\n",
      "         -0.0491,  0.0525, -0.0531,  0.0209,  0.0842, -0.0046, -0.0751, -0.0178,\n",
      "         -0.0522,  0.0858, -0.0845,  0.0038,  0.0097,  0.0083, -0.0185, -0.0584,\n",
      "          0.0911,  0.0207,  0.0589, -0.0831, -0.0029, -0.0587,  0.0456, -0.0303,\n",
      "         -0.0374,  0.0922, -0.0205, -0.0676,  0.0911, -0.0450, -0.0575,  0.0240,\n",
      "          0.0020,  0.0269, -0.0049, -0.0593,  0.0777, -0.0815,  0.0143, -0.0079,\n",
      "          0.0119,  0.0202, -0.0172,  0.0364,  0.0148, -0.0810, -0.0007, -0.0471,\n",
      "          0.0380,  0.0522, -0.0576,  0.0265,  0.0303, -0.0009, -0.0668, -0.0122,\n",
      "          0.0316, -0.0547,  0.0458, -0.0262,  0.0556, -0.0278,  0.0205,  0.0332,\n",
      "         -0.0131, -0.0726,  0.0068,  0.0048,  0.0240, -0.0211, -0.0126,  0.0622,\n",
      "         -0.0004, -0.0745,  0.0598,  0.0399, -0.0352, -0.0178,  0.0034, -0.0112,\n",
      "         -0.0975,  0.0602, -0.0072,  0.0619, -0.0196, -0.0494,  0.0089, -0.0550,\n",
      "          0.0035,  0.0781, -0.0128,  0.0224, -0.1101,  0.0237,  0.0096,  0.0317,\n",
      "         -0.0258, -0.0367,  0.0379,  0.0221, -0.0622,  0.0149,  0.0433,  0.0016,\n",
      "         -0.0581,  0.0141, -0.0061,  0.0482, -0.0531,  0.0211, -0.0255,  0.0044,\n",
      "         -0.0492,  0.0424,  0.0249, -0.0512,  0.0853, -0.0482, -0.0526,  0.0079,\n",
      "          0.0534, -0.1026, -0.0249,  0.0367,  0.0709, -0.0685, -0.0809,  0.0565,\n",
      "         -0.0171, -0.0130,  0.0617, -0.0134, -0.0315,  0.0750, -0.0636, -0.0023,\n",
      "         -0.0517, -0.0248,  0.0051, -0.0543,  0.0500,  0.0402, -0.1269,  0.1004,\n",
      "         -0.0755,  0.0796,  0.0434, -0.0861, -0.0160, -0.0035, -0.0129,  0.0050,\n",
      "          0.0107,  0.0267, -0.0297, -0.0417]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([16.4042], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in lr_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Pytorch - Float Data (Min-max Scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.x.shape = torch.Size([36865, 300])\n",
      "self.y.shape = torch.Size([36865, 1])\n",
      "torch.max(self.x) = tensor(1.)\n",
      "torch.min(self.x) = tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "train_dataset_scaled = Hypnogram(input_path=input_path, \n",
    "                                 output_path=output_path,\n",
    "                                 train=True, \n",
    "                                 scale=True)\n",
    "train_dataset_scaled.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.x.shape = torch.Size([9217, 300])\n",
      "self.y.shape = torch.Size([9217, 1])\n",
      "torch.max(self.x) = tensor(1.)\n",
      "torch.min(self.x) = tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "test_dataset_scaled = Hypnogram(input_path=input_path, \n",
    "                                output_path=output_path,\n",
    "                                train=False, \n",
    "                                scale=True)\n",
    "test_dataset_scaled.print_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd310eb1580>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_dataset_scaled.x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_scaled = DataLoader(train_dataset_scaled, batch_size=batch_size)\n",
    "test_loader_scaled = DataLoader(test_dataset_scaled, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchLinearModel(\n",
       "  (linear): Linear(in_features=300, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "lc_model = TorchLinearModel(300, 1)\n",
    "lc_model.to(device)\n",
    "lc_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- epoch: 1 ---------\n",
      "num_corrects / total_examples = 25736 / 36865\n",
      "training loss = 0.5986\n",
      "training accuracy = 0.6981\n",
      "num_test_corrects / test_total_examples = 6444 / 9217\n",
      "testing accuracy = 0.6991\n",
      "found best test accuracy at epoch 1\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 2 ---------\n",
      "num_corrects / total_examples = 26079 / 36865\n",
      "training loss = 0.5848\n",
      "training accuracy = 0.7074\n",
      "num_test_corrects / test_total_examples = 6490 / 9217\n",
      "testing accuracy = 0.7041\n",
      "found best test accuracy at epoch 2\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 3 ---------\n",
      "num_corrects / total_examples = 26325 / 36865\n",
      "training loss = 0.5744\n",
      "training accuracy = 0.7141\n",
      "num_test_corrects / test_total_examples = 6541 / 9217\n",
      "testing accuracy = 0.7097\n",
      "found best test accuracy at epoch 3\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 4 ---------\n",
      "num_corrects / total_examples = 26514 / 36865\n",
      "training loss = 0.5664\n",
      "training accuracy = 0.7192\n",
      "num_test_corrects / test_total_examples = 6594 / 9217\n",
      "testing accuracy = 0.7154\n",
      "found best test accuracy at epoch 4\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 5 ---------\n",
      "num_corrects / total_examples = 26683 / 36865\n",
      "training loss = 0.5601\n",
      "training accuracy = 0.7238\n",
      "num_test_corrects / test_total_examples = 6630 / 9217\n",
      "testing accuracy = 0.7193\n",
      "found best test accuracy at epoch 5\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 6 ---------\n",
      "num_corrects / total_examples = 26786 / 36865\n",
      "training loss = 0.5550\n",
      "training accuracy = 0.7266\n",
      "num_test_corrects / test_total_examples = 6665 / 9217\n",
      "testing accuracy = 0.7231\n",
      "found best test accuracy at epoch 6\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 7 ---------\n",
      "num_corrects / total_examples = 26889 / 36865\n",
      "training loss = 0.5508\n",
      "training accuracy = 0.7294\n",
      "num_test_corrects / test_total_examples = 6694 / 9217\n",
      "testing accuracy = 0.7263\n",
      "found best test accuracy at epoch 7\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 8 ---------\n",
      "num_corrects / total_examples = 26985 / 36865\n",
      "training loss = 0.5474\n",
      "training accuracy = 0.7320\n",
      "num_test_corrects / test_total_examples = 6730 / 9217\n",
      "testing accuracy = 0.7302\n",
      "found best test accuracy at epoch 8\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 9 ---------\n",
      "num_corrects / total_examples = 27041 / 36865\n",
      "training loss = 0.5446\n",
      "training accuracy = 0.7335\n",
      "num_test_corrects / test_total_examples = 6753 / 9217\n",
      "testing accuracy = 0.7327\n",
      "found best test accuracy at epoch 9\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 10 ---------\n",
      "num_corrects / total_examples = 27116 / 36865\n",
      "training loss = 0.5422\n",
      "training accuracy = 0.7355\n",
      "num_test_corrects / test_total_examples = 6762 / 9217\n",
      "testing accuracy = 0.7336\n",
      "found best test accuracy at epoch 10\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 11 ---------\n",
      "num_corrects / total_examples = 27165 / 36865\n",
      "training loss = 0.5402\n",
      "training accuracy = 0.7369\n",
      "num_test_corrects / test_total_examples = 6782 / 9217\n",
      "testing accuracy = 0.7358\n",
      "found best test accuracy at epoch 11\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 12 ---------\n",
      "num_corrects / total_examples = 27209 / 36865\n",
      "training loss = 0.5384\n",
      "training accuracy = 0.7381\n",
      "num_test_corrects / test_total_examples = 6791 / 9217\n",
      "testing accuracy = 0.7368\n",
      "found best test accuracy at epoch 12\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 13 ---------\n",
      "num_corrects / total_examples = 27255 / 36865\n",
      "training loss = 0.5370\n",
      "training accuracy = 0.7393\n",
      "num_test_corrects / test_total_examples = 6797 / 9217\n",
      "testing accuracy = 0.7374\n",
      "found best test accuracy at epoch 13\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 14 ---------\n",
      "num_corrects / total_examples = 27273 / 36865\n",
      "training loss = 0.5357\n",
      "training accuracy = 0.7398\n",
      "num_test_corrects / test_total_examples = 6796 / 9217\n",
      "testing accuracy = 0.7373\n",
      "--------- epoch: 15 ---------\n",
      "num_corrects / total_examples = 27318 / 36865\n",
      "training loss = 0.5346\n",
      "training accuracy = 0.7410\n",
      "num_test_corrects / test_total_examples = 6806 / 9217\n",
      "testing accuracy = 0.7384\n",
      "found best test accuracy at epoch 15\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 16 ---------\n",
      "num_corrects / total_examples = 27348 / 36865\n",
      "training loss = 0.5337\n",
      "training accuracy = 0.7418\n",
      "num_test_corrects / test_total_examples = 6819 / 9217\n",
      "testing accuracy = 0.7398\n",
      "found best test accuracy at epoch 16\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 17 ---------\n",
      "num_corrects / total_examples = 27387 / 36865\n",
      "training loss = 0.5329\n",
      "training accuracy = 0.7429\n",
      "num_test_corrects / test_total_examples = 6838 / 9217\n",
      "testing accuracy = 0.7419\n",
      "found best test accuracy at epoch 17\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 18 ---------\n",
      "num_corrects / total_examples = 27420 / 36865\n",
      "training loss = 0.5321\n",
      "training accuracy = 0.7438\n",
      "num_test_corrects / test_total_examples = 6847 / 9217\n",
      "testing accuracy = 0.7429\n",
      "found best test accuracy at epoch 18\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 19 ---------\n",
      "num_corrects / total_examples = 27436 / 36865\n",
      "training loss = 0.5315\n",
      "training accuracy = 0.7442\n",
      "num_test_corrects / test_total_examples = 6851 / 9217\n",
      "testing accuracy = 0.7433\n",
      "found best test accuracy at epoch 19\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 20 ---------\n",
      "num_corrects / total_examples = 27466 / 36865\n",
      "training loss = 0.5310\n",
      "training accuracy = 0.7450\n",
      "num_test_corrects / test_total_examples = 6854 / 9217\n",
      "testing accuracy = 0.7436\n",
      "found best test accuracy at epoch 20\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 21 ---------\n",
      "num_corrects / total_examples = 27498 / 36865\n",
      "training loss = 0.5305\n",
      "training accuracy = 0.7459\n",
      "num_test_corrects / test_total_examples = 6857 / 9217\n",
      "testing accuracy = 0.7440\n",
      "found best test accuracy at epoch 21\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 22 ---------\n",
      "num_corrects / total_examples = 27492 / 36865\n",
      "training loss = 0.5301\n",
      "training accuracy = 0.7457\n",
      "num_test_corrects / test_total_examples = 6860 / 9217\n",
      "testing accuracy = 0.7443\n",
      "found best test accuracy at epoch 22\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 23 ---------\n",
      "num_corrects / total_examples = 27500 / 36865\n",
      "training loss = 0.5297\n",
      "training accuracy = 0.7460\n",
      "num_test_corrects / test_total_examples = 6869 / 9217\n",
      "testing accuracy = 0.7453\n",
      "found best test accuracy at epoch 23\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 24 ---------\n",
      "num_corrects / total_examples = 27515 / 36865\n",
      "training loss = 0.5294\n",
      "training accuracy = 0.7464\n",
      "num_test_corrects / test_total_examples = 6863 / 9217\n",
      "testing accuracy = 0.7446\n",
      "--------- epoch: 25 ---------\n",
      "num_corrects / total_examples = 27527 / 36865\n",
      "training loss = 0.5291\n",
      "training accuracy = 0.7467\n",
      "num_test_corrects / test_total_examples = 6868 / 9217\n",
      "testing accuracy = 0.7451\n",
      "--------- epoch: 26 ---------\n",
      "num_corrects / total_examples = 27538 / 36865\n",
      "training loss = 0.5288\n",
      "training accuracy = 0.7470\n",
      "num_test_corrects / test_total_examples = 6872 / 9217\n",
      "testing accuracy = 0.7456\n",
      "found best test accuracy at epoch 26\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 27 ---------\n",
      "num_corrects / total_examples = 27548 / 36865\n",
      "training loss = 0.5286\n",
      "training accuracy = 0.7473\n",
      "num_test_corrects / test_total_examples = 6872 / 9217\n",
      "testing accuracy = 0.7456\n",
      "--------- epoch: 28 ---------\n",
      "num_corrects / total_examples = 27557 / 36865\n",
      "training loss = 0.5284\n",
      "training accuracy = 0.7475\n",
      "num_test_corrects / test_total_examples = 6870 / 9217\n",
      "testing accuracy = 0.7454\n",
      "--------- epoch: 29 ---------\n",
      "num_corrects / total_examples = 27565 / 36865\n",
      "training loss = 0.5282\n",
      "training accuracy = 0.7477\n",
      "num_test_corrects / test_total_examples = 6865 / 9217\n",
      "testing accuracy = 0.7448\n",
      "--------- epoch: 30 ---------\n",
      "num_corrects / total_examples = 27572 / 36865\n",
      "training loss = 0.5280\n",
      "training accuracy = 0.7479\n",
      "num_test_corrects / test_total_examples = 6864 / 9217\n",
      "testing accuracy = 0.7447\n",
      "--------- epoch: 31 ---------\n",
      "num_corrects / total_examples = 27576 / 36865\n",
      "training loss = 0.5279\n",
      "training accuracy = 0.7480\n",
      "num_test_corrects / test_total_examples = 6868 / 9217\n",
      "testing accuracy = 0.7451\n",
      "--------- epoch: 32 ---------\n",
      "num_corrects / total_examples = 27582 / 36865\n",
      "training loss = 0.5277\n",
      "training accuracy = 0.7482\n",
      "num_test_corrects / test_total_examples = 6871 / 9217\n",
      "testing accuracy = 0.7455\n",
      "--------- epoch: 33 ---------\n",
      "num_corrects / total_examples = 27577 / 36865\n",
      "training loss = 0.5276\n",
      "training accuracy = 0.7481\n",
      "num_test_corrects / test_total_examples = 6874 / 9217\n",
      "testing accuracy = 0.7458\n",
      "found best test accuracy at epoch 33\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 34 ---------\n",
      "num_corrects / total_examples = 27583 / 36865\n",
      "training loss = 0.5275\n",
      "training accuracy = 0.7482\n",
      "num_test_corrects / test_total_examples = 6874 / 9217\n",
      "testing accuracy = 0.7458\n",
      "--------- epoch: 35 ---------\n",
      "num_corrects / total_examples = 27600 / 36865\n",
      "training loss = 0.5274\n",
      "training accuracy = 0.7487\n",
      "num_test_corrects / test_total_examples = 6871 / 9217\n",
      "testing accuracy = 0.7455\n",
      "--------- epoch: 36 ---------\n",
      "num_corrects / total_examples = 27602 / 36865\n",
      "training loss = 0.5273\n",
      "training accuracy = 0.7487\n",
      "num_test_corrects / test_total_examples = 6875 / 9217\n",
      "testing accuracy = 0.7459\n",
      "found best test accuracy at epoch 36\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 37 ---------\n",
      "num_corrects / total_examples = 27612 / 36865\n",
      "training loss = 0.5272\n",
      "training accuracy = 0.7490\n",
      "num_test_corrects / test_total_examples = 6877 / 9217\n",
      "testing accuracy = 0.7461\n",
      "found best test accuracy at epoch 37\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 38 ---------\n",
      "num_corrects / total_examples = 27610 / 36865\n",
      "training loss = 0.5271\n",
      "training accuracy = 0.7489\n",
      "num_test_corrects / test_total_examples = 6883 / 9217\n",
      "testing accuracy = 0.7468\n",
      "found best test accuracy at epoch 38\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 39 ---------\n",
      "num_corrects / total_examples = 27615 / 36865\n",
      "training loss = 0.5270\n",
      "training accuracy = 0.7491\n",
      "num_test_corrects / test_total_examples = 6886 / 9217\n",
      "testing accuracy = 0.7471\n",
      "found best test accuracy at epoch 39\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 40 ---------\n",
      "num_corrects / total_examples = 27619 / 36865\n",
      "training loss = 0.5270\n",
      "training accuracy = 0.7492\n",
      "num_test_corrects / test_total_examples = 6885 / 9217\n",
      "testing accuracy = 0.7470\n",
      "--------- epoch: 41 ---------\n",
      "num_corrects / total_examples = 27618 / 36865\n",
      "training loss = 0.5269\n",
      "training accuracy = 0.7492\n",
      "num_test_corrects / test_total_examples = 6890 / 9217\n",
      "testing accuracy = 0.7475\n",
      "found best test accuracy at epoch 41\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 42 ---------\n",
      "num_corrects / total_examples = 27624 / 36865\n",
      "training loss = 0.5268\n",
      "training accuracy = 0.7493\n",
      "num_test_corrects / test_total_examples = 6888 / 9217\n",
      "testing accuracy = 0.7473\n",
      "--------- epoch: 43 ---------\n",
      "num_corrects / total_examples = 27614 / 36865\n",
      "training loss = 0.5268\n",
      "training accuracy = 0.7491\n",
      "num_test_corrects / test_total_examples = 6888 / 9217\n",
      "testing accuracy = 0.7473\n",
      "--------- epoch: 44 ---------\n",
      "num_corrects / total_examples = 27618 / 36865\n",
      "training loss = 0.5267\n",
      "training accuracy = 0.7492\n",
      "num_test_corrects / test_total_examples = 6885 / 9217\n",
      "testing accuracy = 0.7470\n",
      "--------- epoch: 45 ---------\n",
      "num_corrects / total_examples = 27621 / 36865\n",
      "training loss = 0.5267\n",
      "training accuracy = 0.7492\n",
      "num_test_corrects / test_total_examples = 6883 / 9217\n",
      "testing accuracy = 0.7468\n",
      "--------- epoch: 46 ---------\n",
      "num_corrects / total_examples = 27620 / 36865\n",
      "training loss = 0.5266\n",
      "training accuracy = 0.7492\n",
      "num_test_corrects / test_total_examples = 6884 / 9217\n",
      "testing accuracy = 0.7469\n",
      "--------- epoch: 47 ---------\n",
      "num_corrects / total_examples = 27625 / 36865\n",
      "training loss = 0.5266\n",
      "training accuracy = 0.7494\n",
      "num_test_corrects / test_total_examples = 6881 / 9217\n",
      "testing accuracy = 0.7466\n",
      "--------- epoch: 48 ---------\n",
      "num_corrects / total_examples = 27631 / 36865\n",
      "training loss = 0.5266\n",
      "training accuracy = 0.7495\n",
      "num_test_corrects / test_total_examples = 6883 / 9217\n",
      "testing accuracy = 0.7468\n",
      "--------- epoch: 49 ---------\n",
      "num_corrects / total_examples = 27636 / 36865\n",
      "training loss = 0.5265\n",
      "training accuracy = 0.7497\n",
      "num_test_corrects / test_total_examples = 6887 / 9217\n",
      "testing accuracy = 0.7472\n",
      "--------- epoch: 50 ---------\n",
      "num_corrects / total_examples = 27630 / 36865\n",
      "training loss = 0.5265\n",
      "training accuracy = 0.7495\n",
      "num_test_corrects / test_total_examples = 6886 / 9217\n",
      "testing accuracy = 0.7471\n",
      "--------- epoch: 51 ---------\n",
      "num_corrects / total_examples = 27633 / 36865\n",
      "training loss = 0.5265\n",
      "training accuracy = 0.7496\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 52 ---------\n",
      "num_corrects / total_examples = 27637 / 36865\n",
      "training loss = 0.5264\n",
      "training accuracy = 0.7497\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 53 ---------\n",
      "num_corrects / total_examples = 27641 / 36865\n",
      "training loss = 0.5264\n",
      "training accuracy = 0.7498\n",
      "num_test_corrects / test_total_examples = 6888 / 9217\n",
      "testing accuracy = 0.7473\n",
      "--------- epoch: 54 ---------\n",
      "num_corrects / total_examples = 27648 / 36865\n",
      "training loss = 0.5264\n",
      "training accuracy = 0.7500\n",
      "num_test_corrects / test_total_examples = 6887 / 9217\n",
      "testing accuracy = 0.7472\n",
      "--------- epoch: 55 ---------\n",
      "num_corrects / total_examples = 27650 / 36865\n",
      "training loss = 0.5263\n",
      "training accuracy = 0.7500\n",
      "num_test_corrects / test_total_examples = 6888 / 9217\n",
      "testing accuracy = 0.7473\n",
      "--------- epoch: 56 ---------\n",
      "num_corrects / total_examples = 27652 / 36865\n",
      "training loss = 0.5263\n",
      "training accuracy = 0.7501\n",
      "num_test_corrects / test_total_examples = 6888 / 9217\n",
      "testing accuracy = 0.7473\n",
      "--------- epoch: 57 ---------\n",
      "num_corrects / total_examples = 27654 / 36865\n",
      "training loss = 0.5263\n",
      "training accuracy = 0.7501\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 58 ---------\n",
      "num_corrects / total_examples = 27658 / 36865\n",
      "training loss = 0.5262\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6885 / 9217\n",
      "testing accuracy = 0.7470\n",
      "--------- epoch: 59 ---------\n",
      "num_corrects / total_examples = 27659 / 36865\n",
      "training loss = 0.5262\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6885 / 9217\n",
      "testing accuracy = 0.7470\n",
      "--------- epoch: 60 ---------\n",
      "num_corrects / total_examples = 27659 / 36865\n",
      "training loss = 0.5262\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6887 / 9217\n",
      "testing accuracy = 0.7472\n",
      "--------- epoch: 61 ---------\n",
      "num_corrects / total_examples = 27660 / 36865\n",
      "training loss = 0.5262\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6886 / 9217\n",
      "testing accuracy = 0.7471\n",
      "--------- epoch: 62 ---------\n",
      "num_corrects / total_examples = 27662 / 36865\n",
      "training loss = 0.5261\n",
      "training accuracy = 0.7504\n",
      "num_test_corrects / test_total_examples = 6886 / 9217\n",
      "testing accuracy = 0.7471\n",
      "--------- epoch: 63 ---------\n",
      "num_corrects / total_examples = 27660 / 36865\n",
      "training loss = 0.5261\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6887 / 9217\n",
      "testing accuracy = 0.7472\n",
      "--------- epoch: 64 ---------\n",
      "num_corrects / total_examples = 27661 / 36865\n",
      "training loss = 0.5261\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6888 / 9217\n",
      "testing accuracy = 0.7473\n",
      "--------- epoch: 65 ---------\n",
      "num_corrects / total_examples = 27660 / 36865\n",
      "training loss = 0.5261\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6890 / 9217\n",
      "testing accuracy = 0.7475\n",
      "--------- epoch: 66 ---------\n",
      "num_corrects / total_examples = 27658 / 36865\n",
      "training loss = 0.5260\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "found best test accuracy at epoch 66\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 67 ---------\n",
      "num_corrects / total_examples = 27659 / 36865\n",
      "training loss = 0.5260\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6892 / 9217\n",
      "testing accuracy = 0.7477\n",
      "found best test accuracy at epoch 67\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 68 ---------\n",
      "num_corrects / total_examples = 27661 / 36865\n",
      "training loss = 0.5260\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 69 ---------\n",
      "num_corrects / total_examples = 27657 / 36865\n",
      "training loss = 0.5260\n",
      "training accuracy = 0.7502\n",
      "num_test_corrects / test_total_examples = 6892 / 9217\n",
      "testing accuracy = 0.7477\n",
      "--------- epoch: 70 ---------\n",
      "num_corrects / total_examples = 27653 / 36865\n",
      "training loss = 0.5260\n",
      "training accuracy = 0.7501\n",
      "num_test_corrects / test_total_examples = 6892 / 9217\n",
      "testing accuracy = 0.7477\n",
      "--------- epoch: 71 ---------\n",
      "num_corrects / total_examples = 27656 / 36865\n",
      "training loss = 0.5259\n",
      "training accuracy = 0.7502\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 72 ---------\n",
      "num_corrects / total_examples = 27657 / 36865\n",
      "training loss = 0.5259\n",
      "training accuracy = 0.7502\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 73 ---------\n",
      "num_corrects / total_examples = 27657 / 36865\n",
      "training loss = 0.5259\n",
      "training accuracy = 0.7502\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 74 ---------\n",
      "num_corrects / total_examples = 27657 / 36865\n",
      "training loss = 0.5259\n",
      "training accuracy = 0.7502\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 75 ---------\n",
      "num_corrects / total_examples = 27656 / 36865\n",
      "training loss = 0.5259\n",
      "training accuracy = 0.7502\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 76 ---------\n",
      "num_corrects / total_examples = 27659 / 36865\n",
      "training loss = 0.5258\n",
      "training accuracy = 0.7503\n",
      "num_test_corrects / test_total_examples = 6892 / 9217\n",
      "testing accuracy = 0.7477\n",
      "--------- epoch: 77 ---------\n",
      "num_corrects / total_examples = 27662 / 36865\n",
      "training loss = 0.5258\n",
      "training accuracy = 0.7504\n",
      "num_test_corrects / test_total_examples = 6892 / 9217\n",
      "testing accuracy = 0.7477\n",
      "--------- epoch: 78 ---------\n",
      "num_corrects / total_examples = 27664 / 36865\n",
      "training loss = 0.5258\n",
      "training accuracy = 0.7504\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 79 ---------\n",
      "num_corrects / total_examples = 27667 / 36865\n",
      "training loss = 0.5258\n",
      "training accuracy = 0.7505\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 80 ---------\n",
      "num_corrects / total_examples = 27669 / 36865\n",
      "training loss = 0.5258\n",
      "training accuracy = 0.7505\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 81 ---------\n",
      "num_corrects / total_examples = 27668 / 36865\n",
      "training loss = 0.5257\n",
      "training accuracy = 0.7505\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 82 ---------\n",
      "num_corrects / total_examples = 27669 / 36865\n",
      "training loss = 0.5257\n",
      "training accuracy = 0.7505\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 83 ---------\n",
      "num_corrects / total_examples = 27670 / 36865\n",
      "training loss = 0.5257\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 84 ---------\n",
      "num_corrects / total_examples = 27670 / 36865\n",
      "training loss = 0.5257\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 85 ---------\n",
      "num_corrects / total_examples = 27670 / 36865\n",
      "training loss = 0.5257\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 86 ---------\n",
      "num_corrects / total_examples = 27669 / 36865\n",
      "training loss = 0.5256\n",
      "training accuracy = 0.7505\n",
      "num_test_corrects / test_total_examples = 6890 / 9217\n",
      "testing accuracy = 0.7475\n",
      "--------- epoch: 87 ---------\n",
      "num_corrects / total_examples = 27668 / 36865\n",
      "training loss = 0.5256\n",
      "training accuracy = 0.7505\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 88 ---------\n",
      "num_corrects / total_examples = 27668 / 36865\n",
      "training loss = 0.5256\n",
      "training accuracy = 0.7505\n",
      "num_test_corrects / test_total_examples = 6889 / 9217\n",
      "testing accuracy = 0.7474\n",
      "--------- epoch: 89 ---------\n",
      "num_corrects / total_examples = 27668 / 36865\n",
      "training loss = 0.5256\n",
      "training accuracy = 0.7505\n",
      "num_test_corrects / test_total_examples = 6888 / 9217\n",
      "testing accuracy = 0.7473\n",
      "--------- epoch: 90 ---------\n",
      "num_corrects / total_examples = 27671 / 36865\n",
      "training loss = 0.5256\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6888 / 9217\n",
      "testing accuracy = 0.7473\n",
      "--------- epoch: 91 ---------\n",
      "num_corrects / total_examples = 27671 / 36865\n",
      "training loss = 0.5256\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6887 / 9217\n",
      "testing accuracy = 0.7472\n",
      "--------- epoch: 92 ---------\n",
      "num_corrects / total_examples = 27670 / 36865\n",
      "training loss = 0.5255\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6887 / 9217\n",
      "testing accuracy = 0.7472\n",
      "--------- epoch: 93 ---------\n",
      "num_corrects / total_examples = 27672 / 36865\n",
      "training loss = 0.5255\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6888 / 9217\n",
      "testing accuracy = 0.7473\n",
      "--------- epoch: 94 ---------\n",
      "num_corrects / total_examples = 27672 / 36865\n",
      "training loss = 0.5255\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6890 / 9217\n",
      "testing accuracy = 0.7475\n",
      "--------- epoch: 95 ---------\n",
      "num_corrects / total_examples = 27676 / 36865\n",
      "training loss = 0.5255\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6890 / 9217\n",
      "testing accuracy = 0.7475\n",
      "--------- epoch: 96 ---------\n",
      "num_corrects / total_examples = 27675 / 36865\n",
      "training loss = 0.5255\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 97 ---------\n",
      "num_corrects / total_examples = 27676 / 36865\n",
      "training loss = 0.5255\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6890 / 9217\n",
      "testing accuracy = 0.7475\n",
      "--------- epoch: 98 ---------\n",
      "num_corrects / total_examples = 27677 / 36865\n",
      "training loss = 0.5254\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6890 / 9217\n",
      "testing accuracy = 0.7475\n",
      "--------- epoch: 99 ---------\n",
      "num_corrects / total_examples = 27677 / 36865\n",
      "training loss = 0.5254\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6892 / 9217\n",
      "testing accuracy = 0.7477\n",
      "--------- epoch: 100 ---------\n",
      "num_corrects / total_examples = 27675 / 36865\n",
      "training loss = 0.5254\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "found best test accuracy at epoch 100\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 101 ---------\n",
      "num_corrects / total_examples = 27673 / 36865\n",
      "training loss = 0.5254\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 102 ---------\n",
      "num_corrects / total_examples = 27675 / 36865\n",
      "training loss = 0.5254\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "found best test accuracy at epoch 102\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 103 ---------\n",
      "num_corrects / total_examples = 27676 / 36865\n",
      "training loss = 0.5254\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 104 ---------\n",
      "num_corrects / total_examples = 27680 / 36865\n",
      "training loss = 0.5254\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "found best test accuracy at epoch 104\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 105 ---------\n",
      "num_corrects / total_examples = 27683 / 36865\n",
      "training loss = 0.5253\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 106 ---------\n",
      "num_corrects / total_examples = 27684 / 36865\n",
      "training loss = 0.5253\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 107 ---------\n",
      "num_corrects / total_examples = 27684 / 36865\n",
      "training loss = 0.5253\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 108 ---------\n",
      "num_corrects / total_examples = 27685 / 36865\n",
      "training loss = 0.5253\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 109 ---------\n",
      "num_corrects / total_examples = 27683 / 36865\n",
      "training loss = 0.5253\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 110 ---------\n",
      "num_corrects / total_examples = 27684 / 36865\n",
      "training loss = 0.5253\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 111 ---------\n",
      "num_corrects / total_examples = 27684 / 36865\n",
      "training loss = 0.5252\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 112 ---------\n",
      "num_corrects / total_examples = 27683 / 36865\n",
      "training loss = 0.5252\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 113 ---------\n",
      "num_corrects / total_examples = 27682 / 36865\n",
      "training loss = 0.5252\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 114 ---------\n",
      "num_corrects / total_examples = 27681 / 36865\n",
      "training loss = 0.5252\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 115 ---------\n",
      "num_corrects / total_examples = 27679 / 36865\n",
      "training loss = 0.5252\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 116 ---------\n",
      "num_corrects / total_examples = 27680 / 36865\n",
      "training loss = 0.5252\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 117 ---------\n",
      "num_corrects / total_examples = 27679 / 36865\n",
      "training loss = 0.5252\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6892 / 9217\n",
      "testing accuracy = 0.7477\n",
      "--------- epoch: 118 ---------\n",
      "num_corrects / total_examples = 27682 / 36865\n",
      "training loss = 0.5252\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6892 / 9217\n",
      "testing accuracy = 0.7477\n",
      "--------- epoch: 119 ---------\n",
      "num_corrects / total_examples = 27681 / 36865\n",
      "training loss = 0.5251\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 120 ---------\n",
      "num_corrects / total_examples = 27676 / 36865\n",
      "training loss = 0.5251\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6890 / 9217\n",
      "testing accuracy = 0.7475\n",
      "--------- epoch: 121 ---------\n",
      "num_corrects / total_examples = 27676 / 36865\n",
      "training loss = 0.5251\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6890 / 9217\n",
      "testing accuracy = 0.7475\n",
      "--------- epoch: 122 ---------\n",
      "num_corrects / total_examples = 27676 / 36865\n",
      "training loss = 0.5251\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 123 ---------\n",
      "num_corrects / total_examples = 27676 / 36865\n",
      "training loss = 0.5251\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 124 ---------\n",
      "num_corrects / total_examples = 27674 / 36865\n",
      "training loss = 0.5251\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 125 ---------\n",
      "num_corrects / total_examples = 27672 / 36865\n",
      "training loss = 0.5251\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6891 / 9217\n",
      "testing accuracy = 0.7476\n",
      "--------- epoch: 126 ---------\n",
      "num_corrects / total_examples = 27674 / 36865\n",
      "training loss = 0.5250\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 127 ---------\n",
      "num_corrects / total_examples = 27676 / 36865\n",
      "training loss = 0.5250\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 128 ---------\n",
      "num_corrects / total_examples = 27679 / 36865\n",
      "training loss = 0.5250\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 129 ---------\n",
      "num_corrects / total_examples = 27680 / 36865\n",
      "training loss = 0.5250\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 130 ---------\n",
      "num_corrects / total_examples = 27681 / 36865\n",
      "training loss = 0.5250\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 131 ---------\n",
      "num_corrects / total_examples = 27680 / 36865\n",
      "training loss = 0.5250\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 132 ---------\n",
      "num_corrects / total_examples = 27680 / 36865\n",
      "training loss = 0.5250\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 133 ---------\n",
      "num_corrects / total_examples = 27676 / 36865\n",
      "training loss = 0.5250\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 134 ---------\n",
      "num_corrects / total_examples = 27674 / 36865\n",
      "training loss = 0.5250\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 135 ---------\n",
      "num_corrects / total_examples = 27672 / 36865\n",
      "training loss = 0.5249\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "found best test accuracy at epoch 135\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 136 ---------\n",
      "num_corrects / total_examples = 27672 / 36865\n",
      "training loss = 0.5249\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 137 ---------\n",
      "num_corrects / total_examples = 27671 / 36865\n",
      "training loss = 0.5249\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 138 ---------\n",
      "num_corrects / total_examples = 27672 / 36865\n",
      "training loss = 0.5249\n",
      "training accuracy = 0.7506\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 139 ---------\n",
      "num_corrects / total_examples = 27673 / 36865\n",
      "training loss = 0.5249\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 140 ---------\n",
      "num_corrects / total_examples = 27675 / 36865\n",
      "training loss = 0.5249\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 141 ---------\n",
      "num_corrects / total_examples = 27675 / 36865\n",
      "training loss = 0.5249\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 142 ---------\n",
      "num_corrects / total_examples = 27675 / 36865\n",
      "training loss = 0.5249\n",
      "training accuracy = 0.7507\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 143 ---------\n",
      "num_corrects / total_examples = 27677 / 36865\n",
      "training loss = 0.5248\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 144 ---------\n",
      "num_corrects / total_examples = 27678 / 36865\n",
      "training loss = 0.5248\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 145 ---------\n",
      "num_corrects / total_examples = 27677 / 36865\n",
      "training loss = 0.5248\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 146 ---------\n",
      "num_corrects / total_examples = 27678 / 36865\n",
      "training loss = 0.5248\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 147 ---------\n",
      "num_corrects / total_examples = 27678 / 36865\n",
      "training loss = 0.5248\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "found best test accuracy at epoch 147\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 148 ---------\n",
      "num_corrects / total_examples = 27678 / 36865\n",
      "training loss = 0.5248\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "found best test accuracy at epoch 148\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 149 ---------\n",
      "num_corrects / total_examples = 27678 / 36865\n",
      "training loss = 0.5248\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 150 ---------\n",
      "num_corrects / total_examples = 27679 / 36865\n",
      "training loss = 0.5248\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 151 ---------\n",
      "num_corrects / total_examples = 27679 / 36865\n",
      "training loss = 0.5248\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 152 ---------\n",
      "num_corrects / total_examples = 27681 / 36865\n",
      "training loss = 0.5248\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 153 ---------\n",
      "num_corrects / total_examples = 27682 / 36865\n",
      "training loss = 0.5247\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 154 ---------\n",
      "num_corrects / total_examples = 27683 / 36865\n",
      "training loss = 0.5247\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 155 ---------\n",
      "num_corrects / total_examples = 27683 / 36865\n",
      "training loss = 0.5247\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 156 ---------\n",
      "num_corrects / total_examples = 27683 / 36865\n",
      "training loss = 0.5247\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 157 ---------\n",
      "num_corrects / total_examples = 27683 / 36865\n",
      "training loss = 0.5247\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 158 ---------\n",
      "num_corrects / total_examples = 27683 / 36865\n",
      "training loss = 0.5247\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 159 ---------\n",
      "num_corrects / total_examples = 27682 / 36865\n",
      "training loss = 0.5247\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 160 ---------\n",
      "num_corrects / total_examples = 27680 / 36865\n",
      "training loss = 0.5247\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 161 ---------\n",
      "num_corrects / total_examples = 27680 / 36865\n",
      "training loss = 0.5247\n",
      "training accuracy = 0.7508\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 162 ---------\n",
      "num_corrects / total_examples = 27682 / 36865\n",
      "training loss = 0.5247\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 163 ---------\n",
      "num_corrects / total_examples = 27682 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 164 ---------\n",
      "num_corrects / total_examples = 27681 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 165 ---------\n",
      "num_corrects / total_examples = 27683 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 166 ---------\n",
      "num_corrects / total_examples = 27682 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 167 ---------\n",
      "num_corrects / total_examples = 27682 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 168 ---------\n",
      "num_corrects / total_examples = 27681 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 169 ---------\n",
      "num_corrects / total_examples = 27681 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 170 ---------\n",
      "num_corrects / total_examples = 27683 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 171 ---------\n",
      "num_corrects / total_examples = 27682 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 172 ---------\n",
      "num_corrects / total_examples = 27682 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7509\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 173 ---------\n",
      "num_corrects / total_examples = 27684 / 36865\n",
      "training loss = 0.5246\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 174 ---------\n",
      "num_corrects / total_examples = 27685 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 175 ---------\n",
      "num_corrects / total_examples = 27686 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 176 ---------\n",
      "num_corrects / total_examples = 27685 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 177 ---------\n",
      "num_corrects / total_examples = 27685 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 178 ---------\n",
      "num_corrects / total_examples = 27686 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 179 ---------\n",
      "num_corrects / total_examples = 27686 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 180 ---------\n",
      "num_corrects / total_examples = 27686 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 181 ---------\n",
      "num_corrects / total_examples = 27686 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 182 ---------\n",
      "num_corrects / total_examples = 27687 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 183 ---------\n",
      "num_corrects / total_examples = 27689 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7511\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 184 ---------\n",
      "num_corrects / total_examples = 27690 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7511\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 185 ---------\n",
      "num_corrects / total_examples = 27689 / 36865\n",
      "training loss = 0.5245\n",
      "training accuracy = 0.7511\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 186 ---------\n",
      "num_corrects / total_examples = 27688 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7511\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 187 ---------\n",
      "num_corrects / total_examples = 27687 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7510\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 188 ---------\n",
      "num_corrects / total_examples = 27690 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7511\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 189 ---------\n",
      "num_corrects / total_examples = 27689 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7511\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 190 ---------\n",
      "num_corrects / total_examples = 27691 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7511\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 191 ---------\n",
      "num_corrects / total_examples = 27691 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7511\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 192 ---------\n",
      "num_corrects / total_examples = 27692 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 193 ---------\n",
      "num_corrects / total_examples = 27692 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 194 ---------\n",
      "num_corrects / total_examples = 27694 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 195 ---------\n",
      "num_corrects / total_examples = 27693 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 196 ---------\n",
      "num_corrects / total_examples = 27693 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 197 ---------\n",
      "num_corrects / total_examples = 27694 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 198 ---------\n",
      "num_corrects / total_examples = 27696 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7513\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 199 ---------\n",
      "num_corrects / total_examples = 27695 / 36865\n",
      "training loss = 0.5244\n",
      "training accuracy = 0.7513\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 200 ---------\n",
      "num_corrects / total_examples = 27695 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7513\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 201 ---------\n",
      "num_corrects / total_examples = 27696 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7513\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 202 ---------\n",
      "num_corrects / total_examples = 27698 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7513\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 203 ---------\n",
      "num_corrects / total_examples = 27696 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7513\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 204 ---------\n",
      "num_corrects / total_examples = 27696 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7513\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 205 ---------\n",
      "num_corrects / total_examples = 27695 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7513\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 206 ---------\n",
      "num_corrects / total_examples = 27693 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 207 ---------\n",
      "num_corrects / total_examples = 27692 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 208 ---------\n",
      "num_corrects / total_examples = 27693 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 209 ---------\n",
      "num_corrects / total_examples = 27693 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 210 ---------\n",
      "num_corrects / total_examples = 27692 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 211 ---------\n",
      "num_corrects / total_examples = 27692 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 212 ---------\n",
      "num_corrects / total_examples = 27691 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7511\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 213 ---------\n",
      "num_corrects / total_examples = 27692 / 36865\n",
      "training loss = 0.5243\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 214 ---------\n",
      "num_corrects / total_examples = 27693 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 215 ---------\n",
      "num_corrects / total_examples = 27694 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 216 ---------\n",
      "num_corrects / total_examples = 27693 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 217 ---------\n",
      "num_corrects / total_examples = 27693 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 218 ---------\n",
      "num_corrects / total_examples = 27694 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7512\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 219 ---------\n",
      "num_corrects / total_examples = 27696 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7513\n",
      "num_test_corrects / test_total_examples = 6895 / 9217\n",
      "testing accuracy = 0.7481\n",
      "--------- epoch: 220 ---------\n",
      "num_corrects / total_examples = 27700 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7514\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 221 ---------\n",
      "num_corrects / total_examples = 27699 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7514\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 222 ---------\n",
      "num_corrects / total_examples = 27697 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7513\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 223 ---------\n",
      "num_corrects / total_examples = 27699 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7514\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 224 ---------\n",
      "num_corrects / total_examples = 27699 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7514\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 225 ---------\n",
      "num_corrects / total_examples = 27702 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7514\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 226 ---------\n",
      "num_corrects / total_examples = 27702 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7514\n",
      "num_test_corrects / test_total_examples = 6893 / 9217\n",
      "testing accuracy = 0.7479\n",
      "--------- epoch: 227 ---------\n",
      "num_corrects / total_examples = 27704 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7515\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 228 ---------\n",
      "num_corrects / total_examples = 27705 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7515\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 229 ---------\n",
      "num_corrects / total_examples = 27704 / 36865\n",
      "training loss = 0.5242\n",
      "training accuracy = 0.7515\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 230 ---------\n",
      "num_corrects / total_examples = 27706 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7516\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 231 ---------\n",
      "num_corrects / total_examples = 27707 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7516\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 232 ---------\n",
      "num_corrects / total_examples = 27708 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7516\n",
      "num_test_corrects / test_total_examples = 6894 / 9217\n",
      "testing accuracy = 0.7480\n",
      "--------- epoch: 233 ---------\n",
      "num_corrects / total_examples = 27709 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7516\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 234 ---------\n",
      "num_corrects / total_examples = 27709 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7516\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 235 ---------\n",
      "num_corrects / total_examples = 27709 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7516\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 236 ---------\n",
      "num_corrects / total_examples = 27709 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7516\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 237 ---------\n",
      "num_corrects / total_examples = 27710 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7517\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 238 ---------\n",
      "num_corrects / total_examples = 27712 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7517\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 239 ---------\n",
      "num_corrects / total_examples = 27712 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7517\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 240 ---------\n",
      "num_corrects / total_examples = 27714 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 241 ---------\n",
      "num_corrects / total_examples = 27714 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 242 ---------\n",
      "num_corrects / total_examples = 27715 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 243 ---------\n",
      "num_corrects / total_examples = 27715 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 244 ---------\n",
      "num_corrects / total_examples = 27714 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 245 ---------\n",
      "num_corrects / total_examples = 27713 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7517\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 246 ---------\n",
      "num_corrects / total_examples = 27714 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 247 ---------\n",
      "num_corrects / total_examples = 27714 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 248 ---------\n",
      "num_corrects / total_examples = 27715 / 36865\n",
      "training loss = 0.5241\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 249 ---------\n",
      "num_corrects / total_examples = 27715 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 250 ---------\n",
      "num_corrects / total_examples = 27715 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 251 ---------\n",
      "num_corrects / total_examples = 27714 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 252 ---------\n",
      "num_corrects / total_examples = 27714 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 253 ---------\n",
      "num_corrects / total_examples = 27714 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 254 ---------\n",
      "num_corrects / total_examples = 27714 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 255 ---------\n",
      "num_corrects / total_examples = 27715 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 256 ---------\n",
      "num_corrects / total_examples = 27715 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7518\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 257 ---------\n",
      "num_corrects / total_examples = 27717 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7519\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 258 ---------\n",
      "num_corrects / total_examples = 27720 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7519\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 259 ---------\n",
      "num_corrects / total_examples = 27719 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7519\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 260 ---------\n",
      "num_corrects / total_examples = 27720 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7519\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 261 ---------\n",
      "num_corrects / total_examples = 27721 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7520\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 262 ---------\n",
      "num_corrects / total_examples = 27720 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7519\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 263 ---------\n",
      "num_corrects / total_examples = 27721 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7520\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 264 ---------\n",
      "num_corrects / total_examples = 27722 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7520\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 265 ---------\n",
      "num_corrects / total_examples = 27722 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7520\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 266 ---------\n",
      "num_corrects / total_examples = 27722 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7520\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 267 ---------\n",
      "num_corrects / total_examples = 27723 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7520\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 268 ---------\n",
      "num_corrects / total_examples = 27724 / 36865\n",
      "training loss = 0.5240\n",
      "training accuracy = 0.7520\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 269 ---------\n",
      "num_corrects / total_examples = 27725 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7521\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 270 ---------\n",
      "num_corrects / total_examples = 27725 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7521\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 271 ---------\n",
      "num_corrects / total_examples = 27725 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7521\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 272 ---------\n",
      "num_corrects / total_examples = 27725 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7521\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 273 ---------\n",
      "num_corrects / total_examples = 27726 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7521\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 274 ---------\n",
      "num_corrects / total_examples = 27727 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7521\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 275 ---------\n",
      "num_corrects / total_examples = 27728 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7521\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 276 ---------\n",
      "num_corrects / total_examples = 27728 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7521\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 277 ---------\n",
      "num_corrects / total_examples = 27730 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 278 ---------\n",
      "num_corrects / total_examples = 27729 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 279 ---------\n",
      "num_corrects / total_examples = 27728 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7521\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 280 ---------\n",
      "num_corrects / total_examples = 27730 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 281 ---------\n",
      "num_corrects / total_examples = 27731 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6896 / 9217\n",
      "testing accuracy = 0.7482\n",
      "--------- epoch: 282 ---------\n",
      "num_corrects / total_examples = 27731 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 283 ---------\n",
      "num_corrects / total_examples = 27730 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 284 ---------\n",
      "num_corrects / total_examples = 27729 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 285 ---------\n",
      "num_corrects / total_examples = 27729 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 286 ---------\n",
      "num_corrects / total_examples = 27729 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 287 ---------\n",
      "num_corrects / total_examples = 27729 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 288 ---------\n",
      "num_corrects / total_examples = 27730 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 289 ---------\n",
      "num_corrects / total_examples = 27731 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 290 ---------\n",
      "num_corrects / total_examples = 27731 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7522\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 291 ---------\n",
      "num_corrects / total_examples = 27732 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7523\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 292 ---------\n",
      "num_corrects / total_examples = 27732 / 36865\n",
      "training loss = 0.5239\n",
      "training accuracy = 0.7523\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 293 ---------\n",
      "num_corrects / total_examples = 27732 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7523\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 294 ---------\n",
      "num_corrects / total_examples = 27732 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7523\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 295 ---------\n",
      "num_corrects / total_examples = 27733 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7523\n",
      "num_test_corrects / test_total_examples = 6897 / 9217\n",
      "testing accuracy = 0.7483\n",
      "--------- epoch: 296 ---------\n",
      "num_corrects / total_examples = 27732 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7523\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 297 ---------\n",
      "num_corrects / total_examples = 27736 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7524\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 298 ---------\n",
      "num_corrects / total_examples = 27738 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7524\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 299 ---------\n",
      "num_corrects / total_examples = 27738 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7524\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 300 ---------\n",
      "num_corrects / total_examples = 27738 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7524\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 301 ---------\n",
      "num_corrects / total_examples = 27738 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7524\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 302 ---------\n",
      "num_corrects / total_examples = 27737 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7524\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 303 ---------\n",
      "num_corrects / total_examples = 27738 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7524\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 304 ---------\n",
      "num_corrects / total_examples = 27738 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7524\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 305 ---------\n",
      "num_corrects / total_examples = 27739 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7524\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 306 ---------\n",
      "num_corrects / total_examples = 27740 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7525\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 307 ---------\n",
      "num_corrects / total_examples = 27740 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7525\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 308 ---------\n",
      "num_corrects / total_examples = 27740 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7525\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 309 ---------\n",
      "num_corrects / total_examples = 27740 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7525\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 310 ---------\n",
      "num_corrects / total_examples = 27741 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7525\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 311 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6898 / 9217\n",
      "testing accuracy = 0.7484\n",
      "--------- epoch: 312 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 313 ---------\n",
      "num_corrects / total_examples = 27746 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 314 ---------\n",
      "num_corrects / total_examples = 27748 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6899 / 9217\n",
      "testing accuracy = 0.7485\n",
      "--------- epoch: 315 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6900 / 9217\n",
      "testing accuracy = 0.7486\n",
      "found best test accuracy at epoch 315\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 316 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6900 / 9217\n",
      "testing accuracy = 0.7486\n",
      "--------- epoch: 317 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "found best test accuracy at epoch 317\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 318 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6900 / 9217\n",
      "testing accuracy = 0.7486\n",
      "--------- epoch: 319 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5238\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6900 / 9217\n",
      "testing accuracy = 0.7486\n",
      "--------- epoch: 320 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6900 / 9217\n",
      "testing accuracy = 0.7486\n",
      "--------- epoch: 321 ---------\n",
      "num_corrects / total_examples = 27748 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6900 / 9217\n",
      "testing accuracy = 0.7486\n",
      "--------- epoch: 322 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 323 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 324 ---------\n",
      "num_corrects / total_examples = 27746 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 325 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 326 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 327 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6900 / 9217\n",
      "testing accuracy = 0.7486\n",
      "--------- epoch: 328 ---------\n",
      "num_corrects / total_examples = 27743 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 329 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 330 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 331 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 332 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 333 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6901 / 9217\n",
      "testing accuracy = 0.7487\n",
      "--------- epoch: 334 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6902 / 9217\n",
      "testing accuracy = 0.7488\n",
      "found best test accuracy at epoch 334\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 335 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6902 / 9217\n",
      "testing accuracy = 0.7488\n",
      "--------- epoch: 336 ---------\n",
      "num_corrects / total_examples = 27743 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6902 / 9217\n",
      "testing accuracy = 0.7488\n",
      "--------- epoch: 337 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6902 / 9217\n",
      "testing accuracy = 0.7488\n",
      "--------- epoch: 338 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6902 / 9217\n",
      "testing accuracy = 0.7488\n",
      "--------- epoch: 339 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6902 / 9217\n",
      "testing accuracy = 0.7488\n",
      "--------- epoch: 340 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6902 / 9217\n",
      "testing accuracy = 0.7488\n",
      "--------- epoch: 341 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6902 / 9217\n",
      "testing accuracy = 0.7488\n",
      "--------- epoch: 342 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "found best test accuracy at epoch 342\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 343 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 344 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 345 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 346 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 347 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 348 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 349 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 350 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 351 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 352 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5237\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 353 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 354 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6903 / 9217\n",
      "testing accuracy = 0.7489\n",
      "--------- epoch: 355 ---------\n",
      "num_corrects / total_examples = 27746 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "found best test accuracy at epoch 355\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 356 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 357 ---------\n",
      "num_corrects / total_examples = 27746 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 358 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 359 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 360 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6905 / 9217\n",
      "testing accuracy = 0.7492\n",
      "found best test accuracy at epoch 360\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 361 ---------\n",
      "num_corrects / total_examples = 27746 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6905 / 9217\n",
      "testing accuracy = 0.7492\n",
      "--------- epoch: 362 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6905 / 9217\n",
      "testing accuracy = 0.7492\n",
      "--------- epoch: 363 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6905 / 9217\n",
      "testing accuracy = 0.7492\n",
      "--------- epoch: 364 ---------\n",
      "num_corrects / total_examples = 27743 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 365 ---------\n",
      "num_corrects / total_examples = 27743 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 366 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 367 ---------\n",
      "num_corrects / total_examples = 27744 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 368 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 369 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 370 ---------\n",
      "num_corrects / total_examples = 27746 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 371 ---------\n",
      "num_corrects / total_examples = 27746 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 372 ---------\n",
      "num_corrects / total_examples = 27746 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 373 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 374 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6904 / 9217\n",
      "testing accuracy = 0.7491\n",
      "--------- epoch: 375 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6905 / 9217\n",
      "testing accuracy = 0.7492\n",
      "--------- epoch: 376 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6905 / 9217\n",
      "testing accuracy = 0.7492\n",
      "--------- epoch: 377 ---------\n",
      "num_corrects / total_examples = 27748 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6906 / 9217\n",
      "testing accuracy = 0.7493\n",
      "found best test accuracy at epoch 377\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 378 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6906 / 9217\n",
      "testing accuracy = 0.7493\n",
      "--------- epoch: 379 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6906 / 9217\n",
      "testing accuracy = 0.7493\n",
      "--------- epoch: 380 ---------\n",
      "num_corrects / total_examples = 27748 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6906 / 9217\n",
      "testing accuracy = 0.7493\n",
      "--------- epoch: 381 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "found best test accuracy at epoch 381\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 382 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 383 ---------\n",
      "num_corrects / total_examples = 27746 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 384 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "found best test accuracy at epoch 384\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 385 ---------\n",
      "num_corrects / total_examples = 27745 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7526\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 386 ---------\n",
      "num_corrects / total_examples = 27747 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 387 ---------\n",
      "num_corrects / total_examples = 27748 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 388 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 389 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 390 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 391 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 392 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 393 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5236\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 394 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 395 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 396 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 397 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 398 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 399 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 400 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 401 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 402 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 403 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 404 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 405 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 406 ---------\n",
      "num_corrects / total_examples = 27748 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 407 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 408 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 409 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 410 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 411 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 412 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 413 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 414 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 415 ---------\n",
      "num_corrects / total_examples = 27748 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 416 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 417 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 418 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 419 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6907 / 9217\n",
      "testing accuracy = 0.7494\n",
      "--------- epoch: 420 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 421 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6908 / 9217\n",
      "testing accuracy = 0.7495\n",
      "--------- epoch: 422 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6909 / 9217\n",
      "testing accuracy = 0.7496\n",
      "found best test accuracy at epoch 422\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 423 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6909 / 9217\n",
      "testing accuracy = 0.7496\n",
      "--------- epoch: 424 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6910 / 9217\n",
      "testing accuracy = 0.7497\n",
      "found best test accuracy at epoch 424\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 425 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6910 / 9217\n",
      "testing accuracy = 0.7497\n",
      "--------- epoch: 426 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6910 / 9217\n",
      "testing accuracy = 0.7497\n",
      "--------- epoch: 427 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6910 / 9217\n",
      "testing accuracy = 0.7497\n",
      "--------- epoch: 428 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6910 / 9217\n",
      "testing accuracy = 0.7497\n",
      "--------- epoch: 429 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6910 / 9217\n",
      "testing accuracy = 0.7497\n",
      "--------- epoch: 430 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6910 / 9217\n",
      "testing accuracy = 0.7497\n",
      "--------- epoch: 431 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6910 / 9217\n",
      "testing accuracy = 0.7497\n",
      "--------- epoch: 432 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "found best test accuracy at epoch 432\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 433 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 434 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 435 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 436 ---------\n",
      "num_corrects / total_examples = 27748 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6910 / 9217\n",
      "testing accuracy = 0.7497\n",
      "--------- epoch: 437 ---------\n",
      "num_corrects / total_examples = 27748 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 438 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 439 ---------\n",
      "num_corrects / total_examples = 27748 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 440 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 441 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 442 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 443 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6913 / 9217\n",
      "testing accuracy = 0.7500\n",
      "found best test accuracy at epoch 443\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 444 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5235\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6913 / 9217\n",
      "testing accuracy = 0.7500\n",
      "--------- epoch: 445 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6913 / 9217\n",
      "testing accuracy = 0.7500\n",
      "--------- epoch: 446 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6913 / 9217\n",
      "testing accuracy = 0.7500\n",
      "--------- epoch: 447 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6913 / 9217\n",
      "testing accuracy = 0.7500\n",
      "--------- epoch: 448 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6913 / 9217\n",
      "testing accuracy = 0.7500\n",
      "--------- epoch: 449 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6913 / 9217\n",
      "testing accuracy = 0.7500\n",
      "--------- epoch: 450 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6913 / 9217\n",
      "testing accuracy = 0.7500\n",
      "--------- epoch: 451 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6913 / 9217\n",
      "testing accuracy = 0.7500\n",
      "--------- epoch: 452 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6913 / 9217\n",
      "testing accuracy = 0.7500\n",
      "--------- epoch: 453 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 454 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 455 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 456 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 457 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 458 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 459 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 460 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 461 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 462 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 463 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 464 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 465 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 466 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6911 / 9217\n",
      "testing accuracy = 0.7498\n",
      "--------- epoch: 467 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 468 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 469 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 470 ---------\n",
      "num_corrects / total_examples = 27756 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 471 ---------\n",
      "num_corrects / total_examples = 27755 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 472 ---------\n",
      "num_corrects / total_examples = 27755 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 473 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6912 / 9217\n",
      "testing accuracy = 0.7499\n",
      "--------- epoch: 474 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6914 / 9217\n",
      "testing accuracy = 0.7501\n",
      "found best test accuracy at epoch 474\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 475 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6914 / 9217\n",
      "testing accuracy = 0.7501\n",
      "--------- epoch: 476 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6914 / 9217\n",
      "testing accuracy = 0.7501\n",
      "--------- epoch: 477 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6914 / 9217\n",
      "testing accuracy = 0.7501\n",
      "--------- epoch: 478 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "found best test accuracy at epoch 478\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 479 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 480 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 481 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 482 ---------\n",
      "num_corrects / total_examples = 27753 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 483 ---------\n",
      "num_corrects / total_examples = 27754 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7529\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 484 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 485 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 486 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6915 / 9217\n",
      "testing accuracy = 0.7502\n",
      "--------- epoch: 487 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "found best test accuracy at epoch 487\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 488 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 489 ---------\n",
      "num_corrects / total_examples = 27750 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 490 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 491 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6916 / 9217\n",
      "testing accuracy = 0.7504\n",
      "--------- epoch: 492 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "found best test accuracy at epoch 492\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 493 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 494 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 495 ---------\n",
      "num_corrects / total_examples = 27752 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6917 / 9217\n",
      "testing accuracy = 0.7505\n",
      "--------- epoch: 496 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "found best test accuracy at epoch 496\n",
      "save the model checkpoint to /home/dk/Desktop/projects/PocketHHE/weights/hypnogram/lc_model_scaled_hypnogram.pt\n",
      "--------- epoch: 497 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 498 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 499 ---------\n",
      "num_corrects / total_examples = 27751 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7528\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n",
      "--------- epoch: 500 ---------\n",
      "num_corrects / total_examples = 27749 / 36865\n",
      "training loss = 0.5234\n",
      "training accuracy = 0.7527\n",
      "num_test_corrects / test_total_examples = 6918 / 9217\n",
      "testing accuracy = 0.7506\n"
     ]
    }
   ],
   "source": [
    "save_weight_path = project_path / 'weights' / 'hypnogram' / 'lc_model_scaled_hypnogram.pt'\n",
    "train_losses_2, train_accuracies_2, test_accuracies_2 = train(lc_model, \n",
    "                                                              train_loader_scaled, \n",
    "                                                              test_loader_scaled,\n",
    "                                                              save_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd300747520>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd3007bc6a0>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_accuracies_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd30069dca0>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_accuracies_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max train accuracy = 0.752909243106842\n",
      "max test accuracy = 0.7505695996528154\n"
     ]
    }
   ],
   "source": [
    "print(f\"max train accuracy = {max(train_accuracies_2)}\")\n",
    "print(f\"max test accuracy = {max(test_accuracies_2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Pytorch Lightning - Experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(300, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.criterion = torch.nn.BCELoss()\n",
    "        self.training_step_outputs = []\n",
    "        self.val_step_outputs = []\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = train_batch\n",
    "        y_hat = self.sigmoid(self.linear(x))\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.training_step_outputs.append(y_hat.round() == y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        train_preds = torch.cat(self.training_step_outputs, dim=0)\n",
    "        train_acc = torch.sum(train_preds) / train_preds.shape[0]\n",
    "        self.log(\"train_acc\", train_acc)\n",
    "        print(f\"train acc = {train_acc}\")\n",
    "        self.training_step_outputs.clear()\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        y_hat = self.sigmoid(self.linear(x))\n",
    "        self.val_step_outputs.append(y_hat.round() == y)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        val_preds = torch.cat(self.val_step_outputs, dim=0)\n",
    "        val_acc = torch.sum(val_preds) / val_preds.shape[0]\n",
    "        self.log(\"val_acc\", val_acc)\n",
    "        print(f\"val acc = {val_acc}\")\n",
    "        self.val_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "lr_model = LinearModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | linear    | Linear  | 301   \n",
      "1 | sigmoid   | Sigmoid | 0     \n",
      "2 | criterion | BCELoss | 0     \n",
      "--------------------------------------\n",
      "301       Trainable params\n",
      "0         Non-trainable params\n",
      "301       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|| 2/2 [00:00<00:00, 109.46it/s]val acc = 0.75\n",
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dk/miniconda3/envs/pockethhe/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 100/100 [00:00<00:00, 282.40it/s, v_num=2]val acc = 0.7208419442176819\n",
      "Epoch 0: 100%|| 100/100 [00:06<00:00, 14.66it/s, v_num=2] train acc = 0.6074999570846558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 100/100 [00:06<00:00, 14.64it/s, v_num=2]\n"
     ]
    }
   ],
   "source": [
    "# test run on 100 data batches and 1 epoch\n",
    "trainer = pl.Trainer(limit_train_batches=100, max_epochs=1)\n",
    "trainer.fit(model=lr_model, train_dataloaders=train_loader, val_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | linear    | Linear  | 301   \n",
      "1 | sigmoid   | Sigmoid | 0     \n",
      "2 | criterion | BCELoss | 0     \n",
      "--------------------------------------\n",
      "301       Trainable params\n",
      "0         Non-trainable params\n",
      "301       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|| 2/2 [00:00<00:00, 449.29it/s]val acc = 0.625\n",
      "Epoch 0: 100%|| 9217/9217 [00:31<00:00, 292.78it/s, v_num=4]      val acc = 0.6982749104499817\n",
      "Epoch 0: 100%|| 9217/9217 [00:37<00:00, 244.25it/s, v_num=4]train acc = 0.6211311221122742\n",
      "Epoch 1: 100%|| 9217/9217 [00:30<00:00, 298.31it/s, v_num=4]val acc = 0.6988174319267273\n",
      "Epoch 1: 100%|| 9217/9217 [00:37<00:00, 248.06it/s, v_num=4]train acc = 0.6261494755744934\n",
      "Epoch 2: 100%|| 9217/9217 [00:34<00:00, 263.58it/s, v_num=4]val acc = 0.6992514133453369\n",
      "Epoch 2: 100%|| 9217/9217 [00:41<00:00, 221.85it/s, v_num=4]train acc = 0.6296486854553223\n",
      "Epoch 3: 100%|| 9217/9217 [00:36<00:00, 251.53it/s, v_num=4]val acc = 0.7000108361244202\n",
      "Epoch 3: 100%|| 9217/9217 [00:43<00:00, 212.66it/s, v_num=4]train acc = 0.6354536414146423\n",
      "Epoch 4: 100%|| 9217/9217 [00:36<00:00, 255.47it/s, v_num=4]val acc = 0.7004448175430298\n",
      "Epoch 4: 100%|| 9217/9217 [00:41<00:00, 220.83it/s, v_num=4]train acc = 0.6374338865280151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|| 9217/9217 [00:41<00:00, 220.68it/s, v_num=4]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "trainer = pl.Trainer(max_epochs=num_epochs)\n",
    "trainer.fit(model=lr_model, train_dataloaders=train_loader, val_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hesplitnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
