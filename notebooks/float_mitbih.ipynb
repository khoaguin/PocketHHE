{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification on the MIT-BIH Arrhythmia Dataset \n",
    "*Train a Float Neural Net on Float Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.8.1+cu102\n"
     ]
    }
   ],
   "source": [
    "print(f'torch version: {torch.__version__}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needed paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = Path.cwd().parent\n",
    "project_path\n",
    "train_path = project_path / 'data' / 'mit-bih' / 'mitbih_train.hdf5'\n",
    "test_path = project_path / 'data' / 'mit-bih' / 'mitbih_test.hdf5'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data into `.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13245, 128)\n"
     ]
    }
   ],
   "source": [
    "csv_test = project_path / 'data' / 'mit-bih' / 'csv' / 'mitbih_x_test_float.csv'\n",
    "\n",
    "with h5py.File(test_path, 'r') as hdf:\n",
    "    x = hdf['x_test'][:]\n",
    "    x = np.squeeze(x, axis=1)\n",
    "    np.savetxt(csv_test, x, delimiter=\",\")\n",
    "    print(x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryECG(Dataset):\n",
    "    \"\"\"The class used by the client to load the dataset\n",
    "\n",
    "    Args:\n",
    "        Dataset ([type]): [description]\n",
    "    \"\"\"\n",
    "    def __init__(self, train_path: Path, test_path: Path, train=True):\n",
    "        if train:\n",
    "            with h5py.File(train_path, 'r') as hdf:\n",
    "                self.x = hdf['x_train'][:]\n",
    "                self.y = hdf['y_train'][:]\n",
    "                self.binary_y = deepcopy(self.y)  # for binary classification\n",
    "                self.binary_y[self.binary_y > 0] = 1\n",
    "        else:\n",
    "            with h5py.File(test_path, 'r') as hdf:\n",
    "                self.x = hdf['x_test'][:]\n",
    "                self.y = hdf['y_test'][:]\n",
    "                self.binary_y = deepcopy(self.y)\n",
    "                self.binary_y[self.binary_y > 0] = 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.x[idx], dtype=torch.float), torch.tensor(self.binary_y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_dataset = BinaryECG(train=True, train_path=train_path, test_path=test_path)\n",
    "test_dataset = BinaryECG(train=False, train_path=train_path, test_path=test_path)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13245, 1, 128)\n",
      "(13245,)\n",
      "(13245, 1, 128)\n",
      "(13245,)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.x.shape)\n",
    "print(train_dataset.y.shape)\n",
    "print(test_dataset.x.shape)\n",
    "print(test_dataset.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3681e8f580>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x0 = train_dataset.x[0]\n",
    "print(x0.shape)\n",
    "plt.plot(x0[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47613414, 0.5020567 , 0.50298215, 0.46231327, 0.41928844,\n",
       "        0.41842108, 0.43061455, 0.44688531, 0.46408802, 0.47293354,\n",
       "        0.47899632, 0.48582534, 0.48893095, 0.48258178, 0.47326914,\n",
       "        0.46113493, 0.455493  , 0.47573456, 0.49812282, 0.50142268,\n",
       "        0.49909754, 0.4960816 , 0.49134992, 0.4847633 , 0.47915717,\n",
       "        0.47762139, 0.47941395, 0.48882176, 0.49701847, 0.49347912,\n",
       "        0.48651553, 0.48023663, 0.47407637, 0.46977147, 0.46597436,\n",
       "        0.46211728, 0.45808918, 0.45318487, 0.44824447, 0.44322334,\n",
       "        0.43951277, 0.43997245, 0.4410783 , 0.44093789, 0.43854472,\n",
       "        0.42746798, 0.41866831, 0.42362392, 0.43467756, 0.45355196,\n",
       "        0.46949916, 0.46741965, 0.45934733, 0.45182872, 0.44276687,\n",
       "        0.42654805, 0.41805189, 0.43117256, 0.46263137, 0.53157473,\n",
       "        0.59875968, 0.61784347, 0.62479582, 0.63498664, 0.65088522,\n",
       "        0.67939663, 0.6717122 , 0.53892088, 0.38295176, 0.27504905,\n",
       "        0.19901058, 0.18376322, 0.19361621, 0.21105314, 0.22193609,\n",
       "        0.18308223, 0.14579817, 0.15643591, 0.18355273, 0.21700833,\n",
       "        0.25267785, 0.27851548, 0.30314989, 0.33109226, 0.3600533 ,\n",
       "        0.39449314, 0.42292988, 0.43165459, 0.43040441, 0.4204936 ,\n",
       "        0.40817158, 0.39964169, 0.3963274 , 0.40884301, 0.42161387,\n",
       "        0.41734978, 0.4107301 , 0.41234156, 0.41459165, 0.41121749,\n",
       "        0.40965522, 0.42157927, 0.43068396, 0.41869718, 0.40195289,\n",
       "        0.38904382, 0.38004101, 0.38158553, 0.38670215, 0.39322759,\n",
       "        0.39640089, 0.38446916, 0.37183684, 0.37240077, 0.37561081,\n",
       "        0.37670637, 0.37586566, 0.36948463, 0.36176488, 0.35456541,\n",
       "        0.34983307, 0.35228016, 0.35721287, 0.36245956, 0.36675617,\n",
       "        0.36668541, 0.36517786, 0.36366678]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset.binary_y = array([1, 1, 1, ..., 1, 1, 0])\n",
      "train_dataset.y = array([2, 4, 2, ..., 1, 4, 0])\n",
      "there are 3000 normal beats and 10245 abnormal beats in the train dataset\n",
      "there are 3000 normal beats and 10245 abnormal beats in the train dataset\n"
     ]
    }
   ],
   "source": [
    "print(f\"{train_dataset.binary_y = }\")\n",
    "print(f\"{train_dataset.y = }\")\n",
    "\n",
    "unique, train_counts = np.unique(train_dataset.binary_y, return_counts=True)\n",
    "print(f\"there are {train_counts[0]} normal beats and {train_counts[1]} abnormal beats in the train dataset\")\n",
    "unique, test_counts = np.unique(test_dataset.binary_y, return_counts=True)\n",
    "print(f\"there are {test_counts[0]} normal beats and {test_counts[1]} abnormal beats in the train dataset\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_size, output_size)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        y_pred = self.sigmoid(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters, random seed and the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 500 \n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: NVIDIA GeForce RTX 2060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearModel(\n",
       "  (linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'device: {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "lr_model = LinearModel(128, 1)\n",
    "lr_model.to(device)\n",
    "lr_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model's parameters before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0007,  0.0474, -0.0727, -0.0650, -0.0340,  0.0237, -0.0018,  0.0701,\n",
      "         -0.0078,  0.0234, -0.0267, -0.0174, -0.0844, -0.0585, -0.0364,  0.0033,\n",
      "          0.0349,  0.0530, -0.0599, -0.0385,  0.0321,  0.0734, -0.0182,  0.0661,\n",
      "         -0.0142,  0.0094,  0.0800, -0.0820, -0.0556, -0.0224, -0.0345,  0.0764,\n",
      "         -0.0573, -0.0407, -0.0618, -0.0828, -0.0516,  0.0760,  0.0394,  0.0428,\n",
      "          0.0046, -0.0453,  0.0150, -0.0825, -0.0639, -0.0456,  0.0558,  0.0518,\n",
      "         -0.0392, -0.0032,  0.0565,  0.0879,  0.0351,  0.0119,  0.0593, -0.0520,\n",
      "          0.0165, -0.0685, -0.0613, -0.0457,  0.0400,  0.0355, -0.0524,  0.0267,\n",
      "          0.0485, -0.0112,  0.0034,  0.0205,  0.0548,  0.0849, -0.0681, -0.0324,\n",
      "          0.0347,  0.0732,  0.0769,  0.0780,  0.0176, -0.0769,  0.0081, -0.0553,\n",
      "         -0.0824,  0.0785,  0.0672, -0.0882,  0.0165, -0.0149, -0.0145, -0.0405,\n",
      "          0.0340, -0.0524,  0.0324,  0.0447,  0.0633,  0.0330, -0.0875, -0.0573,\n",
      "          0.0441,  0.0185, -0.0690, -0.0509,  0.0832,  0.0596, -0.0385, -0.0222,\n",
      "         -0.0842, -0.0016, -0.0666, -0.0682, -0.0049,  0.0133, -0.0362,  0.0524,\n",
      "         -0.0538,  0.0802,  0.0606, -0.0745, -0.0220,  0.0040,  0.0129,  0.0210,\n",
      "          0.0347,  0.0053, -0.0431,  0.0418, -0.0848, -0.0524, -0.0221, -0.0431]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0309], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in lr_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- epoch: 1 ---------\n",
      "num_corrects / total_examples = 10253 / 13245\n",
      "training loss = 0.4937\n",
      "training accuracy = 0.7741\n",
      "num_test_corrects / test_total_examples = 10251 / 13245\n",
      "testing accuracy = 0.7740\n",
      "--------- epoch: 2 ---------\n",
      "num_corrects / total_examples = 10283 / 13245\n",
      "training loss = 0.4755\n",
      "training accuracy = 0.7764\n",
      "num_test_corrects / test_total_examples = 10275 / 13245\n",
      "testing accuracy = 0.7758\n",
      "--------- epoch: 3 ---------\n",
      "num_corrects / total_examples = 10311 / 13245\n",
      "training loss = 0.4626\n",
      "training accuracy = 0.7785\n",
      "num_test_corrects / test_total_examples = 10274 / 13245\n",
      "testing accuracy = 0.7757\n",
      "--------- epoch: 4 ---------\n",
      "num_corrects / total_examples = 10325 / 13245\n",
      "training loss = 0.4529\n",
      "training accuracy = 0.7795\n",
      "num_test_corrects / test_total_examples = 10307 / 13245\n",
      "testing accuracy = 0.7782\n",
      "--------- epoch: 5 ---------\n",
      "num_corrects / total_examples = 10373 / 13245\n",
      "training loss = 0.4452\n",
      "training accuracy = 0.7832\n",
      "num_test_corrects / test_total_examples = 10371 / 13245\n",
      "testing accuracy = 0.7830\n",
      "--------- epoch: 6 ---------\n",
      "num_corrects / total_examples = 10443 / 13245\n",
      "training loss = 0.4389\n",
      "training accuracy = 0.7884\n",
      "num_test_corrects / test_total_examples = 10465 / 13245\n",
      "testing accuracy = 0.7901\n",
      "--------- epoch: 7 ---------\n",
      "num_corrects / total_examples = 10510 / 13245\n",
      "training loss = 0.4336\n",
      "training accuracy = 0.7935\n",
      "num_test_corrects / test_total_examples = 10551 / 13245\n",
      "testing accuracy = 0.7966\n",
      "--------- epoch: 8 ---------\n",
      "num_corrects / total_examples = 10576 / 13245\n",
      "training loss = 0.4289\n",
      "training accuracy = 0.7985\n",
      "num_test_corrects / test_total_examples = 10597 / 13245\n",
      "testing accuracy = 0.8001\n",
      "--------- epoch: 9 ---------\n",
      "num_corrects / total_examples = 10636 / 13245\n",
      "training loss = 0.4248\n",
      "training accuracy = 0.8030\n",
      "num_test_corrects / test_total_examples = 10639 / 13245\n",
      "testing accuracy = 0.8032\n",
      "--------- epoch: 10 ---------\n",
      "num_corrects / total_examples = 10689 / 13245\n",
      "training loss = 0.4211\n",
      "training accuracy = 0.8070\n",
      "num_test_corrects / test_total_examples = 10689 / 13245\n",
      "testing accuracy = 0.8070\n",
      "--------- epoch: 11 ---------\n",
      "num_corrects / total_examples = 10729 / 13245\n",
      "training loss = 0.4178\n",
      "training accuracy = 0.8100\n",
      "num_test_corrects / test_total_examples = 10733 / 13245\n",
      "testing accuracy = 0.8103\n",
      "--------- epoch: 12 ---------\n",
      "num_corrects / total_examples = 10764 / 13245\n",
      "training loss = 0.4147\n",
      "training accuracy = 0.8127\n",
      "num_test_corrects / test_total_examples = 10776 / 13245\n",
      "testing accuracy = 0.8136\n",
      "--------- epoch: 13 ---------\n",
      "num_corrects / total_examples = 10813 / 13245\n",
      "training loss = 0.4119\n",
      "training accuracy = 0.8164\n",
      "num_test_corrects / test_total_examples = 10813 / 13245\n",
      "testing accuracy = 0.8164\n",
      "--------- epoch: 14 ---------\n",
      "num_corrects / total_examples = 10840 / 13245\n",
      "training loss = 0.4093\n",
      "training accuracy = 0.8184\n",
      "num_test_corrects / test_total_examples = 10848 / 13245\n",
      "testing accuracy = 0.8190\n",
      "--------- epoch: 15 ---------\n",
      "num_corrects / total_examples = 10877 / 13245\n",
      "training loss = 0.4068\n",
      "training accuracy = 0.8212\n",
      "num_test_corrects / test_total_examples = 10875 / 13245\n",
      "testing accuracy = 0.8211\n",
      "--------- epoch: 16 ---------\n",
      "num_corrects / total_examples = 10895 / 13245\n",
      "training loss = 0.4045\n",
      "training accuracy = 0.8226\n",
      "num_test_corrects / test_total_examples = 10897 / 13245\n",
      "testing accuracy = 0.8227\n",
      "--------- epoch: 17 ---------\n",
      "num_corrects / total_examples = 10919 / 13245\n",
      "training loss = 0.4023\n",
      "training accuracy = 0.8244\n",
      "num_test_corrects / test_total_examples = 10939 / 13245\n",
      "testing accuracy = 0.8259\n",
      "--------- epoch: 18 ---------\n",
      "num_corrects / total_examples = 10942 / 13245\n",
      "training loss = 0.4002\n",
      "training accuracy = 0.8261\n",
      "num_test_corrects / test_total_examples = 10970 / 13245\n",
      "testing accuracy = 0.8282\n",
      "--------- epoch: 19 ---------\n",
      "num_corrects / total_examples = 10975 / 13245\n",
      "training loss = 0.3983\n",
      "training accuracy = 0.8286\n",
      "num_test_corrects / test_total_examples = 10992 / 13245\n",
      "testing accuracy = 0.8299\n",
      "--------- epoch: 20 ---------\n",
      "num_corrects / total_examples = 10994 / 13245\n",
      "training loss = 0.3965\n",
      "training accuracy = 0.8300\n",
      "num_test_corrects / test_total_examples = 11010 / 13245\n",
      "testing accuracy = 0.8313\n",
      "--------- epoch: 21 ---------\n",
      "num_corrects / total_examples = 11012 / 13245\n",
      "training loss = 0.3947\n",
      "training accuracy = 0.8314\n",
      "num_test_corrects / test_total_examples = 11026 / 13245\n",
      "testing accuracy = 0.8325\n",
      "--------- epoch: 22 ---------\n",
      "num_corrects / total_examples = 11033 / 13245\n",
      "training loss = 0.3930\n",
      "training accuracy = 0.8330\n",
      "num_test_corrects / test_total_examples = 11044 / 13245\n",
      "testing accuracy = 0.8338\n",
      "--------- epoch: 23 ---------\n",
      "num_corrects / total_examples = 11050 / 13245\n",
      "training loss = 0.3914\n",
      "training accuracy = 0.8343\n",
      "num_test_corrects / test_total_examples = 11060 / 13245\n",
      "testing accuracy = 0.8350\n",
      "--------- epoch: 24 ---------\n",
      "num_corrects / total_examples = 11064 / 13245\n",
      "training loss = 0.3899\n",
      "training accuracy = 0.8353\n",
      "num_test_corrects / test_total_examples = 11075 / 13245\n",
      "testing accuracy = 0.8362\n",
      "--------- epoch: 25 ---------\n",
      "num_corrects / total_examples = 11077 / 13245\n",
      "training loss = 0.3884\n",
      "training accuracy = 0.8363\n",
      "num_test_corrects / test_total_examples = 11093 / 13245\n",
      "testing accuracy = 0.8375\n",
      "--------- epoch: 26 ---------\n",
      "num_corrects / total_examples = 11093 / 13245\n",
      "training loss = 0.3870\n",
      "training accuracy = 0.8375\n",
      "num_test_corrects / test_total_examples = 11108 / 13245\n",
      "testing accuracy = 0.8387\n",
      "--------- epoch: 27 ---------\n",
      "num_corrects / total_examples = 11104 / 13245\n",
      "training loss = 0.3857\n",
      "training accuracy = 0.8384\n",
      "num_test_corrects / test_total_examples = 11124 / 13245\n",
      "testing accuracy = 0.8399\n",
      "--------- epoch: 28 ---------\n",
      "num_corrects / total_examples = 11118 / 13245\n",
      "training loss = 0.3844\n",
      "training accuracy = 0.8394\n",
      "num_test_corrects / test_total_examples = 11139 / 13245\n",
      "testing accuracy = 0.8410\n",
      "--------- epoch: 29 ---------\n",
      "num_corrects / total_examples = 11130 / 13245\n",
      "training loss = 0.3831\n",
      "training accuracy = 0.8403\n",
      "num_test_corrects / test_total_examples = 11152 / 13245\n",
      "testing accuracy = 0.8420\n",
      "--------- epoch: 30 ---------\n",
      "num_corrects / total_examples = 11149 / 13245\n",
      "training loss = 0.3819\n",
      "training accuracy = 0.8418\n",
      "num_test_corrects / test_total_examples = 11167 / 13245\n",
      "testing accuracy = 0.8431\n",
      "--------- epoch: 31 ---------\n",
      "num_corrects / total_examples = 11157 / 13245\n",
      "training loss = 0.3808\n",
      "training accuracy = 0.8424\n",
      "num_test_corrects / test_total_examples = 11172 / 13245\n",
      "testing accuracy = 0.8435\n",
      "--------- epoch: 32 ---------\n",
      "num_corrects / total_examples = 11168 / 13245\n",
      "training loss = 0.3796\n",
      "training accuracy = 0.8432\n",
      "num_test_corrects / test_total_examples = 11179 / 13245\n",
      "testing accuracy = 0.8440\n",
      "--------- epoch: 33 ---------\n",
      "num_corrects / total_examples = 11177 / 13245\n",
      "training loss = 0.3786\n",
      "training accuracy = 0.8439\n",
      "num_test_corrects / test_total_examples = 11194 / 13245\n",
      "testing accuracy = 0.8451\n",
      "--------- epoch: 34 ---------\n",
      "num_corrects / total_examples = 11191 / 13245\n",
      "training loss = 0.3775\n",
      "training accuracy = 0.8449\n",
      "num_test_corrects / test_total_examples = 11199 / 13245\n",
      "testing accuracy = 0.8455\n",
      "--------- epoch: 35 ---------\n",
      "num_corrects / total_examples = 11200 / 13245\n",
      "training loss = 0.3765\n",
      "training accuracy = 0.8456\n",
      "num_test_corrects / test_total_examples = 11208 / 13245\n",
      "testing accuracy = 0.8462\n",
      "--------- epoch: 36 ---------\n",
      "num_corrects / total_examples = 11207 / 13245\n",
      "training loss = 0.3755\n",
      "training accuracy = 0.8461\n",
      "num_test_corrects / test_total_examples = 11222 / 13245\n",
      "testing accuracy = 0.8473\n",
      "--------- epoch: 37 ---------\n",
      "num_corrects / total_examples = 11216 / 13245\n",
      "training loss = 0.3746\n",
      "training accuracy = 0.8468\n",
      "num_test_corrects / test_total_examples = 11226 / 13245\n",
      "testing accuracy = 0.8476\n",
      "--------- epoch: 38 ---------\n",
      "num_corrects / total_examples = 11225 / 13245\n",
      "training loss = 0.3737\n",
      "training accuracy = 0.8475\n",
      "num_test_corrects / test_total_examples = 11233 / 13245\n",
      "testing accuracy = 0.8481\n",
      "--------- epoch: 39 ---------\n",
      "num_corrects / total_examples = 11231 / 13245\n",
      "training loss = 0.3728\n",
      "training accuracy = 0.8479\n",
      "num_test_corrects / test_total_examples = 11243 / 13245\n",
      "testing accuracy = 0.8488\n",
      "--------- epoch: 40 ---------\n",
      "num_corrects / total_examples = 11240 / 13245\n",
      "training loss = 0.3719\n",
      "training accuracy = 0.8486\n",
      "num_test_corrects / test_total_examples = 11249 / 13245\n",
      "testing accuracy = 0.8493\n",
      "--------- epoch: 41 ---------\n",
      "num_corrects / total_examples = 11243 / 13245\n",
      "training loss = 0.3711\n",
      "training accuracy = 0.8488\n",
      "num_test_corrects / test_total_examples = 11256 / 13245\n",
      "testing accuracy = 0.8498\n",
      "--------- epoch: 42 ---------\n",
      "num_corrects / total_examples = 11252 / 13245\n",
      "training loss = 0.3702\n",
      "training accuracy = 0.8495\n",
      "num_test_corrects / test_total_examples = 11264 / 13245\n",
      "testing accuracy = 0.8504\n",
      "--------- epoch: 43 ---------\n",
      "num_corrects / total_examples = 11263 / 13245\n",
      "training loss = 0.3694\n",
      "training accuracy = 0.8504\n",
      "num_test_corrects / test_total_examples = 11267 / 13245\n",
      "testing accuracy = 0.8507\n",
      "--------- epoch: 44 ---------\n",
      "num_corrects / total_examples = 11275 / 13245\n",
      "training loss = 0.3687\n",
      "training accuracy = 0.8513\n",
      "num_test_corrects / test_total_examples = 11268 / 13245\n",
      "testing accuracy = 0.8507\n",
      "--------- epoch: 45 ---------\n",
      "num_corrects / total_examples = 11278 / 13245\n",
      "training loss = 0.3679\n",
      "training accuracy = 0.8515\n",
      "num_test_corrects / test_total_examples = 11275 / 13245\n",
      "testing accuracy = 0.8513\n",
      "--------- epoch: 46 ---------\n",
      "num_corrects / total_examples = 11282 / 13245\n",
      "training loss = 0.3672\n",
      "training accuracy = 0.8518\n",
      "num_test_corrects / test_total_examples = 11280 / 13245\n",
      "testing accuracy = 0.8516\n",
      "--------- epoch: 47 ---------\n",
      "num_corrects / total_examples = 11292 / 13245\n",
      "training loss = 0.3664\n",
      "training accuracy = 0.8525\n",
      "num_test_corrects / test_total_examples = 11281 / 13245\n",
      "testing accuracy = 0.8517\n",
      "--------- epoch: 48 ---------\n",
      "num_corrects / total_examples = 11303 / 13245\n",
      "training loss = 0.3657\n",
      "training accuracy = 0.8534\n",
      "num_test_corrects / test_total_examples = 11289 / 13245\n",
      "testing accuracy = 0.8523\n",
      "--------- epoch: 49 ---------\n",
      "num_corrects / total_examples = 11311 / 13245\n",
      "training loss = 0.3651\n",
      "training accuracy = 0.8540\n",
      "num_test_corrects / test_total_examples = 11291 / 13245\n",
      "testing accuracy = 0.8525\n",
      "--------- epoch: 50 ---------\n",
      "num_corrects / total_examples = 11318 / 13245\n",
      "training loss = 0.3644\n",
      "training accuracy = 0.8545\n",
      "num_test_corrects / test_total_examples = 11296 / 13245\n",
      "testing accuracy = 0.8529\n",
      "--------- epoch: 51 ---------\n",
      "num_corrects / total_examples = 11321 / 13245\n",
      "training loss = 0.3637\n",
      "training accuracy = 0.8547\n",
      "num_test_corrects / test_total_examples = 11304 / 13245\n",
      "testing accuracy = 0.8535\n",
      "--------- epoch: 52 ---------\n",
      "num_corrects / total_examples = 11325 / 13245\n",
      "training loss = 0.3631\n",
      "training accuracy = 0.8550\n",
      "num_test_corrects / test_total_examples = 11308 / 13245\n",
      "testing accuracy = 0.8538\n",
      "--------- epoch: 53 ---------\n",
      "num_corrects / total_examples = 11330 / 13245\n",
      "training loss = 0.3625\n",
      "training accuracy = 0.8554\n",
      "num_test_corrects / test_total_examples = 11315 / 13245\n",
      "testing accuracy = 0.8543\n",
      "--------- epoch: 54 ---------\n",
      "num_corrects / total_examples = 11337 / 13245\n",
      "training loss = 0.3619\n",
      "training accuracy = 0.8559\n",
      "num_test_corrects / test_total_examples = 11318 / 13245\n",
      "testing accuracy = 0.8545\n",
      "--------- epoch: 55 ---------\n",
      "num_corrects / total_examples = 11340 / 13245\n",
      "training loss = 0.3613\n",
      "training accuracy = 0.8562\n",
      "num_test_corrects / test_total_examples = 11323 / 13245\n",
      "testing accuracy = 0.8549\n",
      "--------- epoch: 56 ---------\n",
      "num_corrects / total_examples = 11348 / 13245\n",
      "training loss = 0.3607\n",
      "training accuracy = 0.8568\n",
      "num_test_corrects / test_total_examples = 11328 / 13245\n",
      "testing accuracy = 0.8553\n",
      "--------- epoch: 57 ---------\n",
      "num_corrects / total_examples = 11355 / 13245\n",
      "training loss = 0.3601\n",
      "training accuracy = 0.8573\n",
      "num_test_corrects / test_total_examples = 11326 / 13245\n",
      "testing accuracy = 0.8551\n",
      "--------- epoch: 58 ---------\n",
      "num_corrects / total_examples = 11357 / 13245\n",
      "training loss = 0.3596\n",
      "training accuracy = 0.8575\n",
      "num_test_corrects / test_total_examples = 11332 / 13245\n",
      "testing accuracy = 0.8556\n",
      "--------- epoch: 59 ---------\n",
      "num_corrects / total_examples = 11367 / 13245\n",
      "training loss = 0.3590\n",
      "training accuracy = 0.8582\n",
      "num_test_corrects / test_total_examples = 11337 / 13245\n",
      "testing accuracy = 0.8559\n",
      "--------- epoch: 60 ---------\n",
      "num_corrects / total_examples = 11374 / 13245\n",
      "training loss = 0.3585\n",
      "training accuracy = 0.8587\n",
      "num_test_corrects / test_total_examples = 11337 / 13245\n",
      "testing accuracy = 0.8559\n",
      "--------- epoch: 61 ---------\n",
      "num_corrects / total_examples = 11381 / 13245\n",
      "training loss = 0.3579\n",
      "training accuracy = 0.8593\n",
      "num_test_corrects / test_total_examples = 11344 / 13245\n",
      "testing accuracy = 0.8565\n",
      "--------- epoch: 62 ---------\n",
      "num_corrects / total_examples = 11384 / 13245\n",
      "training loss = 0.3574\n",
      "training accuracy = 0.8595\n",
      "num_test_corrects / test_total_examples = 11344 / 13245\n",
      "testing accuracy = 0.8565\n",
      "--------- epoch: 63 ---------\n",
      "num_corrects / total_examples = 11390 / 13245\n",
      "training loss = 0.3569\n",
      "training accuracy = 0.8599\n",
      "num_test_corrects / test_total_examples = 11344 / 13245\n",
      "testing accuracy = 0.8565\n",
      "--------- epoch: 64 ---------\n",
      "num_corrects / total_examples = 11399 / 13245\n",
      "training loss = 0.3564\n",
      "training accuracy = 0.8606\n",
      "num_test_corrects / test_total_examples = 11345 / 13245\n",
      "testing accuracy = 0.8565\n",
      "--------- epoch: 65 ---------\n",
      "num_corrects / total_examples = 11402 / 13245\n",
      "training loss = 0.3559\n",
      "training accuracy = 0.8609\n",
      "num_test_corrects / test_total_examples = 11347 / 13245\n",
      "testing accuracy = 0.8567\n",
      "--------- epoch: 66 ---------\n",
      "num_corrects / total_examples = 11405 / 13245\n",
      "training loss = 0.3555\n",
      "training accuracy = 0.8611\n",
      "num_test_corrects / test_total_examples = 11355 / 13245\n",
      "testing accuracy = 0.8573\n",
      "--------- epoch: 67 ---------\n",
      "num_corrects / total_examples = 11407 / 13245\n",
      "training loss = 0.3550\n",
      "training accuracy = 0.8612\n",
      "num_test_corrects / test_total_examples = 11357 / 13245\n",
      "testing accuracy = 0.8575\n",
      "--------- epoch: 68 ---------\n",
      "num_corrects / total_examples = 11415 / 13245\n",
      "training loss = 0.3545\n",
      "training accuracy = 0.8618\n",
      "num_test_corrects / test_total_examples = 11360 / 13245\n",
      "testing accuracy = 0.8577\n",
      "--------- epoch: 69 ---------\n",
      "num_corrects / total_examples = 11424 / 13245\n",
      "training loss = 0.3541\n",
      "training accuracy = 0.8625\n",
      "num_test_corrects / test_total_examples = 11368 / 13245\n",
      "testing accuracy = 0.8583\n",
      "--------- epoch: 70 ---------\n",
      "num_corrects / total_examples = 11434 / 13245\n",
      "training loss = 0.3536\n",
      "training accuracy = 0.8633\n",
      "num_test_corrects / test_total_examples = 11375 / 13245\n",
      "testing accuracy = 0.8588\n",
      "--------- epoch: 71 ---------\n",
      "num_corrects / total_examples = 11439 / 13245\n",
      "training loss = 0.3532\n",
      "training accuracy = 0.8636\n",
      "num_test_corrects / test_total_examples = 11377 / 13245\n",
      "testing accuracy = 0.8590\n",
      "--------- epoch: 72 ---------\n",
      "num_corrects / total_examples = 11447 / 13245\n",
      "training loss = 0.3528\n",
      "training accuracy = 0.8643\n",
      "num_test_corrects / test_total_examples = 11382 / 13245\n",
      "testing accuracy = 0.8593\n",
      "--------- epoch: 73 ---------\n",
      "num_corrects / total_examples = 11454 / 13245\n",
      "training loss = 0.3524\n",
      "training accuracy = 0.8648\n",
      "num_test_corrects / test_total_examples = 11385 / 13245\n",
      "testing accuracy = 0.8596\n",
      "--------- epoch: 74 ---------\n",
      "num_corrects / total_examples = 11459 / 13245\n",
      "training loss = 0.3519\n",
      "training accuracy = 0.8652\n",
      "num_test_corrects / test_total_examples = 11388 / 13245\n",
      "testing accuracy = 0.8598\n",
      "--------- epoch: 75 ---------\n",
      "num_corrects / total_examples = 11473 / 13245\n",
      "training loss = 0.3515\n",
      "training accuracy = 0.8662\n",
      "num_test_corrects / test_total_examples = 11388 / 13245\n",
      "testing accuracy = 0.8598\n",
      "--------- epoch: 76 ---------\n",
      "num_corrects / total_examples = 11477 / 13245\n",
      "training loss = 0.3511\n",
      "training accuracy = 0.8665\n",
      "num_test_corrects / test_total_examples = 11390 / 13245\n",
      "testing accuracy = 0.8599\n",
      "--------- epoch: 77 ---------\n",
      "num_corrects / total_examples = 11481 / 13245\n",
      "training loss = 0.3507\n",
      "training accuracy = 0.8668\n",
      "num_test_corrects / test_total_examples = 11392 / 13245\n",
      "testing accuracy = 0.8601\n",
      "--------- epoch: 78 ---------\n",
      "num_corrects / total_examples = 11481 / 13245\n",
      "training loss = 0.3503\n",
      "training accuracy = 0.8668\n",
      "num_test_corrects / test_total_examples = 11396 / 13245\n",
      "testing accuracy = 0.8604\n",
      "--------- epoch: 79 ---------\n",
      "num_corrects / total_examples = 11486 / 13245\n",
      "training loss = 0.3500\n",
      "training accuracy = 0.8672\n",
      "num_test_corrects / test_total_examples = 11399 / 13245\n",
      "testing accuracy = 0.8606\n",
      "--------- epoch: 80 ---------\n",
      "num_corrects / total_examples = 11493 / 13245\n",
      "training loss = 0.3496\n",
      "training accuracy = 0.8677\n",
      "num_test_corrects / test_total_examples = 11403 / 13245\n",
      "testing accuracy = 0.8609\n",
      "--------- epoch: 81 ---------\n",
      "num_corrects / total_examples = 11497 / 13245\n",
      "training loss = 0.3492\n",
      "training accuracy = 0.8680\n",
      "num_test_corrects / test_total_examples = 11405 / 13245\n",
      "testing accuracy = 0.8611\n",
      "--------- epoch: 82 ---------\n",
      "num_corrects / total_examples = 11500 / 13245\n",
      "training loss = 0.3488\n",
      "training accuracy = 0.8683\n",
      "num_test_corrects / test_total_examples = 11412 / 13245\n",
      "testing accuracy = 0.8616\n",
      "--------- epoch: 83 ---------\n",
      "num_corrects / total_examples = 11504 / 13245\n",
      "training loss = 0.3485\n",
      "training accuracy = 0.8686\n",
      "num_test_corrects / test_total_examples = 11419 / 13245\n",
      "testing accuracy = 0.8621\n",
      "--------- epoch: 84 ---------\n",
      "num_corrects / total_examples = 11512 / 13245\n",
      "training loss = 0.3481\n",
      "training accuracy = 0.8692\n",
      "num_test_corrects / test_total_examples = 11425 / 13245\n",
      "testing accuracy = 0.8626\n",
      "--------- epoch: 85 ---------\n",
      "num_corrects / total_examples = 11515 / 13245\n",
      "training loss = 0.3478\n",
      "training accuracy = 0.8694\n",
      "num_test_corrects / test_total_examples = 11428 / 13245\n",
      "testing accuracy = 0.8628\n",
      "--------- epoch: 86 ---------\n",
      "num_corrects / total_examples = 11528 / 13245\n",
      "training loss = 0.3474\n",
      "training accuracy = 0.8704\n",
      "num_test_corrects / test_total_examples = 11432 / 13245\n",
      "testing accuracy = 0.8631\n",
      "--------- epoch: 87 ---------\n",
      "num_corrects / total_examples = 11531 / 13245\n",
      "training loss = 0.3471\n",
      "training accuracy = 0.8706\n",
      "num_test_corrects / test_total_examples = 11437 / 13245\n",
      "testing accuracy = 0.8635\n",
      "--------- epoch: 88 ---------\n",
      "num_corrects / total_examples = 11537 / 13245\n",
      "training loss = 0.3468\n",
      "training accuracy = 0.8710\n",
      "num_test_corrects / test_total_examples = 11441 / 13245\n",
      "testing accuracy = 0.8638\n",
      "--------- epoch: 89 ---------\n",
      "num_corrects / total_examples = 11539 / 13245\n",
      "training loss = 0.3464\n",
      "training accuracy = 0.8712\n",
      "num_test_corrects / test_total_examples = 11446 / 13245\n",
      "testing accuracy = 0.8642\n",
      "--------- epoch: 90 ---------\n",
      "num_corrects / total_examples = 11540 / 13245\n",
      "training loss = 0.3461\n",
      "training accuracy = 0.8713\n",
      "num_test_corrects / test_total_examples = 11448 / 13245\n",
      "testing accuracy = 0.8643\n",
      "--------- epoch: 91 ---------\n",
      "num_corrects / total_examples = 11544 / 13245\n",
      "training loss = 0.3458\n",
      "training accuracy = 0.8716\n",
      "num_test_corrects / test_total_examples = 11450 / 13245\n",
      "testing accuracy = 0.8645\n",
      "--------- epoch: 92 ---------\n",
      "num_corrects / total_examples = 11545 / 13245\n",
      "training loss = 0.3455\n",
      "training accuracy = 0.8716\n",
      "num_test_corrects / test_total_examples = 11454 / 13245\n",
      "testing accuracy = 0.8648\n",
      "--------- epoch: 93 ---------\n",
      "num_corrects / total_examples = 11550 / 13245\n",
      "training loss = 0.3452\n",
      "training accuracy = 0.8720\n",
      "num_test_corrects / test_total_examples = 11456 / 13245\n",
      "testing accuracy = 0.8649\n",
      "--------- epoch: 94 ---------\n",
      "num_corrects / total_examples = 11550 / 13245\n",
      "training loss = 0.3449\n",
      "training accuracy = 0.8720\n",
      "num_test_corrects / test_total_examples = 11465 / 13245\n",
      "testing accuracy = 0.8656\n",
      "--------- epoch: 95 ---------\n",
      "num_corrects / total_examples = 11555 / 13245\n",
      "training loss = 0.3446\n",
      "training accuracy = 0.8724\n",
      "num_test_corrects / test_total_examples = 11466 / 13245\n",
      "testing accuracy = 0.8657\n",
      "--------- epoch: 96 ---------\n",
      "num_corrects / total_examples = 11557 / 13245\n",
      "training loss = 0.3443\n",
      "training accuracy = 0.8726\n",
      "num_test_corrects / test_total_examples = 11465 / 13245\n",
      "testing accuracy = 0.8656\n",
      "--------- epoch: 97 ---------\n",
      "num_corrects / total_examples = 11557 / 13245\n",
      "training loss = 0.3440\n",
      "training accuracy = 0.8726\n",
      "num_test_corrects / test_total_examples = 11467 / 13245\n",
      "testing accuracy = 0.8658\n",
      "--------- epoch: 98 ---------\n",
      "num_corrects / total_examples = 11557 / 13245\n",
      "training loss = 0.3437\n",
      "training accuracy = 0.8726\n",
      "num_test_corrects / test_total_examples = 11472 / 13245\n",
      "testing accuracy = 0.8661\n",
      "--------- epoch: 99 ---------\n",
      "num_corrects / total_examples = 11554 / 13245\n",
      "training loss = 0.3434\n",
      "training accuracy = 0.8723\n",
      "num_test_corrects / test_total_examples = 11479 / 13245\n",
      "testing accuracy = 0.8667\n",
      "--------- epoch: 100 ---------\n",
      "num_corrects / total_examples = 11557 / 13245\n",
      "training loss = 0.3431\n",
      "training accuracy = 0.8726\n",
      "num_test_corrects / test_total_examples = 11485 / 13245\n",
      "testing accuracy = 0.8671\n",
      "--------- epoch: 101 ---------\n",
      "num_corrects / total_examples = 11556 / 13245\n",
      "training loss = 0.3428\n",
      "training accuracy = 0.8725\n",
      "num_test_corrects / test_total_examples = 11486 / 13245\n",
      "testing accuracy = 0.8672\n",
      "--------- epoch: 102 ---------\n",
      "num_corrects / total_examples = 11557 / 13245\n",
      "training loss = 0.3425\n",
      "training accuracy = 0.8726\n",
      "num_test_corrects / test_total_examples = 11487 / 13245\n",
      "testing accuracy = 0.8673\n",
      "--------- epoch: 103 ---------\n",
      "num_corrects / total_examples = 11557 / 13245\n",
      "training loss = 0.3423\n",
      "training accuracy = 0.8726\n",
      "num_test_corrects / test_total_examples = 11492 / 13245\n",
      "testing accuracy = 0.8676\n",
      "--------- epoch: 104 ---------\n",
      "num_corrects / total_examples = 11561 / 13245\n",
      "training loss = 0.3420\n",
      "training accuracy = 0.8729\n",
      "num_test_corrects / test_total_examples = 11495 / 13245\n",
      "testing accuracy = 0.8679\n",
      "--------- epoch: 105 ---------\n",
      "num_corrects / total_examples = 11565 / 13245\n",
      "training loss = 0.3417\n",
      "training accuracy = 0.8732\n",
      "num_test_corrects / test_total_examples = 11501 / 13245\n",
      "testing accuracy = 0.8683\n",
      "--------- epoch: 106 ---------\n",
      "num_corrects / total_examples = 11568 / 13245\n",
      "training loss = 0.3415\n",
      "training accuracy = 0.8734\n",
      "num_test_corrects / test_total_examples = 11503 / 13245\n",
      "testing accuracy = 0.8685\n",
      "--------- epoch: 107 ---------\n",
      "num_corrects / total_examples = 11571 / 13245\n",
      "training loss = 0.3412\n",
      "training accuracy = 0.8736\n",
      "num_test_corrects / test_total_examples = 11508 / 13245\n",
      "testing accuracy = 0.8689\n",
      "--------- epoch: 108 ---------\n",
      "num_corrects / total_examples = 11573 / 13245\n",
      "training loss = 0.3410\n",
      "training accuracy = 0.8738\n",
      "num_test_corrects / test_total_examples = 11509 / 13245\n",
      "testing accuracy = 0.8689\n",
      "--------- epoch: 109 ---------\n",
      "num_corrects / total_examples = 11577 / 13245\n",
      "training loss = 0.3407\n",
      "training accuracy = 0.8741\n",
      "num_test_corrects / test_total_examples = 11512 / 13245\n",
      "testing accuracy = 0.8692\n",
      "--------- epoch: 110 ---------\n",
      "num_corrects / total_examples = 11579 / 13245\n",
      "training loss = 0.3405\n",
      "training accuracy = 0.8742\n",
      "num_test_corrects / test_total_examples = 11515 / 13245\n",
      "testing accuracy = 0.8694\n",
      "--------- epoch: 111 ---------\n",
      "num_corrects / total_examples = 11582 / 13245\n",
      "training loss = 0.3402\n",
      "training accuracy = 0.8744\n",
      "num_test_corrects / test_total_examples = 11519 / 13245\n",
      "testing accuracy = 0.8697\n",
      "--------- epoch: 112 ---------\n",
      "num_corrects / total_examples = 11590 / 13245\n",
      "training loss = 0.3400\n",
      "training accuracy = 0.8750\n",
      "num_test_corrects / test_total_examples = 11521 / 13245\n",
      "testing accuracy = 0.8698\n",
      "--------- epoch: 113 ---------\n",
      "num_corrects / total_examples = 11596 / 13245\n",
      "training loss = 0.3397\n",
      "training accuracy = 0.8755\n",
      "num_test_corrects / test_total_examples = 11523 / 13245\n",
      "testing accuracy = 0.8700\n",
      "--------- epoch: 114 ---------\n",
      "num_corrects / total_examples = 11601 / 13245\n",
      "training loss = 0.3395\n",
      "training accuracy = 0.8759\n",
      "num_test_corrects / test_total_examples = 11523 / 13245\n",
      "testing accuracy = 0.8700\n",
      "--------- epoch: 115 ---------\n",
      "num_corrects / total_examples = 11606 / 13245\n",
      "training loss = 0.3392\n",
      "training accuracy = 0.8763\n",
      "num_test_corrects / test_total_examples = 11524 / 13245\n",
      "testing accuracy = 0.8701\n",
      "--------- epoch: 116 ---------\n",
      "num_corrects / total_examples = 11609 / 13245\n",
      "training loss = 0.3390\n",
      "training accuracy = 0.8765\n",
      "num_test_corrects / test_total_examples = 11529 / 13245\n",
      "testing accuracy = 0.8704\n",
      "--------- epoch: 117 ---------\n",
      "num_corrects / total_examples = 11611 / 13245\n",
      "training loss = 0.3388\n",
      "training accuracy = 0.8766\n",
      "num_test_corrects / test_total_examples = 11535 / 13245\n",
      "testing accuracy = 0.8709\n",
      "--------- epoch: 118 ---------\n",
      "num_corrects / total_examples = 11615 / 13245\n",
      "training loss = 0.3385\n",
      "training accuracy = 0.8769\n",
      "num_test_corrects / test_total_examples = 11533 / 13245\n",
      "testing accuracy = 0.8707\n",
      "--------- epoch: 119 ---------\n",
      "num_corrects / total_examples = 11618 / 13245\n",
      "training loss = 0.3383\n",
      "training accuracy = 0.8772\n",
      "num_test_corrects / test_total_examples = 11542 / 13245\n",
      "testing accuracy = 0.8714\n",
      "--------- epoch: 120 ---------\n",
      "num_corrects / total_examples = 11620 / 13245\n",
      "training loss = 0.3381\n",
      "training accuracy = 0.8773\n",
      "num_test_corrects / test_total_examples = 11547 / 13245\n",
      "testing accuracy = 0.8718\n",
      "--------- epoch: 121 ---------\n",
      "num_corrects / total_examples = 11627 / 13245\n",
      "training loss = 0.3379\n",
      "training accuracy = 0.8778\n",
      "num_test_corrects / test_total_examples = 11551 / 13245\n",
      "testing accuracy = 0.8721\n",
      "--------- epoch: 122 ---------\n",
      "num_corrects / total_examples = 11629 / 13245\n",
      "training loss = 0.3377\n",
      "training accuracy = 0.8780\n",
      "num_test_corrects / test_total_examples = 11554 / 13245\n",
      "testing accuracy = 0.8723\n",
      "--------- epoch: 123 ---------\n",
      "num_corrects / total_examples = 11630 / 13245\n",
      "training loss = 0.3374\n",
      "training accuracy = 0.8781\n",
      "num_test_corrects / test_total_examples = 11562 / 13245\n",
      "testing accuracy = 0.8729\n",
      "--------- epoch: 124 ---------\n",
      "num_corrects / total_examples = 11632 / 13245\n",
      "training loss = 0.3372\n",
      "training accuracy = 0.8782\n",
      "num_test_corrects / test_total_examples = 11564 / 13245\n",
      "testing accuracy = 0.8731\n",
      "--------- epoch: 125 ---------\n",
      "num_corrects / total_examples = 11632 / 13245\n",
      "training loss = 0.3370\n",
      "training accuracy = 0.8782\n",
      "num_test_corrects / test_total_examples = 11567 / 13245\n",
      "testing accuracy = 0.8733\n",
      "--------- epoch: 126 ---------\n",
      "num_corrects / total_examples = 11633 / 13245\n",
      "training loss = 0.3368\n",
      "training accuracy = 0.8783\n",
      "num_test_corrects / test_total_examples = 11576 / 13245\n",
      "testing accuracy = 0.8740\n",
      "--------- epoch: 127 ---------\n",
      "num_corrects / total_examples = 11633 / 13245\n",
      "training loss = 0.3366\n",
      "training accuracy = 0.8783\n",
      "num_test_corrects / test_total_examples = 11575 / 13245\n",
      "testing accuracy = 0.8739\n",
      "--------- epoch: 128 ---------\n",
      "num_corrects / total_examples = 11633 / 13245\n",
      "training loss = 0.3364\n",
      "training accuracy = 0.8783\n",
      "num_test_corrects / test_total_examples = 11579 / 13245\n",
      "testing accuracy = 0.8742\n",
      "--------- epoch: 129 ---------\n",
      "num_corrects / total_examples = 11636 / 13245\n",
      "training loss = 0.3362\n",
      "training accuracy = 0.8785\n",
      "num_test_corrects / test_total_examples = 11580 / 13245\n",
      "testing accuracy = 0.8743\n",
      "--------- epoch: 130 ---------\n",
      "num_corrects / total_examples = 11640 / 13245\n",
      "training loss = 0.3360\n",
      "training accuracy = 0.8788\n",
      "num_test_corrects / test_total_examples = 11580 / 13245\n",
      "testing accuracy = 0.8743\n",
      "--------- epoch: 131 ---------\n",
      "num_corrects / total_examples = 11643 / 13245\n",
      "training loss = 0.3358\n",
      "training accuracy = 0.8790\n",
      "num_test_corrects / test_total_examples = 11582 / 13245\n",
      "testing accuracy = 0.8744\n",
      "--------- epoch: 132 ---------\n",
      "num_corrects / total_examples = 11648 / 13245\n",
      "training loss = 0.3356\n",
      "training accuracy = 0.8794\n",
      "num_test_corrects / test_total_examples = 11582 / 13245\n",
      "testing accuracy = 0.8744\n",
      "--------- epoch: 133 ---------\n",
      "num_corrects / total_examples = 11647 / 13245\n",
      "training loss = 0.3354\n",
      "training accuracy = 0.8794\n",
      "num_test_corrects / test_total_examples = 11584 / 13245\n",
      "testing accuracy = 0.8746\n",
      "--------- epoch: 134 ---------\n",
      "num_corrects / total_examples = 11648 / 13245\n",
      "training loss = 0.3352\n",
      "training accuracy = 0.8794\n",
      "num_test_corrects / test_total_examples = 11586 / 13245\n",
      "testing accuracy = 0.8747\n",
      "--------- epoch: 135 ---------\n",
      "num_corrects / total_examples = 11650 / 13245\n",
      "training loss = 0.3350\n",
      "training accuracy = 0.8796\n",
      "num_test_corrects / test_total_examples = 11586 / 13245\n",
      "testing accuracy = 0.8747\n",
      "--------- epoch: 136 ---------\n",
      "num_corrects / total_examples = 11652 / 13245\n",
      "training loss = 0.3348\n",
      "training accuracy = 0.8797\n",
      "num_test_corrects / test_total_examples = 11588 / 13245\n",
      "testing accuracy = 0.8749\n",
      "--------- epoch: 137 ---------\n",
      "num_corrects / total_examples = 11652 / 13245\n",
      "training loss = 0.3346\n",
      "training accuracy = 0.8797\n",
      "num_test_corrects / test_total_examples = 11590 / 13245\n",
      "testing accuracy = 0.8750\n",
      "--------- epoch: 138 ---------\n",
      "num_corrects / total_examples = 11650 / 13245\n",
      "training loss = 0.3344\n",
      "training accuracy = 0.8796\n",
      "num_test_corrects / test_total_examples = 11590 / 13245\n",
      "testing accuracy = 0.8750\n",
      "--------- epoch: 139 ---------\n",
      "num_corrects / total_examples = 11652 / 13245\n",
      "training loss = 0.3343\n",
      "training accuracy = 0.8797\n",
      "num_test_corrects / test_total_examples = 11593 / 13245\n",
      "testing accuracy = 0.8753\n",
      "--------- epoch: 140 ---------\n",
      "num_corrects / total_examples = 11657 / 13245\n",
      "training loss = 0.3341\n",
      "training accuracy = 0.8801\n",
      "num_test_corrects / test_total_examples = 11592 / 13245\n",
      "testing accuracy = 0.8752\n",
      "--------- epoch: 141 ---------\n",
      "num_corrects / total_examples = 11658 / 13245\n",
      "training loss = 0.3339\n",
      "training accuracy = 0.8802\n",
      "num_test_corrects / test_total_examples = 11594 / 13245\n",
      "testing accuracy = 0.8753\n",
      "--------- epoch: 142 ---------\n",
      "num_corrects / total_examples = 11659 / 13245\n",
      "training loss = 0.3337\n",
      "training accuracy = 0.8803\n",
      "num_test_corrects / test_total_examples = 11596 / 13245\n",
      "testing accuracy = 0.8755\n",
      "--------- epoch: 143 ---------\n",
      "num_corrects / total_examples = 11664 / 13245\n",
      "training loss = 0.3335\n",
      "training accuracy = 0.8806\n",
      "num_test_corrects / test_total_examples = 11603 / 13245\n",
      "testing accuracy = 0.8760\n",
      "--------- epoch: 144 ---------\n",
      "num_corrects / total_examples = 11666 / 13245\n",
      "training loss = 0.3334\n",
      "training accuracy = 0.8808\n",
      "num_test_corrects / test_total_examples = 11609 / 13245\n",
      "testing accuracy = 0.8765\n",
      "--------- epoch: 145 ---------\n",
      "num_corrects / total_examples = 11667 / 13245\n",
      "training loss = 0.3332\n",
      "training accuracy = 0.8809\n",
      "num_test_corrects / test_total_examples = 11611 / 13245\n",
      "testing accuracy = 0.8766\n",
      "--------- epoch: 146 ---------\n",
      "num_corrects / total_examples = 11667 / 13245\n",
      "training loss = 0.3330\n",
      "training accuracy = 0.8809\n",
      "num_test_corrects / test_total_examples = 11612 / 13245\n",
      "testing accuracy = 0.8767\n",
      "--------- epoch: 147 ---------\n",
      "num_corrects / total_examples = 11667 / 13245\n",
      "training loss = 0.3328\n",
      "training accuracy = 0.8809\n",
      "num_test_corrects / test_total_examples = 11613 / 13245\n",
      "testing accuracy = 0.8768\n",
      "--------- epoch: 148 ---------\n",
      "num_corrects / total_examples = 11672 / 13245\n",
      "training loss = 0.3327\n",
      "training accuracy = 0.8812\n",
      "num_test_corrects / test_total_examples = 11615 / 13245\n",
      "testing accuracy = 0.8769\n",
      "--------- epoch: 149 ---------\n",
      "num_corrects / total_examples = 11671 / 13245\n",
      "training loss = 0.3325\n",
      "training accuracy = 0.8812\n",
      "num_test_corrects / test_total_examples = 11618 / 13245\n",
      "testing accuracy = 0.8772\n",
      "--------- epoch: 150 ---------\n",
      "num_corrects / total_examples = 11673 / 13245\n",
      "training loss = 0.3323\n",
      "training accuracy = 0.8813\n",
      "num_test_corrects / test_total_examples = 11623 / 13245\n",
      "testing accuracy = 0.8775\n",
      "--------- epoch: 151 ---------\n",
      "num_corrects / total_examples = 11674 / 13245\n",
      "training loss = 0.3322\n",
      "training accuracy = 0.8814\n",
      "num_test_corrects / test_total_examples = 11627 / 13245\n",
      "testing accuracy = 0.8778\n",
      "--------- epoch: 152 ---------\n",
      "num_corrects / total_examples = 11671 / 13245\n",
      "training loss = 0.3320\n",
      "training accuracy = 0.8812\n",
      "num_test_corrects / test_total_examples = 11631 / 13245\n",
      "testing accuracy = 0.8781\n",
      "--------- epoch: 153 ---------\n",
      "num_corrects / total_examples = 11673 / 13245\n",
      "training loss = 0.3318\n",
      "training accuracy = 0.8813\n",
      "num_test_corrects / test_total_examples = 11632 / 13245\n",
      "testing accuracy = 0.8782\n",
      "--------- epoch: 154 ---------\n",
      "num_corrects / total_examples = 11675 / 13245\n",
      "training loss = 0.3317\n",
      "training accuracy = 0.8815\n",
      "num_test_corrects / test_total_examples = 11635 / 13245\n",
      "testing accuracy = 0.8784\n",
      "--------- epoch: 155 ---------\n",
      "num_corrects / total_examples = 11676 / 13245\n",
      "training loss = 0.3315\n",
      "training accuracy = 0.8815\n",
      "num_test_corrects / test_total_examples = 11636 / 13245\n",
      "testing accuracy = 0.8785\n",
      "--------- epoch: 156 ---------\n",
      "num_corrects / total_examples = 11679 / 13245\n",
      "training loss = 0.3314\n",
      "training accuracy = 0.8818\n",
      "num_test_corrects / test_total_examples = 11640 / 13245\n",
      "testing accuracy = 0.8788\n",
      "--------- epoch: 157 ---------\n",
      "num_corrects / total_examples = 11677 / 13245\n",
      "training loss = 0.3312\n",
      "training accuracy = 0.8816\n",
      "num_test_corrects / test_total_examples = 11641 / 13245\n",
      "testing accuracy = 0.8789\n",
      "--------- epoch: 158 ---------\n",
      "num_corrects / total_examples = 11678 / 13245\n",
      "training loss = 0.3311\n",
      "training accuracy = 0.8817\n",
      "num_test_corrects / test_total_examples = 11642 / 13245\n",
      "testing accuracy = 0.8790\n",
      "--------- epoch: 159 ---------\n",
      "num_corrects / total_examples = 11678 / 13245\n",
      "training loss = 0.3309\n",
      "training accuracy = 0.8817\n",
      "num_test_corrects / test_total_examples = 11642 / 13245\n",
      "testing accuracy = 0.8790\n",
      "--------- epoch: 160 ---------\n",
      "num_corrects / total_examples = 11682 / 13245\n",
      "training loss = 0.3307\n",
      "training accuracy = 0.8820\n",
      "num_test_corrects / test_total_examples = 11644 / 13245\n",
      "testing accuracy = 0.8791\n",
      "--------- epoch: 161 ---------\n",
      "num_corrects / total_examples = 11684 / 13245\n",
      "training loss = 0.3306\n",
      "training accuracy = 0.8821\n",
      "num_test_corrects / test_total_examples = 11647 / 13245\n",
      "testing accuracy = 0.8794\n",
      "--------- epoch: 162 ---------\n",
      "num_corrects / total_examples = 11684 / 13245\n",
      "training loss = 0.3304\n",
      "training accuracy = 0.8821\n",
      "num_test_corrects / test_total_examples = 11651 / 13245\n",
      "testing accuracy = 0.8797\n",
      "--------- epoch: 163 ---------\n",
      "num_corrects / total_examples = 11690 / 13245\n",
      "training loss = 0.3303\n",
      "training accuracy = 0.8826\n",
      "num_test_corrects / test_total_examples = 11652 / 13245\n",
      "testing accuracy = 0.8797\n",
      "--------- epoch: 164 ---------\n",
      "num_corrects / total_examples = 11691 / 13245\n",
      "training loss = 0.3302\n",
      "training accuracy = 0.8827\n",
      "num_test_corrects / test_total_examples = 11656 / 13245\n",
      "testing accuracy = 0.8800\n",
      "--------- epoch: 165 ---------\n",
      "num_corrects / total_examples = 11692 / 13245\n",
      "training loss = 0.3300\n",
      "training accuracy = 0.8827\n",
      "num_test_corrects / test_total_examples = 11660 / 13245\n",
      "testing accuracy = 0.8803\n",
      "--------- epoch: 166 ---------\n",
      "num_corrects / total_examples = 11694 / 13245\n",
      "training loss = 0.3299\n",
      "training accuracy = 0.8829\n",
      "num_test_corrects / test_total_examples = 11659 / 13245\n",
      "testing accuracy = 0.8803\n",
      "--------- epoch: 167 ---------\n",
      "num_corrects / total_examples = 11697 / 13245\n",
      "training loss = 0.3297\n",
      "training accuracy = 0.8831\n",
      "num_test_corrects / test_total_examples = 11661 / 13245\n",
      "testing accuracy = 0.8804\n",
      "--------- epoch: 168 ---------\n",
      "num_corrects / total_examples = 11700 / 13245\n",
      "training loss = 0.3296\n",
      "training accuracy = 0.8834\n",
      "num_test_corrects / test_total_examples = 11661 / 13245\n",
      "testing accuracy = 0.8804\n",
      "--------- epoch: 169 ---------\n",
      "num_corrects / total_examples = 11701 / 13245\n",
      "training loss = 0.3294\n",
      "training accuracy = 0.8834\n",
      "num_test_corrects / test_total_examples = 11662 / 13245\n",
      "testing accuracy = 0.8805\n",
      "--------- epoch: 170 ---------\n",
      "num_corrects / total_examples = 11701 / 13245\n",
      "training loss = 0.3293\n",
      "training accuracy = 0.8834\n",
      "num_test_corrects / test_total_examples = 11662 / 13245\n",
      "testing accuracy = 0.8805\n",
      "--------- epoch: 171 ---------\n",
      "num_corrects / total_examples = 11703 / 13245\n",
      "training loss = 0.3291\n",
      "training accuracy = 0.8836\n",
      "num_test_corrects / test_total_examples = 11663 / 13245\n",
      "testing accuracy = 0.8806\n",
      "--------- epoch: 172 ---------\n",
      "num_corrects / total_examples = 11705 / 13245\n",
      "training loss = 0.3290\n",
      "training accuracy = 0.8837\n",
      "num_test_corrects / test_total_examples = 11664 / 13245\n",
      "testing accuracy = 0.8806\n",
      "--------- epoch: 173 ---------\n",
      "num_corrects / total_examples = 11707 / 13245\n",
      "training loss = 0.3289\n",
      "training accuracy = 0.8839\n",
      "num_test_corrects / test_total_examples = 11665 / 13245\n",
      "testing accuracy = 0.8807\n",
      "--------- epoch: 174 ---------\n",
      "num_corrects / total_examples = 11705 / 13245\n",
      "training loss = 0.3287\n",
      "training accuracy = 0.8837\n",
      "num_test_corrects / test_total_examples = 11668 / 13245\n",
      "testing accuracy = 0.8809\n",
      "--------- epoch: 175 ---------\n",
      "num_corrects / total_examples = 11706 / 13245\n",
      "training loss = 0.3286\n",
      "training accuracy = 0.8838\n",
      "num_test_corrects / test_total_examples = 11671 / 13245\n",
      "testing accuracy = 0.8812\n",
      "--------- epoch: 176 ---------\n",
      "num_corrects / total_examples = 11706 / 13245\n",
      "training loss = 0.3285\n",
      "training accuracy = 0.8838\n",
      "num_test_corrects / test_total_examples = 11670 / 13245\n",
      "testing accuracy = 0.8811\n",
      "--------- epoch: 177 ---------\n",
      "num_corrects / total_examples = 11706 / 13245\n",
      "training loss = 0.3283\n",
      "training accuracy = 0.8838\n",
      "num_test_corrects / test_total_examples = 11669 / 13245\n",
      "testing accuracy = 0.8810\n",
      "--------- epoch: 178 ---------\n",
      "num_corrects / total_examples = 11705 / 13245\n",
      "training loss = 0.3282\n",
      "training accuracy = 0.8837\n",
      "num_test_corrects / test_total_examples = 11668 / 13245\n",
      "testing accuracy = 0.8809\n",
      "--------- epoch: 179 ---------\n",
      "num_corrects / total_examples = 11706 / 13245\n",
      "training loss = 0.3281\n",
      "training accuracy = 0.8838\n",
      "num_test_corrects / test_total_examples = 11669 / 13245\n",
      "testing accuracy = 0.8810\n",
      "--------- epoch: 180 ---------\n",
      "num_corrects / total_examples = 11711 / 13245\n",
      "training loss = 0.3279\n",
      "training accuracy = 0.8842\n",
      "num_test_corrects / test_total_examples = 11669 / 13245\n",
      "testing accuracy = 0.8810\n",
      "--------- epoch: 181 ---------\n",
      "num_corrects / total_examples = 11709 / 13245\n",
      "training loss = 0.3278\n",
      "training accuracy = 0.8840\n",
      "num_test_corrects / test_total_examples = 11667 / 13245\n",
      "testing accuracy = 0.8809\n",
      "--------- epoch: 182 ---------\n",
      "num_corrects / total_examples = 11709 / 13245\n",
      "training loss = 0.3277\n",
      "training accuracy = 0.8840\n",
      "num_test_corrects / test_total_examples = 11669 / 13245\n",
      "testing accuracy = 0.8810\n",
      "--------- epoch: 183 ---------\n",
      "num_corrects / total_examples = 11709 / 13245\n",
      "training loss = 0.3275\n",
      "training accuracy = 0.8840\n",
      "num_test_corrects / test_total_examples = 11672 / 13245\n",
      "testing accuracy = 0.8812\n",
      "--------- epoch: 184 ---------\n",
      "num_corrects / total_examples = 11711 / 13245\n",
      "training loss = 0.3274\n",
      "training accuracy = 0.8842\n",
      "num_test_corrects / test_total_examples = 11672 / 13245\n",
      "testing accuracy = 0.8812\n",
      "--------- epoch: 185 ---------\n",
      "num_corrects / total_examples = 11712 / 13245\n",
      "training loss = 0.3273\n",
      "training accuracy = 0.8843\n",
      "num_test_corrects / test_total_examples = 11673 / 13245\n",
      "testing accuracy = 0.8813\n",
      "--------- epoch: 186 ---------\n",
      "num_corrects / total_examples = 11714 / 13245\n",
      "training loss = 0.3272\n",
      "training accuracy = 0.8844\n",
      "num_test_corrects / test_total_examples = 11674 / 13245\n",
      "testing accuracy = 0.8814\n",
      "--------- epoch: 187 ---------\n",
      "num_corrects / total_examples = 11715 / 13245\n",
      "training loss = 0.3270\n",
      "training accuracy = 0.8845\n",
      "num_test_corrects / test_total_examples = 11675 / 13245\n",
      "testing accuracy = 0.8815\n",
      "--------- epoch: 188 ---------\n",
      "num_corrects / total_examples = 11715 / 13245\n",
      "training loss = 0.3269\n",
      "training accuracy = 0.8845\n",
      "num_test_corrects / test_total_examples = 11676 / 13245\n",
      "testing accuracy = 0.8815\n",
      "--------- epoch: 189 ---------\n",
      "num_corrects / total_examples = 11716 / 13245\n",
      "training loss = 0.3268\n",
      "training accuracy = 0.8846\n",
      "num_test_corrects / test_total_examples = 11679 / 13245\n",
      "testing accuracy = 0.8818\n",
      "--------- epoch: 190 ---------\n",
      "num_corrects / total_examples = 11718 / 13245\n",
      "training loss = 0.3267\n",
      "training accuracy = 0.8847\n",
      "num_test_corrects / test_total_examples = 11680 / 13245\n",
      "testing accuracy = 0.8818\n",
      "--------- epoch: 191 ---------\n",
      "num_corrects / total_examples = 11719 / 13245\n",
      "training loss = 0.3266\n",
      "training accuracy = 0.8848\n",
      "num_test_corrects / test_total_examples = 11681 / 13245\n",
      "testing accuracy = 0.8819\n",
      "--------- epoch: 192 ---------\n",
      "num_corrects / total_examples = 11720 / 13245\n",
      "training loss = 0.3264\n",
      "training accuracy = 0.8849\n",
      "num_test_corrects / test_total_examples = 11681 / 13245\n",
      "testing accuracy = 0.8819\n",
      "--------- epoch: 193 ---------\n",
      "num_corrects / total_examples = 11723 / 13245\n",
      "training loss = 0.3263\n",
      "training accuracy = 0.8851\n",
      "num_test_corrects / test_total_examples = 11681 / 13245\n",
      "testing accuracy = 0.8819\n",
      "--------- epoch: 194 ---------\n",
      "num_corrects / total_examples = 11724 / 13245\n",
      "training loss = 0.3262\n",
      "training accuracy = 0.8852\n",
      "num_test_corrects / test_total_examples = 11682 / 13245\n",
      "testing accuracy = 0.8820\n",
      "--------- epoch: 195 ---------\n",
      "num_corrects / total_examples = 11723 / 13245\n",
      "training loss = 0.3261\n",
      "training accuracy = 0.8851\n",
      "num_test_corrects / test_total_examples = 11684 / 13245\n",
      "testing accuracy = 0.8821\n",
      "--------- epoch: 196 ---------\n",
      "num_corrects / total_examples = 11725 / 13245\n",
      "training loss = 0.3260\n",
      "training accuracy = 0.8852\n",
      "num_test_corrects / test_total_examples = 11684 / 13245\n",
      "testing accuracy = 0.8821\n",
      "--------- epoch: 197 ---------\n",
      "num_corrects / total_examples = 11726 / 13245\n",
      "training loss = 0.3259\n",
      "training accuracy = 0.8853\n",
      "num_test_corrects / test_total_examples = 11686 / 13245\n",
      "testing accuracy = 0.8823\n",
      "--------- epoch: 198 ---------\n",
      "num_corrects / total_examples = 11728 / 13245\n",
      "training loss = 0.3257\n",
      "training accuracy = 0.8855\n",
      "num_test_corrects / test_total_examples = 11685 / 13245\n",
      "testing accuracy = 0.8822\n",
      "--------- epoch: 199 ---------\n",
      "num_corrects / total_examples = 11730 / 13245\n",
      "training loss = 0.3256\n",
      "training accuracy = 0.8856\n",
      "num_test_corrects / test_total_examples = 11688 / 13245\n",
      "testing accuracy = 0.8824\n",
      "--------- epoch: 200 ---------\n",
      "num_corrects / total_examples = 11731 / 13245\n",
      "training loss = 0.3255\n",
      "training accuracy = 0.8857\n",
      "num_test_corrects / test_total_examples = 11688 / 13245\n",
      "testing accuracy = 0.8824\n",
      "--------- epoch: 201 ---------\n",
      "num_corrects / total_examples = 11732 / 13245\n",
      "training loss = 0.3254\n",
      "training accuracy = 0.8858\n",
      "num_test_corrects / test_total_examples = 11690 / 13245\n",
      "testing accuracy = 0.8826\n",
      "--------- epoch: 202 ---------\n",
      "num_corrects / total_examples = 11731 / 13245\n",
      "training loss = 0.3253\n",
      "training accuracy = 0.8857\n",
      "num_test_corrects / test_total_examples = 11692 / 13245\n",
      "testing accuracy = 0.8827\n",
      "--------- epoch: 203 ---------\n",
      "num_corrects / total_examples = 11728 / 13245\n",
      "training loss = 0.3252\n",
      "training accuracy = 0.8855\n",
      "num_test_corrects / test_total_examples = 11693 / 13245\n",
      "testing accuracy = 0.8828\n",
      "--------- epoch: 204 ---------\n",
      "num_corrects / total_examples = 11729 / 13245\n",
      "training loss = 0.3251\n",
      "training accuracy = 0.8855\n",
      "num_test_corrects / test_total_examples = 11694 / 13245\n",
      "testing accuracy = 0.8829\n",
      "--------- epoch: 205 ---------\n",
      "num_corrects / total_examples = 11728 / 13245\n",
      "training loss = 0.3250\n",
      "training accuracy = 0.8855\n",
      "num_test_corrects / test_total_examples = 11694 / 13245\n",
      "testing accuracy = 0.8829\n",
      "--------- epoch: 206 ---------\n",
      "num_corrects / total_examples = 11729 / 13245\n",
      "training loss = 0.3248\n",
      "training accuracy = 0.8855\n",
      "num_test_corrects / test_total_examples = 11696 / 13245\n",
      "testing accuracy = 0.8831\n",
      "--------- epoch: 207 ---------\n",
      "num_corrects / total_examples = 11730 / 13245\n",
      "training loss = 0.3247\n",
      "training accuracy = 0.8856\n",
      "num_test_corrects / test_total_examples = 11696 / 13245\n",
      "testing accuracy = 0.8831\n",
      "--------- epoch: 208 ---------\n",
      "num_corrects / total_examples = 11729 / 13245\n",
      "training loss = 0.3246\n",
      "training accuracy = 0.8855\n",
      "num_test_corrects / test_total_examples = 11697 / 13245\n",
      "testing accuracy = 0.8831\n",
      "--------- epoch: 209 ---------\n",
      "num_corrects / total_examples = 11732 / 13245\n",
      "training loss = 0.3245\n",
      "training accuracy = 0.8858\n",
      "num_test_corrects / test_total_examples = 11694 / 13245\n",
      "testing accuracy = 0.8829\n",
      "--------- epoch: 210 ---------\n",
      "num_corrects / total_examples = 11734 / 13245\n",
      "training loss = 0.3244\n",
      "training accuracy = 0.8859\n",
      "num_test_corrects / test_total_examples = 11694 / 13245\n",
      "testing accuracy = 0.8829\n",
      "--------- epoch: 211 ---------\n",
      "num_corrects / total_examples = 11737 / 13245\n",
      "training loss = 0.3243\n",
      "training accuracy = 0.8861\n",
      "num_test_corrects / test_total_examples = 11693 / 13245\n",
      "testing accuracy = 0.8828\n",
      "--------- epoch: 212 ---------\n",
      "num_corrects / total_examples = 11738 / 13245\n",
      "training loss = 0.3242\n",
      "training accuracy = 0.8862\n",
      "num_test_corrects / test_total_examples = 11693 / 13245\n",
      "testing accuracy = 0.8828\n",
      "--------- epoch: 213 ---------\n",
      "num_corrects / total_examples = 11738 / 13245\n",
      "training loss = 0.3241\n",
      "training accuracy = 0.8862\n",
      "num_test_corrects / test_total_examples = 11692 / 13245\n",
      "testing accuracy = 0.8827\n",
      "--------- epoch: 214 ---------\n",
      "num_corrects / total_examples = 11739 / 13245\n",
      "training loss = 0.3240\n",
      "training accuracy = 0.8863\n",
      "num_test_corrects / test_total_examples = 11694 / 13245\n",
      "testing accuracy = 0.8829\n",
      "--------- epoch: 215 ---------\n",
      "num_corrects / total_examples = 11740 / 13245\n",
      "training loss = 0.3239\n",
      "training accuracy = 0.8864\n",
      "num_test_corrects / test_total_examples = 11694 / 13245\n",
      "testing accuracy = 0.8829\n",
      "--------- epoch: 216 ---------\n",
      "num_corrects / total_examples = 11741 / 13245\n",
      "training loss = 0.3238\n",
      "training accuracy = 0.8864\n",
      "num_test_corrects / test_total_examples = 11695 / 13245\n",
      "testing accuracy = 0.8830\n",
      "--------- epoch: 217 ---------\n",
      "num_corrects / total_examples = 11741 / 13245\n",
      "training loss = 0.3237\n",
      "training accuracy = 0.8864\n",
      "num_test_corrects / test_total_examples = 11697 / 13245\n",
      "testing accuracy = 0.8831\n",
      "--------- epoch: 218 ---------\n",
      "num_corrects / total_examples = 11741 / 13245\n",
      "training loss = 0.3236\n",
      "training accuracy = 0.8864\n",
      "num_test_corrects / test_total_examples = 11697 / 13245\n",
      "testing accuracy = 0.8831\n",
      "--------- epoch: 219 ---------\n",
      "num_corrects / total_examples = 11741 / 13245\n",
      "training loss = 0.3235\n",
      "training accuracy = 0.8864\n",
      "num_test_corrects / test_total_examples = 11698 / 13245\n",
      "testing accuracy = 0.8832\n",
      "--------- epoch: 220 ---------\n",
      "num_corrects / total_examples = 11742 / 13245\n",
      "training loss = 0.3234\n",
      "training accuracy = 0.8865\n",
      "num_test_corrects / test_total_examples = 11698 / 13245\n",
      "testing accuracy = 0.8832\n",
      "--------- epoch: 221 ---------\n",
      "num_corrects / total_examples = 11744 / 13245\n",
      "training loss = 0.3233\n",
      "training accuracy = 0.8867\n",
      "num_test_corrects / test_total_examples = 11698 / 13245\n",
      "testing accuracy = 0.8832\n",
      "--------- epoch: 222 ---------\n",
      "num_corrects / total_examples = 11744 / 13245\n",
      "training loss = 0.3232\n",
      "training accuracy = 0.8867\n",
      "num_test_corrects / test_total_examples = 11699 / 13245\n",
      "testing accuracy = 0.8833\n",
      "--------- epoch: 223 ---------\n",
      "num_corrects / total_examples = 11744 / 13245\n",
      "training loss = 0.3231\n",
      "training accuracy = 0.8867\n",
      "num_test_corrects / test_total_examples = 11700 / 13245\n",
      "testing accuracy = 0.8834\n",
      "--------- epoch: 224 ---------\n",
      "num_corrects / total_examples = 11746 / 13245\n",
      "training loss = 0.3230\n",
      "training accuracy = 0.8868\n",
      "num_test_corrects / test_total_examples = 11702 / 13245\n",
      "testing accuracy = 0.8835\n",
      "--------- epoch: 225 ---------\n",
      "num_corrects / total_examples = 11747 / 13245\n",
      "training loss = 0.3229\n",
      "training accuracy = 0.8869\n",
      "num_test_corrects / test_total_examples = 11705 / 13245\n",
      "testing accuracy = 0.8837\n",
      "--------- epoch: 226 ---------\n",
      "num_corrects / total_examples = 11750 / 13245\n",
      "training loss = 0.3228\n",
      "training accuracy = 0.8871\n",
      "num_test_corrects / test_total_examples = 11706 / 13245\n",
      "testing accuracy = 0.8838\n",
      "--------- epoch: 227 ---------\n",
      "num_corrects / total_examples = 11752 / 13245\n",
      "training loss = 0.3227\n",
      "training accuracy = 0.8873\n",
      "num_test_corrects / test_total_examples = 11706 / 13245\n",
      "testing accuracy = 0.8838\n",
      "--------- epoch: 228 ---------\n",
      "num_corrects / total_examples = 11751 / 13245\n",
      "training loss = 0.3226\n",
      "training accuracy = 0.8872\n",
      "num_test_corrects / test_total_examples = 11707 / 13245\n",
      "testing accuracy = 0.8839\n",
      "--------- epoch: 229 ---------\n",
      "num_corrects / total_examples = 11750 / 13245\n",
      "training loss = 0.3225\n",
      "training accuracy = 0.8871\n",
      "num_test_corrects / test_total_examples = 11706 / 13245\n",
      "testing accuracy = 0.8838\n",
      "--------- epoch: 230 ---------\n",
      "num_corrects / total_examples = 11751 / 13245\n",
      "training loss = 0.3224\n",
      "training accuracy = 0.8872\n",
      "num_test_corrects / test_total_examples = 11706 / 13245\n",
      "testing accuracy = 0.8838\n",
      "--------- epoch: 231 ---------\n",
      "num_corrects / total_examples = 11749 / 13245\n",
      "training loss = 0.3223\n",
      "training accuracy = 0.8871\n",
      "num_test_corrects / test_total_examples = 11706 / 13245\n",
      "testing accuracy = 0.8838\n",
      "--------- epoch: 232 ---------\n",
      "num_corrects / total_examples = 11750 / 13245\n",
      "training loss = 0.3222\n",
      "training accuracy = 0.8871\n",
      "num_test_corrects / test_total_examples = 11706 / 13245\n",
      "testing accuracy = 0.8838\n",
      "--------- epoch: 233 ---------\n",
      "num_corrects / total_examples = 11751 / 13245\n",
      "training loss = 0.3221\n",
      "training accuracy = 0.8872\n",
      "num_test_corrects / test_total_examples = 11706 / 13245\n",
      "testing accuracy = 0.8838\n",
      "--------- epoch: 234 ---------\n",
      "num_corrects / total_examples = 11754 / 13245\n",
      "training loss = 0.3220\n",
      "training accuracy = 0.8874\n",
      "num_test_corrects / test_total_examples = 11706 / 13245\n",
      "testing accuracy = 0.8838\n",
      "--------- epoch: 235 ---------\n",
      "num_corrects / total_examples = 11755 / 13245\n",
      "training loss = 0.3219\n",
      "training accuracy = 0.8875\n",
      "num_test_corrects / test_total_examples = 11707 / 13245\n",
      "testing accuracy = 0.8839\n",
      "--------- epoch: 236 ---------\n",
      "num_corrects / total_examples = 11754 / 13245\n",
      "training loss = 0.3219\n",
      "training accuracy = 0.8874\n",
      "num_test_corrects / test_total_examples = 11707 / 13245\n",
      "testing accuracy = 0.8839\n",
      "--------- epoch: 237 ---------\n",
      "num_corrects / total_examples = 11756 / 13245\n",
      "training loss = 0.3218\n",
      "training accuracy = 0.8876\n",
      "num_test_corrects / test_total_examples = 11707 / 13245\n",
      "testing accuracy = 0.8839\n",
      "--------- epoch: 238 ---------\n",
      "num_corrects / total_examples = 11757 / 13245\n",
      "training loss = 0.3217\n",
      "training accuracy = 0.8877\n",
      "num_test_corrects / test_total_examples = 11708 / 13245\n",
      "testing accuracy = 0.8840\n",
      "--------- epoch: 239 ---------\n",
      "num_corrects / total_examples = 11758 / 13245\n",
      "training loss = 0.3216\n",
      "training accuracy = 0.8877\n",
      "num_test_corrects / test_total_examples = 11709 / 13245\n",
      "testing accuracy = 0.8840\n",
      "--------- epoch: 240 ---------\n",
      "num_corrects / total_examples = 11759 / 13245\n",
      "training loss = 0.3215\n",
      "training accuracy = 0.8878\n",
      "num_test_corrects / test_total_examples = 11709 / 13245\n",
      "testing accuracy = 0.8840\n",
      "--------- epoch: 241 ---------\n",
      "num_corrects / total_examples = 11760 / 13245\n",
      "training loss = 0.3214\n",
      "training accuracy = 0.8879\n",
      "num_test_corrects / test_total_examples = 11710 / 13245\n",
      "testing accuracy = 0.8841\n",
      "--------- epoch: 242 ---------\n",
      "num_corrects / total_examples = 11761 / 13245\n",
      "training loss = 0.3213\n",
      "training accuracy = 0.8880\n",
      "num_test_corrects / test_total_examples = 11711 / 13245\n",
      "testing accuracy = 0.8842\n",
      "--------- epoch: 243 ---------\n",
      "num_corrects / total_examples = 11762 / 13245\n",
      "training loss = 0.3212\n",
      "training accuracy = 0.8880\n",
      "num_test_corrects / test_total_examples = 11713 / 13245\n",
      "testing accuracy = 0.8843\n",
      "--------- epoch: 244 ---------\n",
      "num_corrects / total_examples = 11762 / 13245\n",
      "training loss = 0.3211\n",
      "training accuracy = 0.8880\n",
      "num_test_corrects / test_total_examples = 11716 / 13245\n",
      "testing accuracy = 0.8846\n",
      "--------- epoch: 245 ---------\n",
      "num_corrects / total_examples = 11760 / 13245\n",
      "training loss = 0.3211\n",
      "training accuracy = 0.8879\n",
      "num_test_corrects / test_total_examples = 11716 / 13245\n",
      "testing accuracy = 0.8846\n",
      "--------- epoch: 246 ---------\n",
      "num_corrects / total_examples = 11759 / 13245\n",
      "training loss = 0.3210\n",
      "training accuracy = 0.8878\n",
      "num_test_corrects / test_total_examples = 11717 / 13245\n",
      "testing accuracy = 0.8846\n",
      "--------- epoch: 247 ---------\n",
      "num_corrects / total_examples = 11759 / 13245\n",
      "training loss = 0.3209\n",
      "training accuracy = 0.8878\n",
      "num_test_corrects / test_total_examples = 11719 / 13245\n",
      "testing accuracy = 0.8848\n",
      "--------- epoch: 248 ---------\n",
      "num_corrects / total_examples = 11760 / 13245\n",
      "training loss = 0.3208\n",
      "training accuracy = 0.8879\n",
      "num_test_corrects / test_total_examples = 11720 / 13245\n",
      "testing accuracy = 0.8849\n",
      "--------- epoch: 249 ---------\n",
      "num_corrects / total_examples = 11759 / 13245\n",
      "training loss = 0.3207\n",
      "training accuracy = 0.8878\n",
      "num_test_corrects / test_total_examples = 11720 / 13245\n",
      "testing accuracy = 0.8849\n",
      "--------- epoch: 250 ---------\n",
      "num_corrects / total_examples = 11760 / 13245\n",
      "training loss = 0.3206\n",
      "training accuracy = 0.8879\n",
      "num_test_corrects / test_total_examples = 11720 / 13245\n",
      "testing accuracy = 0.8849\n",
      "--------- epoch: 251 ---------\n",
      "num_corrects / total_examples = 11760 / 13245\n",
      "training loss = 0.3205\n",
      "training accuracy = 0.8879\n",
      "num_test_corrects / test_total_examples = 11718 / 13245\n",
      "testing accuracy = 0.8847\n",
      "--------- epoch: 252 ---------\n",
      "num_corrects / total_examples = 11760 / 13245\n",
      "training loss = 0.3205\n",
      "training accuracy = 0.8879\n",
      "num_test_corrects / test_total_examples = 11719 / 13245\n",
      "testing accuracy = 0.8848\n",
      "--------- epoch: 253 ---------\n",
      "num_corrects / total_examples = 11761 / 13245\n",
      "training loss = 0.3204\n",
      "training accuracy = 0.8880\n",
      "num_test_corrects / test_total_examples = 11719 / 13245\n",
      "testing accuracy = 0.8848\n",
      "--------- epoch: 254 ---------\n",
      "num_corrects / total_examples = 11759 / 13245\n",
      "training loss = 0.3203\n",
      "training accuracy = 0.8878\n",
      "num_test_corrects / test_total_examples = 11721 / 13245\n",
      "testing accuracy = 0.8849\n",
      "--------- epoch: 255 ---------\n",
      "num_corrects / total_examples = 11760 / 13245\n",
      "training loss = 0.3202\n",
      "training accuracy = 0.8879\n",
      "num_test_corrects / test_total_examples = 11719 / 13245\n",
      "testing accuracy = 0.8848\n",
      "--------- epoch: 256 ---------\n",
      "num_corrects / total_examples = 11760 / 13245\n",
      "training loss = 0.3201\n",
      "training accuracy = 0.8879\n",
      "num_test_corrects / test_total_examples = 11720 / 13245\n",
      "testing accuracy = 0.8849\n",
      "--------- epoch: 257 ---------\n",
      "num_corrects / total_examples = 11760 / 13245\n",
      "training loss = 0.3200\n",
      "training accuracy = 0.8879\n",
      "num_test_corrects / test_total_examples = 11721 / 13245\n",
      "testing accuracy = 0.8849\n",
      "--------- epoch: 258 ---------\n",
      "num_corrects / total_examples = 11761 / 13245\n",
      "training loss = 0.3200\n",
      "training accuracy = 0.8880\n",
      "num_test_corrects / test_total_examples = 11723 / 13245\n",
      "testing accuracy = 0.8851\n",
      "--------- epoch: 259 ---------\n",
      "num_corrects / total_examples = 11761 / 13245\n",
      "training loss = 0.3199\n",
      "training accuracy = 0.8880\n",
      "num_test_corrects / test_total_examples = 11723 / 13245\n",
      "testing accuracy = 0.8851\n",
      "--------- epoch: 260 ---------\n",
      "num_corrects / total_examples = 11763 / 13245\n",
      "training loss = 0.3198\n",
      "training accuracy = 0.8881\n",
      "num_test_corrects / test_total_examples = 11724 / 13245\n",
      "testing accuracy = 0.8852\n",
      "--------- epoch: 261 ---------\n",
      "num_corrects / total_examples = 11762 / 13245\n",
      "training loss = 0.3197\n",
      "training accuracy = 0.8880\n",
      "num_test_corrects / test_total_examples = 11724 / 13245\n",
      "testing accuracy = 0.8852\n",
      "--------- epoch: 262 ---------\n",
      "num_corrects / total_examples = 11762 / 13245\n",
      "training loss = 0.3196\n",
      "training accuracy = 0.8880\n",
      "num_test_corrects / test_total_examples = 11725 / 13245\n",
      "testing accuracy = 0.8852\n",
      "--------- epoch: 263 ---------\n",
      "num_corrects / total_examples = 11763 / 13245\n",
      "training loss = 0.3196\n",
      "training accuracy = 0.8881\n",
      "num_test_corrects / test_total_examples = 11726 / 13245\n",
      "testing accuracy = 0.8853\n",
      "--------- epoch: 264 ---------\n",
      "num_corrects / total_examples = 11763 / 13245\n",
      "training loss = 0.3195\n",
      "training accuracy = 0.8881\n",
      "num_test_corrects / test_total_examples = 11727 / 13245\n",
      "testing accuracy = 0.8854\n",
      "--------- epoch: 265 ---------\n",
      "num_corrects / total_examples = 11763 / 13245\n",
      "training loss = 0.3194\n",
      "training accuracy = 0.8881\n",
      "num_test_corrects / test_total_examples = 11727 / 13245\n",
      "testing accuracy = 0.8854\n",
      "--------- epoch: 266 ---------\n",
      "num_corrects / total_examples = 11764 / 13245\n",
      "training loss = 0.3193\n",
      "training accuracy = 0.8882\n",
      "num_test_corrects / test_total_examples = 11728 / 13245\n",
      "testing accuracy = 0.8855\n",
      "--------- epoch: 267 ---------\n",
      "num_corrects / total_examples = 11764 / 13245\n",
      "training loss = 0.3192\n",
      "training accuracy = 0.8882\n",
      "num_test_corrects / test_total_examples = 11729 / 13245\n",
      "testing accuracy = 0.8855\n",
      "--------- epoch: 268 ---------\n",
      "num_corrects / total_examples = 11764 / 13245\n",
      "training loss = 0.3192\n",
      "training accuracy = 0.8882\n",
      "num_test_corrects / test_total_examples = 11728 / 13245\n",
      "testing accuracy = 0.8855\n",
      "--------- epoch: 269 ---------\n",
      "num_corrects / total_examples = 11764 / 13245\n",
      "training loss = 0.3191\n",
      "training accuracy = 0.8882\n",
      "num_test_corrects / test_total_examples = 11726 / 13245\n",
      "testing accuracy = 0.8853\n",
      "--------- epoch: 270 ---------\n",
      "num_corrects / total_examples = 11765 / 13245\n",
      "training loss = 0.3190\n",
      "training accuracy = 0.8883\n",
      "num_test_corrects / test_total_examples = 11726 / 13245\n",
      "testing accuracy = 0.8853\n",
      "--------- epoch: 271 ---------\n",
      "num_corrects / total_examples = 11764 / 13245\n",
      "training loss = 0.3189\n",
      "training accuracy = 0.8882\n",
      "num_test_corrects / test_total_examples = 11726 / 13245\n",
      "testing accuracy = 0.8853\n",
      "--------- epoch: 272 ---------\n",
      "num_corrects / total_examples = 11763 / 13245\n",
      "training loss = 0.3189\n",
      "training accuracy = 0.8881\n",
      "num_test_corrects / test_total_examples = 11727 / 13245\n",
      "testing accuracy = 0.8854\n",
      "--------- epoch: 273 ---------\n",
      "num_corrects / total_examples = 11763 / 13245\n",
      "training loss = 0.3188\n",
      "training accuracy = 0.8881\n",
      "num_test_corrects / test_total_examples = 11729 / 13245\n",
      "testing accuracy = 0.8855\n",
      "--------- epoch: 274 ---------\n",
      "num_corrects / total_examples = 11764 / 13245\n",
      "training loss = 0.3187\n",
      "training accuracy = 0.8882\n",
      "num_test_corrects / test_total_examples = 11729 / 13245\n",
      "testing accuracy = 0.8855\n",
      "--------- epoch: 275 ---------\n",
      "num_corrects / total_examples = 11765 / 13245\n",
      "training loss = 0.3186\n",
      "training accuracy = 0.8883\n",
      "num_test_corrects / test_total_examples = 11729 / 13245\n",
      "testing accuracy = 0.8855\n",
      "--------- epoch: 276 ---------\n",
      "num_corrects / total_examples = 11766 / 13245\n",
      "training loss = 0.3186\n",
      "training accuracy = 0.8883\n",
      "num_test_corrects / test_total_examples = 11729 / 13245\n",
      "testing accuracy = 0.8855\n",
      "--------- epoch: 277 ---------\n",
      "num_corrects / total_examples = 11766 / 13245\n",
      "training loss = 0.3185\n",
      "training accuracy = 0.8883\n",
      "num_test_corrects / test_total_examples = 11729 / 13245\n",
      "testing accuracy = 0.8855\n",
      "--------- epoch: 278 ---------\n",
      "num_corrects / total_examples = 11766 / 13245\n",
      "training loss = 0.3184\n",
      "training accuracy = 0.8883\n",
      "num_test_corrects / test_total_examples = 11730 / 13245\n",
      "testing accuracy = 0.8856\n",
      "--------- epoch: 279 ---------\n",
      "num_corrects / total_examples = 11767 / 13245\n",
      "training loss = 0.3183\n",
      "training accuracy = 0.8884\n",
      "num_test_corrects / test_total_examples = 11731 / 13245\n",
      "testing accuracy = 0.8857\n",
      "--------- epoch: 280 ---------\n",
      "num_corrects / total_examples = 11768 / 13245\n",
      "training loss = 0.3183\n",
      "training accuracy = 0.8885\n",
      "num_test_corrects / test_total_examples = 11734 / 13245\n",
      "testing accuracy = 0.8859\n",
      "--------- epoch: 281 ---------\n",
      "num_corrects / total_examples = 11768 / 13245\n",
      "training loss = 0.3182\n",
      "training accuracy = 0.8885\n",
      "num_test_corrects / test_total_examples = 11735 / 13245\n",
      "testing accuracy = 0.8860\n",
      "--------- epoch: 282 ---------\n",
      "num_corrects / total_examples = 11769 / 13245\n",
      "training loss = 0.3181\n",
      "training accuracy = 0.8886\n",
      "num_test_corrects / test_total_examples = 11735 / 13245\n",
      "testing accuracy = 0.8860\n",
      "--------- epoch: 283 ---------\n",
      "num_corrects / total_examples = 11770 / 13245\n",
      "training loss = 0.3181\n",
      "training accuracy = 0.8886\n",
      "num_test_corrects / test_total_examples = 11733 / 13245\n",
      "testing accuracy = 0.8858\n",
      "--------- epoch: 284 ---------\n",
      "num_corrects / total_examples = 11772 / 13245\n",
      "training loss = 0.3180\n",
      "training accuracy = 0.8888\n",
      "num_test_corrects / test_total_examples = 11734 / 13245\n",
      "testing accuracy = 0.8859\n",
      "--------- epoch: 285 ---------\n",
      "num_corrects / total_examples = 11772 / 13245\n",
      "training loss = 0.3179\n",
      "training accuracy = 0.8888\n",
      "num_test_corrects / test_total_examples = 11736 / 13245\n",
      "testing accuracy = 0.8861\n",
      "--------- epoch: 286 ---------\n",
      "num_corrects / total_examples = 11774 / 13245\n",
      "training loss = 0.3178\n",
      "training accuracy = 0.8889\n",
      "num_test_corrects / test_total_examples = 11737 / 13245\n",
      "testing accuracy = 0.8861\n",
      "--------- epoch: 287 ---------\n",
      "num_corrects / total_examples = 11774 / 13245\n",
      "training loss = 0.3178\n",
      "training accuracy = 0.8889\n",
      "num_test_corrects / test_total_examples = 11738 / 13245\n",
      "testing accuracy = 0.8862\n",
      "--------- epoch: 288 ---------\n",
      "num_corrects / total_examples = 11775 / 13245\n",
      "training loss = 0.3177\n",
      "training accuracy = 0.8890\n",
      "num_test_corrects / test_total_examples = 11738 / 13245\n",
      "testing accuracy = 0.8862\n",
      "--------- epoch: 289 ---------\n",
      "num_corrects / total_examples = 11775 / 13245\n",
      "training loss = 0.3176\n",
      "training accuracy = 0.8890\n",
      "num_test_corrects / test_total_examples = 11742 / 13245\n",
      "testing accuracy = 0.8865\n",
      "--------- epoch: 290 ---------\n",
      "num_corrects / total_examples = 11774 / 13245\n",
      "training loss = 0.3176\n",
      "training accuracy = 0.8889\n",
      "num_test_corrects / test_total_examples = 11742 / 13245\n",
      "testing accuracy = 0.8865\n",
      "--------- epoch: 291 ---------\n",
      "num_corrects / total_examples = 11775 / 13245\n",
      "training loss = 0.3175\n",
      "training accuracy = 0.8890\n",
      "num_test_corrects / test_total_examples = 11742 / 13245\n",
      "testing accuracy = 0.8865\n",
      "--------- epoch: 292 ---------\n",
      "num_corrects / total_examples = 11776 / 13245\n",
      "training loss = 0.3174\n",
      "training accuracy = 0.8891\n",
      "num_test_corrects / test_total_examples = 11742 / 13245\n",
      "testing accuracy = 0.8865\n",
      "--------- epoch: 293 ---------\n",
      "num_corrects / total_examples = 11775 / 13245\n",
      "training loss = 0.3173\n",
      "training accuracy = 0.8890\n",
      "num_test_corrects / test_total_examples = 11743 / 13245\n",
      "testing accuracy = 0.8866\n",
      "--------- epoch: 294 ---------\n",
      "num_corrects / total_examples = 11775 / 13245\n",
      "training loss = 0.3173\n",
      "training accuracy = 0.8890\n",
      "num_test_corrects / test_total_examples = 11743 / 13245\n",
      "testing accuracy = 0.8866\n",
      "--------- epoch: 295 ---------\n",
      "num_corrects / total_examples = 11775 / 13245\n",
      "training loss = 0.3172\n",
      "training accuracy = 0.8890\n",
      "num_test_corrects / test_total_examples = 11743 / 13245\n",
      "testing accuracy = 0.8866\n",
      "--------- epoch: 296 ---------\n",
      "num_corrects / total_examples = 11775 / 13245\n",
      "training loss = 0.3171\n",
      "training accuracy = 0.8890\n",
      "num_test_corrects / test_total_examples = 11743 / 13245\n",
      "testing accuracy = 0.8866\n",
      "--------- epoch: 297 ---------\n",
      "num_corrects / total_examples = 11775 / 13245\n",
      "training loss = 0.3171\n",
      "training accuracy = 0.8890\n",
      "num_test_corrects / test_total_examples = 11745 / 13245\n",
      "testing accuracy = 0.8867\n",
      "--------- epoch: 298 ---------\n",
      "num_corrects / total_examples = 11773 / 13245\n",
      "training loss = 0.3170\n",
      "training accuracy = 0.8889\n",
      "num_test_corrects / test_total_examples = 11746 / 13245\n",
      "testing accuracy = 0.8868\n",
      "--------- epoch: 299 ---------\n",
      "num_corrects / total_examples = 11774 / 13245\n",
      "training loss = 0.3169\n",
      "training accuracy = 0.8889\n",
      "num_test_corrects / test_total_examples = 11746 / 13245\n",
      "testing accuracy = 0.8868\n",
      "--------- epoch: 300 ---------\n",
      "num_corrects / total_examples = 11777 / 13245\n",
      "training loss = 0.3169\n",
      "training accuracy = 0.8892\n",
      "num_test_corrects / test_total_examples = 11747 / 13245\n",
      "testing accuracy = 0.8869\n",
      "--------- epoch: 301 ---------\n",
      "num_corrects / total_examples = 11779 / 13245\n",
      "training loss = 0.3168\n",
      "training accuracy = 0.8893\n",
      "num_test_corrects / test_total_examples = 11748 / 13245\n",
      "testing accuracy = 0.8870\n",
      "--------- epoch: 302 ---------\n",
      "num_corrects / total_examples = 11781 / 13245\n",
      "training loss = 0.3167\n",
      "training accuracy = 0.8895\n",
      "num_test_corrects / test_total_examples = 11747 / 13245\n",
      "testing accuracy = 0.8869\n",
      "--------- epoch: 303 ---------\n",
      "num_corrects / total_examples = 11781 / 13245\n",
      "training loss = 0.3167\n",
      "training accuracy = 0.8895\n",
      "num_test_corrects / test_total_examples = 11749 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 304 ---------\n",
      "num_corrects / total_examples = 11782 / 13245\n",
      "training loss = 0.3166\n",
      "training accuracy = 0.8895\n",
      "num_test_corrects / test_total_examples = 11750 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 305 ---------\n",
      "num_corrects / total_examples = 11784 / 13245\n",
      "training loss = 0.3165\n",
      "training accuracy = 0.8897\n",
      "num_test_corrects / test_total_examples = 11749 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 306 ---------\n",
      "num_corrects / total_examples = 11785 / 13245\n",
      "training loss = 0.3165\n",
      "training accuracy = 0.8898\n",
      "num_test_corrects / test_total_examples = 11749 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 307 ---------\n",
      "num_corrects / total_examples = 11785 / 13245\n",
      "training loss = 0.3164\n",
      "training accuracy = 0.8898\n",
      "num_test_corrects / test_total_examples = 11749 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 308 ---------\n",
      "num_corrects / total_examples = 11786 / 13245\n",
      "training loss = 0.3163\n",
      "training accuracy = 0.8898\n",
      "num_test_corrects / test_total_examples = 11750 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 309 ---------\n",
      "num_corrects / total_examples = 11786 / 13245\n",
      "training loss = 0.3163\n",
      "training accuracy = 0.8898\n",
      "num_test_corrects / test_total_examples = 11750 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 310 ---------\n",
      "num_corrects / total_examples = 11785 / 13245\n",
      "training loss = 0.3162\n",
      "training accuracy = 0.8898\n",
      "num_test_corrects / test_total_examples = 11750 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 311 ---------\n",
      "num_corrects / total_examples = 11785 / 13245\n",
      "training loss = 0.3162\n",
      "training accuracy = 0.8898\n",
      "num_test_corrects / test_total_examples = 11750 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 312 ---------\n",
      "num_corrects / total_examples = 11785 / 13245\n",
      "training loss = 0.3161\n",
      "training accuracy = 0.8898\n",
      "num_test_corrects / test_total_examples = 11750 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 313 ---------\n",
      "num_corrects / total_examples = 11784 / 13245\n",
      "training loss = 0.3160\n",
      "training accuracy = 0.8897\n",
      "num_test_corrects / test_total_examples = 11750 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 314 ---------\n",
      "num_corrects / total_examples = 11785 / 13245\n",
      "training loss = 0.3160\n",
      "training accuracy = 0.8898\n",
      "num_test_corrects / test_total_examples = 11750 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 315 ---------\n",
      "num_corrects / total_examples = 11788 / 13245\n",
      "training loss = 0.3159\n",
      "training accuracy = 0.8900\n",
      "num_test_corrects / test_total_examples = 11749 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 316 ---------\n",
      "num_corrects / total_examples = 11788 / 13245\n",
      "training loss = 0.3158\n",
      "training accuracy = 0.8900\n",
      "num_test_corrects / test_total_examples = 11749 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 317 ---------\n",
      "num_corrects / total_examples = 11789 / 13245\n",
      "training loss = 0.3158\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 11749 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 318 ---------\n",
      "num_corrects / total_examples = 11789 / 13245\n",
      "training loss = 0.3157\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 11750 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 319 ---------\n",
      "num_corrects / total_examples = 11789 / 13245\n",
      "training loss = 0.3156\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 11750 / 13245\n",
      "testing accuracy = 0.8871\n",
      "--------- epoch: 320 ---------\n",
      "num_corrects / total_examples = 11788 / 13245\n",
      "training loss = 0.3156\n",
      "training accuracy = 0.8900\n",
      "num_test_corrects / test_total_examples = 11751 / 13245\n",
      "testing accuracy = 0.8872\n",
      "--------- epoch: 321 ---------\n",
      "num_corrects / total_examples = 11788 / 13245\n",
      "training loss = 0.3155\n",
      "training accuracy = 0.8900\n",
      "num_test_corrects / test_total_examples = 11751 / 13245\n",
      "testing accuracy = 0.8872\n",
      "--------- epoch: 322 ---------\n",
      "num_corrects / total_examples = 11788 / 13245\n",
      "training loss = 0.3155\n",
      "training accuracy = 0.8900\n",
      "num_test_corrects / test_total_examples = 11752 / 13245\n",
      "testing accuracy = 0.8873\n",
      "--------- epoch: 323 ---------\n",
      "num_corrects / total_examples = 11790 / 13245\n",
      "training loss = 0.3154\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 11753 / 13245\n",
      "testing accuracy = 0.8874\n",
      "--------- epoch: 324 ---------\n",
      "num_corrects / total_examples = 11790 / 13245\n",
      "training loss = 0.3153\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 11753 / 13245\n",
      "testing accuracy = 0.8874\n",
      "--------- epoch: 325 ---------\n",
      "num_corrects / total_examples = 11790 / 13245\n",
      "training loss = 0.3153\n",
      "training accuracy = 0.8901\n",
      "num_test_corrects / test_total_examples = 11754 / 13245\n",
      "testing accuracy = 0.8874\n",
      "--------- epoch: 326 ---------\n",
      "num_corrects / total_examples = 11792 / 13245\n",
      "training loss = 0.3152\n",
      "training accuracy = 0.8903\n",
      "num_test_corrects / test_total_examples = 11754 / 13245\n",
      "testing accuracy = 0.8874\n",
      "--------- epoch: 327 ---------\n",
      "num_corrects / total_examples = 11792 / 13245\n",
      "training loss = 0.3152\n",
      "training accuracy = 0.8903\n",
      "num_test_corrects / test_total_examples = 11757 / 13245\n",
      "testing accuracy = 0.8877\n",
      "--------- epoch: 328 ---------\n",
      "num_corrects / total_examples = 11793 / 13245\n",
      "training loss = 0.3151\n",
      "training accuracy = 0.8904\n",
      "num_test_corrects / test_total_examples = 11757 / 13245\n",
      "testing accuracy = 0.8877\n",
      "--------- epoch: 329 ---------\n",
      "num_corrects / total_examples = 11795 / 13245\n",
      "training loss = 0.3150\n",
      "training accuracy = 0.8905\n",
      "num_test_corrects / test_total_examples = 11758 / 13245\n",
      "testing accuracy = 0.8877\n",
      "--------- epoch: 330 ---------\n",
      "num_corrects / total_examples = 11795 / 13245\n",
      "training loss = 0.3150\n",
      "training accuracy = 0.8905\n",
      "num_test_corrects / test_total_examples = 11758 / 13245\n",
      "testing accuracy = 0.8877\n",
      "--------- epoch: 331 ---------\n",
      "num_corrects / total_examples = 11795 / 13245\n",
      "training loss = 0.3149\n",
      "training accuracy = 0.8905\n",
      "num_test_corrects / test_total_examples = 11759 / 13245\n",
      "testing accuracy = 0.8878\n",
      "--------- epoch: 332 ---------\n",
      "num_corrects / total_examples = 11795 / 13245\n",
      "training loss = 0.3149\n",
      "training accuracy = 0.8905\n",
      "num_test_corrects / test_total_examples = 11761 / 13245\n",
      "testing accuracy = 0.8880\n",
      "--------- epoch: 333 ---------\n",
      "num_corrects / total_examples = 11797 / 13245\n",
      "training loss = 0.3148\n",
      "training accuracy = 0.8907\n",
      "num_test_corrects / test_total_examples = 11762 / 13245\n",
      "testing accuracy = 0.8880\n",
      "--------- epoch: 334 ---------\n",
      "num_corrects / total_examples = 11797 / 13245\n",
      "training loss = 0.3147\n",
      "training accuracy = 0.8907\n",
      "num_test_corrects / test_total_examples = 11761 / 13245\n",
      "testing accuracy = 0.8880\n",
      "--------- epoch: 335 ---------\n",
      "num_corrects / total_examples = 11797 / 13245\n",
      "training loss = 0.3147\n",
      "training accuracy = 0.8907\n",
      "num_test_corrects / test_total_examples = 11760 / 13245\n",
      "testing accuracy = 0.8879\n",
      "--------- epoch: 336 ---------\n",
      "num_corrects / total_examples = 11797 / 13245\n",
      "training loss = 0.3146\n",
      "training accuracy = 0.8907\n",
      "num_test_corrects / test_total_examples = 11760 / 13245\n",
      "testing accuracy = 0.8879\n",
      "--------- epoch: 337 ---------\n",
      "num_corrects / total_examples = 11798 / 13245\n",
      "training loss = 0.3146\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 11762 / 13245\n",
      "testing accuracy = 0.8880\n",
      "--------- epoch: 338 ---------\n",
      "num_corrects / total_examples = 11799 / 13245\n",
      "training loss = 0.3145\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 11763 / 13245\n",
      "testing accuracy = 0.8881\n",
      "--------- epoch: 339 ---------\n",
      "num_corrects / total_examples = 11799 / 13245\n",
      "training loss = 0.3145\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 11763 / 13245\n",
      "testing accuracy = 0.8881\n",
      "--------- epoch: 340 ---------\n",
      "num_corrects / total_examples = 11799 / 13245\n",
      "training loss = 0.3144\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 11764 / 13245\n",
      "testing accuracy = 0.8882\n",
      "--------- epoch: 341 ---------\n",
      "num_corrects / total_examples = 11800 / 13245\n",
      "training loss = 0.3143\n",
      "training accuracy = 0.8909\n",
      "num_test_corrects / test_total_examples = 11766 / 13245\n",
      "testing accuracy = 0.8883\n",
      "--------- epoch: 342 ---------\n",
      "num_corrects / total_examples = 11801 / 13245\n",
      "training loss = 0.3143\n",
      "training accuracy = 0.8910\n",
      "num_test_corrects / test_total_examples = 11766 / 13245\n",
      "testing accuracy = 0.8883\n",
      "--------- epoch: 343 ---------\n",
      "num_corrects / total_examples = 11801 / 13245\n",
      "training loss = 0.3142\n",
      "training accuracy = 0.8910\n",
      "num_test_corrects / test_total_examples = 11766 / 13245\n",
      "testing accuracy = 0.8883\n",
      "--------- epoch: 344 ---------\n",
      "num_corrects / total_examples = 11800 / 13245\n",
      "training loss = 0.3142\n",
      "training accuracy = 0.8909\n",
      "num_test_corrects / test_total_examples = 11766 / 13245\n",
      "testing accuracy = 0.8883\n",
      "--------- epoch: 345 ---------\n",
      "num_corrects / total_examples = 11800 / 13245\n",
      "training loss = 0.3141\n",
      "training accuracy = 0.8909\n",
      "num_test_corrects / test_total_examples = 11767 / 13245\n",
      "testing accuracy = 0.8884\n",
      "--------- epoch: 346 ---------\n",
      "num_corrects / total_examples = 11799 / 13245\n",
      "training loss = 0.3141\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 11768 / 13245\n",
      "testing accuracy = 0.8885\n",
      "--------- epoch: 347 ---------\n",
      "num_corrects / total_examples = 11798 / 13245\n",
      "training loss = 0.3140\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 11767 / 13245\n",
      "testing accuracy = 0.8884\n",
      "--------- epoch: 348 ---------\n",
      "num_corrects / total_examples = 11799 / 13245\n",
      "training loss = 0.3140\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 11768 / 13245\n",
      "testing accuracy = 0.8885\n",
      "--------- epoch: 349 ---------\n",
      "num_corrects / total_examples = 11799 / 13245\n",
      "training loss = 0.3139\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 11766 / 13245\n",
      "testing accuracy = 0.8883\n",
      "--------- epoch: 350 ---------\n",
      "num_corrects / total_examples = 11798 / 13245\n",
      "training loss = 0.3138\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 11765 / 13245\n",
      "testing accuracy = 0.8883\n",
      "--------- epoch: 351 ---------\n",
      "num_corrects / total_examples = 11798 / 13245\n",
      "training loss = 0.3138\n",
      "training accuracy = 0.8908\n",
      "num_test_corrects / test_total_examples = 11767 / 13245\n",
      "testing accuracy = 0.8884\n",
      "--------- epoch: 352 ---------\n",
      "num_corrects / total_examples = 11800 / 13245\n",
      "training loss = 0.3137\n",
      "training accuracy = 0.8909\n",
      "num_test_corrects / test_total_examples = 11766 / 13245\n",
      "testing accuracy = 0.8883\n",
      "--------- epoch: 353 ---------\n",
      "num_corrects / total_examples = 11800 / 13245\n",
      "training loss = 0.3137\n",
      "training accuracy = 0.8909\n",
      "num_test_corrects / test_total_examples = 11768 / 13245\n",
      "testing accuracy = 0.8885\n",
      "--------- epoch: 354 ---------\n",
      "num_corrects / total_examples = 11800 / 13245\n",
      "training loss = 0.3136\n",
      "training accuracy = 0.8909\n",
      "num_test_corrects / test_total_examples = 11767 / 13245\n",
      "testing accuracy = 0.8884\n",
      "--------- epoch: 355 ---------\n",
      "num_corrects / total_examples = 11802 / 13245\n",
      "training loss = 0.3136\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11767 / 13245\n",
      "testing accuracy = 0.8884\n",
      "--------- epoch: 356 ---------\n",
      "num_corrects / total_examples = 11802 / 13245\n",
      "training loss = 0.3135\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11767 / 13245\n",
      "testing accuracy = 0.8884\n",
      "--------- epoch: 357 ---------\n",
      "num_corrects / total_examples = 11801 / 13245\n",
      "training loss = 0.3135\n",
      "training accuracy = 0.8910\n",
      "num_test_corrects / test_total_examples = 11769 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 358 ---------\n",
      "num_corrects / total_examples = 11801 / 13245\n",
      "training loss = 0.3134\n",
      "training accuracy = 0.8910\n",
      "num_test_corrects / test_total_examples = 11769 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 359 ---------\n",
      "num_corrects / total_examples = 11800 / 13245\n",
      "training loss = 0.3134\n",
      "training accuracy = 0.8909\n",
      "num_test_corrects / test_total_examples = 11766 / 13245\n",
      "testing accuracy = 0.8883\n",
      "--------- epoch: 360 ---------\n",
      "num_corrects / total_examples = 11801 / 13245\n",
      "training loss = 0.3133\n",
      "training accuracy = 0.8910\n",
      "num_test_corrects / test_total_examples = 11767 / 13245\n",
      "testing accuracy = 0.8884\n",
      "--------- epoch: 361 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3132\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11767 / 13245\n",
      "testing accuracy = 0.8884\n",
      "--------- epoch: 362 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3132\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11767 / 13245\n",
      "testing accuracy = 0.8884\n",
      "--------- epoch: 363 ---------\n",
      "num_corrects / total_examples = 11803 / 13245\n",
      "training loss = 0.3131\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11768 / 13245\n",
      "testing accuracy = 0.8885\n",
      "--------- epoch: 364 ---------\n",
      "num_corrects / total_examples = 11803 / 13245\n",
      "training loss = 0.3131\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11768 / 13245\n",
      "testing accuracy = 0.8885\n",
      "--------- epoch: 365 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3130\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11768 / 13245\n",
      "testing accuracy = 0.8885\n",
      "--------- epoch: 366 ---------\n",
      "num_corrects / total_examples = 11803 / 13245\n",
      "training loss = 0.3130\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11768 / 13245\n",
      "testing accuracy = 0.8885\n",
      "--------- epoch: 367 ---------\n",
      "num_corrects / total_examples = 11802 / 13245\n",
      "training loss = 0.3129\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11769 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 368 ---------\n",
      "num_corrects / total_examples = 11802 / 13245\n",
      "training loss = 0.3129\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11769 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 369 ---------\n",
      "num_corrects / total_examples = 11803 / 13245\n",
      "training loss = 0.3128\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 370 ---------\n",
      "num_corrects / total_examples = 11803 / 13245\n",
      "training loss = 0.3128\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 371 ---------\n",
      "num_corrects / total_examples = 11802 / 13245\n",
      "training loss = 0.3127\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 372 ---------\n",
      "num_corrects / total_examples = 11802 / 13245\n",
      "training loss = 0.3127\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11769 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 373 ---------\n",
      "num_corrects / total_examples = 11803 / 13245\n",
      "training loss = 0.3126\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 374 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3126\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 375 ---------\n",
      "num_corrects / total_examples = 11806 / 13245\n",
      "training loss = 0.3125\n",
      "training accuracy = 0.8914\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 376 ---------\n",
      "num_corrects / total_examples = 11806 / 13245\n",
      "training loss = 0.3125\n",
      "training accuracy = 0.8914\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 377 ---------\n",
      "num_corrects / total_examples = 11806 / 13245\n",
      "training loss = 0.3124\n",
      "training accuracy = 0.8914\n",
      "num_test_corrects / test_total_examples = 11772 / 13245\n",
      "testing accuracy = 0.8888\n",
      "--------- epoch: 378 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3124\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11772 / 13245\n",
      "testing accuracy = 0.8888\n",
      "--------- epoch: 379 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3123\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11773 / 13245\n",
      "testing accuracy = 0.8889\n",
      "--------- epoch: 380 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3123\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11773 / 13245\n",
      "testing accuracy = 0.8889\n",
      "--------- epoch: 381 ---------\n",
      "num_corrects / total_examples = 11806 / 13245\n",
      "training loss = 0.3122\n",
      "training accuracy = 0.8914\n",
      "num_test_corrects / test_total_examples = 11771 / 13245\n",
      "testing accuracy = 0.8887\n",
      "--------- epoch: 382 ---------\n",
      "num_corrects / total_examples = 11806 / 13245\n",
      "training loss = 0.3122\n",
      "training accuracy = 0.8914\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 383 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3121\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 384 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3121\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 385 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3120\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 386 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3120\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 387 ---------\n",
      "num_corrects / total_examples = 11803 / 13245\n",
      "training loss = 0.3119\n",
      "training accuracy = 0.8911\n",
      "num_test_corrects / test_total_examples = 11769 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 388 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3119\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11769 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 389 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3118\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11769 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 390 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3118\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11770 / 13245\n",
      "testing accuracy = 0.8886\n",
      "--------- epoch: 391 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3117\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11771 / 13245\n",
      "testing accuracy = 0.8887\n",
      "--------- epoch: 392 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3117\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11772 / 13245\n",
      "testing accuracy = 0.8888\n",
      "--------- epoch: 393 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3116\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11773 / 13245\n",
      "testing accuracy = 0.8889\n",
      "--------- epoch: 394 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3116\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11772 / 13245\n",
      "testing accuracy = 0.8888\n",
      "--------- epoch: 395 ---------\n",
      "num_corrects / total_examples = 11805 / 13245\n",
      "training loss = 0.3115\n",
      "training accuracy = 0.8913\n",
      "num_test_corrects / test_total_examples = 11773 / 13245\n",
      "testing accuracy = 0.8889\n",
      "--------- epoch: 396 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3115\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11775 / 13245\n",
      "testing accuracy = 0.8890\n",
      "--------- epoch: 397 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3115\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 398 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3114\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 399 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3114\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 400 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3113\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 401 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3113\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11775 / 13245\n",
      "testing accuracy = 0.8890\n",
      "--------- epoch: 402 ---------\n",
      "num_corrects / total_examples = 11804 / 13245\n",
      "training loss = 0.3112\n",
      "training accuracy = 0.8912\n",
      "num_test_corrects / test_total_examples = 11775 / 13245\n",
      "testing accuracy = 0.8890\n",
      "--------- epoch: 403 ---------\n",
      "num_corrects / total_examples = 11808 / 13245\n",
      "training loss = 0.3112\n",
      "training accuracy = 0.8915\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 404 ---------\n",
      "num_corrects / total_examples = 11809 / 13245\n",
      "training loss = 0.3111\n",
      "training accuracy = 0.8916\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 405 ---------\n",
      "num_corrects / total_examples = 11809 / 13245\n",
      "training loss = 0.3111\n",
      "training accuracy = 0.8916\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 406 ---------\n",
      "num_corrects / total_examples = 11810 / 13245\n",
      "training loss = 0.3110\n",
      "training accuracy = 0.8917\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 407 ---------\n",
      "num_corrects / total_examples = 11811 / 13245\n",
      "training loss = 0.3110\n",
      "training accuracy = 0.8917\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 408 ---------\n",
      "num_corrects / total_examples = 11813 / 13245\n",
      "training loss = 0.3109\n",
      "training accuracy = 0.8919\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 409 ---------\n",
      "num_corrects / total_examples = 11813 / 13245\n",
      "training loss = 0.3109\n",
      "training accuracy = 0.8919\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 410 ---------\n",
      "num_corrects / total_examples = 11812 / 13245\n",
      "training loss = 0.3109\n",
      "training accuracy = 0.8918\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 411 ---------\n",
      "num_corrects / total_examples = 11813 / 13245\n",
      "training loss = 0.3108\n",
      "training accuracy = 0.8919\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 412 ---------\n",
      "num_corrects / total_examples = 11814 / 13245\n",
      "training loss = 0.3108\n",
      "training accuracy = 0.8920\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 413 ---------\n",
      "num_corrects / total_examples = 11814 / 13245\n",
      "training loss = 0.3107\n",
      "training accuracy = 0.8920\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 414 ---------\n",
      "num_corrects / total_examples = 11815 / 13245\n",
      "training loss = 0.3107\n",
      "training accuracy = 0.8920\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 415 ---------\n",
      "num_corrects / total_examples = 11815 / 13245\n",
      "training loss = 0.3106\n",
      "training accuracy = 0.8920\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 416 ---------\n",
      "num_corrects / total_examples = 11816 / 13245\n",
      "training loss = 0.3106\n",
      "training accuracy = 0.8921\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 417 ---------\n",
      "num_corrects / total_examples = 11817 / 13245\n",
      "training loss = 0.3105\n",
      "training accuracy = 0.8922\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 418 ---------\n",
      "num_corrects / total_examples = 11817 / 13245\n",
      "training loss = 0.3105\n",
      "training accuracy = 0.8922\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 419 ---------\n",
      "num_corrects / total_examples = 11818 / 13245\n",
      "training loss = 0.3105\n",
      "training accuracy = 0.8923\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 420 ---------\n",
      "num_corrects / total_examples = 11818 / 13245\n",
      "training loss = 0.3104\n",
      "training accuracy = 0.8923\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 421 ---------\n",
      "num_corrects / total_examples = 11818 / 13245\n",
      "training loss = 0.3104\n",
      "training accuracy = 0.8923\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 422 ---------\n",
      "num_corrects / total_examples = 11818 / 13245\n",
      "training loss = 0.3103\n",
      "training accuracy = 0.8923\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 423 ---------\n",
      "num_corrects / total_examples = 11820 / 13245\n",
      "training loss = 0.3103\n",
      "training accuracy = 0.8924\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 424 ---------\n",
      "num_corrects / total_examples = 11820 / 13245\n",
      "training loss = 0.3102\n",
      "training accuracy = 0.8924\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 425 ---------\n",
      "num_corrects / total_examples = 11819 / 13245\n",
      "training loss = 0.3102\n",
      "training accuracy = 0.8923\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 426 ---------\n",
      "num_corrects / total_examples = 11820 / 13245\n",
      "training loss = 0.3101\n",
      "training accuracy = 0.8924\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 427 ---------\n",
      "num_corrects / total_examples = 11821 / 13245\n",
      "training loss = 0.3101\n",
      "training accuracy = 0.8925\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 428 ---------\n",
      "num_corrects / total_examples = 11821 / 13245\n",
      "training loss = 0.3101\n",
      "training accuracy = 0.8925\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 429 ---------\n",
      "num_corrects / total_examples = 11821 / 13245\n",
      "training loss = 0.3100\n",
      "training accuracy = 0.8925\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 430 ---------\n",
      "num_corrects / total_examples = 11822 / 13245\n",
      "training loss = 0.3100\n",
      "training accuracy = 0.8926\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 431 ---------\n",
      "num_corrects / total_examples = 11822 / 13245\n",
      "training loss = 0.3099\n",
      "training accuracy = 0.8926\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 432 ---------\n",
      "num_corrects / total_examples = 11822 / 13245\n",
      "training loss = 0.3099\n",
      "training accuracy = 0.8926\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 433 ---------\n",
      "num_corrects / total_examples = 11822 / 13245\n",
      "training loss = 0.3099\n",
      "training accuracy = 0.8926\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 434 ---------\n",
      "num_corrects / total_examples = 11823 / 13245\n",
      "training loss = 0.3098\n",
      "training accuracy = 0.8926\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 435 ---------\n",
      "num_corrects / total_examples = 11824 / 13245\n",
      "training loss = 0.3098\n",
      "training accuracy = 0.8927\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 436 ---------\n",
      "num_corrects / total_examples = 11824 / 13245\n",
      "training loss = 0.3097\n",
      "training accuracy = 0.8927\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 437 ---------\n",
      "num_corrects / total_examples = 11824 / 13245\n",
      "training loss = 0.3097\n",
      "training accuracy = 0.8927\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 438 ---------\n",
      "num_corrects / total_examples = 11824 / 13245\n",
      "training loss = 0.3096\n",
      "training accuracy = 0.8927\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 439 ---------\n",
      "num_corrects / total_examples = 11825 / 13245\n",
      "training loss = 0.3096\n",
      "training accuracy = 0.8928\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 440 ---------\n",
      "num_corrects / total_examples = 11825 / 13245\n",
      "training loss = 0.3096\n",
      "training accuracy = 0.8928\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 441 ---------\n",
      "num_corrects / total_examples = 11825 / 13245\n",
      "training loss = 0.3095\n",
      "training accuracy = 0.8928\n",
      "num_test_corrects / test_total_examples = 11775 / 13245\n",
      "testing accuracy = 0.8890\n",
      "--------- epoch: 442 ---------\n",
      "num_corrects / total_examples = 11826 / 13245\n",
      "training loss = 0.3095\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 443 ---------\n",
      "num_corrects / total_examples = 11826 / 13245\n",
      "training loss = 0.3094\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 444 ---------\n",
      "num_corrects / total_examples = 11825 / 13245\n",
      "training loss = 0.3094\n",
      "training accuracy = 0.8928\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 445 ---------\n",
      "num_corrects / total_examples = 11826 / 13245\n",
      "training loss = 0.3094\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 446 ---------\n",
      "num_corrects / total_examples = 11825 / 13245\n",
      "training loss = 0.3093\n",
      "training accuracy = 0.8928\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 447 ---------\n",
      "num_corrects / total_examples = 11826 / 13245\n",
      "training loss = 0.3093\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 448 ---------\n",
      "num_corrects / total_examples = 11827 / 13245\n",
      "training loss = 0.3092\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 449 ---------\n",
      "num_corrects / total_examples = 11825 / 13245\n",
      "training loss = 0.3092\n",
      "training accuracy = 0.8928\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 450 ---------\n",
      "num_corrects / total_examples = 11825 / 13245\n",
      "training loss = 0.3092\n",
      "training accuracy = 0.8928\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 451 ---------\n",
      "num_corrects / total_examples = 11826 / 13245\n",
      "training loss = 0.3091\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 452 ---------\n",
      "num_corrects / total_examples = 11826 / 13245\n",
      "training loss = 0.3091\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 453 ---------\n",
      "num_corrects / total_examples = 11826 / 13245\n",
      "training loss = 0.3090\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 454 ---------\n",
      "num_corrects / total_examples = 11827 / 13245\n",
      "training loss = 0.3090\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 455 ---------\n",
      "num_corrects / total_examples = 11827 / 13245\n",
      "training loss = 0.3090\n",
      "training accuracy = 0.8929\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 456 ---------\n",
      "num_corrects / total_examples = 11829 / 13245\n",
      "training loss = 0.3089\n",
      "training accuracy = 0.8931\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 457 ---------\n",
      "num_corrects / total_examples = 11829 / 13245\n",
      "training loss = 0.3089\n",
      "training accuracy = 0.8931\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 458 ---------\n",
      "num_corrects / total_examples = 11830 / 13245\n",
      "training loss = 0.3088\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 459 ---------\n",
      "num_corrects / total_examples = 11831 / 13245\n",
      "training loss = 0.3088\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 460 ---------\n",
      "num_corrects / total_examples = 11832 / 13245\n",
      "training loss = 0.3088\n",
      "training accuracy = 0.8933\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 461 ---------\n",
      "num_corrects / total_examples = 11833 / 13245\n",
      "training loss = 0.3087\n",
      "training accuracy = 0.8934\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 462 ---------\n",
      "num_corrects / total_examples = 11832 / 13245\n",
      "training loss = 0.3087\n",
      "training accuracy = 0.8933\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 463 ---------\n",
      "num_corrects / total_examples = 11832 / 13245\n",
      "training loss = 0.3086\n",
      "training accuracy = 0.8933\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 464 ---------\n",
      "num_corrects / total_examples = 11832 / 13245\n",
      "training loss = 0.3086\n",
      "training accuracy = 0.8933\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 465 ---------\n",
      "num_corrects / total_examples = 11832 / 13245\n",
      "training loss = 0.3086\n",
      "training accuracy = 0.8933\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 466 ---------\n",
      "num_corrects / total_examples = 11831 / 13245\n",
      "training loss = 0.3085\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 467 ---------\n",
      "num_corrects / total_examples = 11831 / 13245\n",
      "training loss = 0.3085\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 468 ---------\n",
      "num_corrects / total_examples = 11831 / 13245\n",
      "training loss = 0.3084\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 469 ---------\n",
      "num_corrects / total_examples = 11831 / 13245\n",
      "training loss = 0.3084\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 11775 / 13245\n",
      "testing accuracy = 0.8890\n",
      "--------- epoch: 470 ---------\n",
      "num_corrects / total_examples = 11831 / 13245\n",
      "training loss = 0.3084\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 11775 / 13245\n",
      "testing accuracy = 0.8890\n",
      "--------- epoch: 471 ---------\n",
      "num_corrects / total_examples = 11831 / 13245\n",
      "training loss = 0.3083\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 11775 / 13245\n",
      "testing accuracy = 0.8890\n",
      "--------- epoch: 472 ---------\n",
      "num_corrects / total_examples = 11831 / 13245\n",
      "training loss = 0.3083\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 11775 / 13245\n",
      "testing accuracy = 0.8890\n",
      "--------- epoch: 473 ---------\n",
      "num_corrects / total_examples = 11831 / 13245\n",
      "training loss = 0.3083\n",
      "training accuracy = 0.8932\n",
      "num_test_corrects / test_total_examples = 11775 / 13245\n",
      "testing accuracy = 0.8890\n",
      "--------- epoch: 474 ---------\n",
      "num_corrects / total_examples = 11832 / 13245\n",
      "training loss = 0.3082\n",
      "training accuracy = 0.8933\n",
      "num_test_corrects / test_total_examples = 11773 / 13245\n",
      "testing accuracy = 0.8889\n",
      "--------- epoch: 475 ---------\n",
      "num_corrects / total_examples = 11833 / 13245\n",
      "training loss = 0.3082\n",
      "training accuracy = 0.8934\n",
      "num_test_corrects / test_total_examples = 11774 / 13245\n",
      "testing accuracy = 0.8889\n",
      "--------- epoch: 476 ---------\n",
      "num_corrects / total_examples = 11834 / 13245\n",
      "training loss = 0.3081\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11775 / 13245\n",
      "testing accuracy = 0.8890\n",
      "--------- epoch: 477 ---------\n",
      "num_corrects / total_examples = 11834 / 13245\n",
      "training loss = 0.3081\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 478 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3081\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 479 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3080\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 480 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3080\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11776 / 13245\n",
      "testing accuracy = 0.8891\n",
      "--------- epoch: 481 ---------\n",
      "num_corrects / total_examples = 11836 / 13245\n",
      "training loss = 0.3080\n",
      "training accuracy = 0.8936\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 482 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3079\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 483 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3079\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 484 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3078\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 485 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3078\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 486 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3078\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 487 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3077\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 488 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3077\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 489 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3077\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 490 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3076\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11778 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 491 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3076\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 492 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3076\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11777 / 13245\n",
      "testing accuracy = 0.8892\n",
      "--------- epoch: 493 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3075\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 494 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3075\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 495 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3075\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 496 ---------\n",
      "num_corrects / total_examples = 11836 / 13245\n",
      "training loss = 0.3074\n",
      "training accuracy = 0.8936\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 497 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3074\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 498 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3073\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 499 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3073\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n",
      "--------- epoch: 500 ---------\n",
      "num_corrects / total_examples = 11835 / 13245\n",
      "training loss = 0.3073\n",
      "training accuracy = 0.8935\n",
      "num_test_corrects / test_total_examples = 11779 / 13245\n",
      "testing accuracy = 0.8893\n"
     ]
    }
   ],
   "source": [
    "def train(model):\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for e in range(epoch):\n",
    "        print(f\"--------- epoch: {e+1} ---------\")\n",
    "        # training\n",
    "        train_loss = 0.0\n",
    "        corrects = 0\n",
    "        total_examples = 0\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()  # zero the gradients\n",
    "            # prepare data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).to(torch.float32)\n",
    "            # the forward pass\n",
    "            y_pred = model(x)\n",
    "            y_pred = y_pred.reshape(y.shape)\n",
    "            # the backward pass\n",
    "            loss = criterion(y_pred, y)  # calculate the loss\n",
    "            loss.backward()  # get the gradients\n",
    "            optimizer.step()  # update the params based on the gradients\n",
    "            # collect training results\n",
    "            train_loss += loss.item()\n",
    "            corrects += torch.sum((y_pred.round() == y))\n",
    "            total_examples += len(y)\n",
    "\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "        train_accuracies.append(corrects / total_examples)\n",
    "        print(f\"num_corrects / total_examples = {corrects.item()} / {total_examples}\")\n",
    "        print(f\"training loss = {train_losses[-1]:.4f}\")\n",
    "        print(f\"training accuracy = {train_accuracies[-1]:.4f}\")\n",
    "        # print(total_examples)\n",
    "\n",
    "        # testing\n",
    "        test_corrects = 0\n",
    "        test_total_examples = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (x, y) in enumerate(test_loader):\n",
    "                # prepare data\n",
    "                x = x.to(device)\n",
    "                y = y.to(device).to(torch.float32)\n",
    "                # the forward pass\n",
    "                y_pred = model(x)\n",
    "                y_pred = y_pred.reshape(y.shape)\n",
    "                # collect testing results\n",
    "                test_corrects += torch.sum((y_pred.round() == y))\n",
    "                test_total_examples += len(y)\n",
    "\n",
    "        test_accuracies.append(test_corrects.item() / test_total_examples)\n",
    "        print(f\"num_test_corrects / test_total_examples = {test_corrects.item()} / {test_total_examples}\")\n",
    "        print(f\"testing accuracy = {test_accuracies[-1]:.4f}\")\n",
    "\n",
    "    return train_losses, train_accuracies, test_accuracies\n",
    "\n",
    "train_losses, train_accuracies, test_accuracies = train(lr_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3681e7a9a0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f361034ff10>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if device.type == 'cuda':\n",
    "    train_accuracies = [acc.cpu() for acc in train_accuracies]\n",
    "plt.plot(train_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f361029f910>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max train accuracy = 0.8936202526092529\n",
      "max test accuracy = 0.8893167232918082\n"
     ]
    }
   ],
   "source": [
    "print(f\"max train accuracy = {max(train_accuracies)}\")\n",
    "print(f\"max test accuracy = {max(test_accuracies)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model's parameters after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-7.0723e-01,  2.3166e-01, -1.0806e+00, -1.1813e+00, -1.2395e+00,\n",
      "         -1.0088e+00, -8.0522e-01, -4.2706e-01, -1.4465e-01,  2.6792e-01,\n",
      "          6.0549e-01,  1.0214e+00,  1.2795e+00,  1.3862e+00,  1.3916e+00,\n",
      "          1.4228e+00,  1.3954e+00,  1.2803e+00,  1.0397e+00,  9.8549e-01,\n",
      "          1.0222e+00,  1.0583e+00,  1.0066e+00,  1.2150e+00,  1.2875e+00,\n",
      "          1.4060e+00,  1.5296e+00,  1.4570e+00,  1.4517e+00,  1.1210e+00,\n",
      "          6.1198e-01,  2.5064e-01, -4.4335e-01, -1.1471e+00, -1.9848e+00,\n",
      "         -2.9147e+00, -3.7443e+00, -4.2132e+00, -4.6427e+00, -4.8241e+00,\n",
      "         -4.9234e+00, -5.0617e+00, -4.9285e+00, -4.7380e+00, -4.0394e+00,\n",
      "         -2.5894e+00, -7.4513e-01,  1.1292e+00,  2.7366e+00,  3.7419e+00,\n",
      "          4.4980e+00,  4.8511e+00,  5.3533e+00,  7.6196e+00,  9.5158e+00,\n",
      "          9.3011e+00,  7.7108e+00,  2.8401e+00, -1.6202e+00, -1.7046e+00,\n",
      "         -1.4540e+00, -4.9301e+00, -7.1217e+00, -2.5923e+00,  1.4171e+00,\n",
      "          4.9544e+00,  2.0151e+00, -3.5169e+00, -7.5940e+00,  3.9090e+00,\n",
      "          1.9221e+01,  8.4398e+00, -6.2782e+00, -7.9425e+00, -6.2925e+00,\n",
      "         -3.5564e+00, -6.1355e-01,  2.8443e-01,  3.6491e-01, -1.0052e+00,\n",
      "         -2.7945e+00, -4.0458e+00, -5.2164e+00, -6.2624e+00, -6.1657e+00,\n",
      "         -4.1911e+00, -1.5289e+00,  1.0719e+00,  3.3470e+00,  3.8221e+00,\n",
      "          3.8296e+00,  3.6850e+00,  3.4263e+00,  3.2802e+00,  2.9369e+00,\n",
      "          2.4221e+00,  1.8402e+00,  1.0805e+00,  2.1213e-01, -6.7641e-01,\n",
      "         -1.2724e+00, -1.4293e+00, -1.4244e+00, -1.2659e+00, -1.1720e+00,\n",
      "         -9.8966e-01, -9.3476e-01, -7.2451e-01, -4.5979e-01, -3.6441e-01,\n",
      "         -3.4509e-01, -8.3291e-02,  6.7752e-03,  2.2802e-01,  2.6067e-01,\n",
      "          1.7016e-01,  2.2898e-01,  2.9843e-01,  3.5194e-01,  3.2108e-01,\n",
      "          2.2702e-01,  1.5655e-02, -2.5381e-01, -4.7820e-01, -8.9042e-01,\n",
      "         -9.8072e-01, -9.7504e-01, -1.0529e+00]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([2.3370], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in lr_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model params\n",
    "torch.save(lr_model.state_dict(), project_path/ 'weights' / 'ecg_float' / 'lr_model_epoch500_lr1e-3.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Inference with different number of test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "lr_model = LinearModel(128, 1)\n",
    "lr_model.to(device)\n",
    "lr_model.load_state_dict(torch.load((project_path/ 'weights' / 'ecg_float' / 'lr_model_epoch500_lr1e-3.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(inference_dataset) = 1000\n",
      "num_test_corrects / test_total_examples = 879 / 1000\n",
      "testing accuracy = 87.90%\n"
     ]
    }
   ],
   "source": [
    "inference_samples = 1000\n",
    "inference_dataset = torch.utils.data.Subset(test_dataset, indices=list(range(inference_samples)))\n",
    "inference_dataloader = DataLoader(inference_dataset, batch_size=batch_size)\n",
    "print(f\"{len(inference_dataset) = }\")\n",
    "\n",
    "# sample0 = inference_dataset[0]\n",
    "# print(sample0[0].shape)\n",
    "# plt.plot(sample0[0][0])\n",
    "\n",
    "# inference\n",
    "def inference(model, dataloader):\n",
    "    test_corrects = 0\n",
    "    test_total_examples = 0\n",
    "    test_accuracies = []\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(dataloader):\n",
    "            # prepare data\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).to(torch.float32)\n",
    "            # the forward pass\n",
    "            y_pred = model(x)\n",
    "            y_pred = y_pred.reshape(y.shape)\n",
    "            # collect testing results\n",
    "            test_corrects += torch.sum((y_pred.round() == y))\n",
    "            test_total_examples += len(y)\n",
    "\n",
    "    test_accuracies.append(test_corrects.item() / test_total_examples)\n",
    "    print(f\"num_test_corrects / test_total_examples = {test_corrects.item()} / {test_total_examples}\")\n",
    "    print(f\"testing accuracy = {test_accuracies[-1]*100:.2f}%\")\n",
    "\n",
    "inference(lr_model, inference_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hesplitnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "399b82d93c603256e32b1f50bab1e44304c1a63f6ab0c291ce7489d4e70c0394"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
