{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import MNIST \n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import brevitas.nn as qnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/dk/Desktop/projects/PocketHHE/data/mnist')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_path = Path.cwd().parents[1]\n",
    "mnist_path = project_path/'data/mnist'\n",
    "mnist_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/dk/Desktop/projects/PocketHHE/weights/mnists/qat/quant_1fc_2bits_mnist_plain_2bits_weights.pth')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_bit_width = 2  # quantize weights into [-1, 0, 1]\n",
    "input_bit_width = 2  # quantize dataset into [0,1,2,3]\n",
    "save_weight_path = project_path/f\"weights/mnists/qat/quant_1fc_{input_bit_width}bits_mnist_plain_{weight_bit_width}bits_weights.pth\"\n",
    "save_weight_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset and dataloader for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options:  \n",
    "\n",
    "0. Quantize into [0, 0.25, 0.5, 0.75, 1]\n",
    "1. Quantize into [0, 1]\n",
    "2. Quantize into [0, 1, 2, 3]\n",
    "3. Quantize into [0, 1, 2, 3, ..., 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_processing(option: int):\n",
    "    if option == 0:\n",
    "        transform = transforms.Compose([\n",
    "            ToTensor(),\n",
    "            Lambda(torch.flatten),\n",
    "            lambda x: (x*4).int(),\n",
    "            lambda x: x.float()/4,\n",
    "        ])\n",
    "    elif option == 2:\n",
    "        transform = transforms.Compose([\n",
    "            ToTensor(),\n",
    "            Lambda(torch.flatten),\n",
    "            lambda x: (x * 3).int().float(),\n",
    "        ])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    train_dataset = MNIST(root=mnist_path, download=False, transform=transform)\n",
    "    test_dataset = MNIST(root=mnist_path, train=False, transform=transform)\n",
    "    \n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "train_dataset, test_dataset = mnist_processing(option=input_bit_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset shape: torch.Size([60000, 28, 28])\n",
      "test_dataset shape: torch.Size([10000, 28, 28])\n",
      "torch.Size([784])\n",
      "Processed MNIST data unique values = tensor([0., 1., 2., 3.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x70f849d298b0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"train_dataset shape: {train_dataset.data.shape}\")\n",
    "print(f\"test_dataset shape: {test_dataset.data.shape}\")\n",
    "\n",
    "im = train_dataset[0][0]\n",
    "print(im.shape)\n",
    "print(f\"Processed MNIST data unique values = {im.unique()}\")\n",
    "plt.imshow(im.reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing training and validation data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(43)\n",
    "val_size = 5000\n",
    "train_size = len(train_dataset) - val_size\n",
    "train_ds, val_ds = random_split(train_dataset, [train_size, val_size])\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size * 2, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size * 2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get if device is GPU or CPU. Bring data onto the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "\n",
    "    def __init__(self, data_loader, device):\n",
    "        self.dl = data_loader\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "    \n",
    "device = get_default_device()\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "train_loader = DeviceDataLoader(train_loader, device)\n",
    "val_loader = DeviceDataLoader(val_loader, device)\n",
    "test_loader = DeviceDataLoader(test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = torch.Size([32, 784]), y.shape = torch.Size([32])\n",
      "x.unique() = tensor([0., 1., 2., 3.])\n",
      "y.unique() = tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(f\"{x.shape = }, {y.shape = }\")\n",
    "    print(f\"{x.unique() = }\")\n",
    "    print(f\"{y.unique() = }\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the FC Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    \"\"\"\n",
    "    PytorchLightining style\n",
    "    \"\"\"\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch) -> Dict:\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
    "        acc = accuracy(out, labels)  # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "\n",
    "    def validation_epoch_end(self, outputs) -> Dict:\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()  # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "\n",
    "    def epoch_end(self, epoch, result) -> None:\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch + 1, result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the QAT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTLinearQuantModel(\n",
       "  (fc1): QuantLinear(\n",
       "    in_features=784, out_features=10, bias=False\n",
       "    (input_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (output_quant): ActQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "    (weight_quant): WeightQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "      (tensor_quant): RescalingIntQuant(\n",
       "        (int_quant): IntQuant(\n",
       "          (float_to_int_impl): RoundSte()\n",
       "          (tensor_clamp_impl): TensorClampSte()\n",
       "          (delay_wrapper): DelayWrapper(\n",
       "            (delay_impl): _NoDelay()\n",
       "          )\n",
       "        )\n",
       "        (scaling_impl): StatsFromParameterScaling(\n",
       "          (parameter_list_stats): _ParameterListStats(\n",
       "            (first_tracked_param): _ViewParameterWrapper(\n",
       "              (view_shape_impl): OverTensorView()\n",
       "            )\n",
       "            (stats): _Stats(\n",
       "              (stats_impl): AbsMax()\n",
       "            )\n",
       "          )\n",
       "          (stats_scaling_impl): _StatsScaling(\n",
       "            (affine_rescaling): Identity()\n",
       "            (restrict_clamp_scaling): _RestrictClampValue(\n",
       "              (clamp_min_ste): ScalarClampMinSte()\n",
       "              (restrict_value_impl): FloatRestrictValue()\n",
       "            )\n",
       "            (restrict_scaling_pre): Identity()\n",
       "          )\n",
       "        )\n",
       "        (int_scaling_impl): IntScaling()\n",
       "        (zero_point_impl): ZeroZeroPoint(\n",
       "          (zero_point): StatelessBuffer()\n",
       "        )\n",
       "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
       "          (bit_width): StatelessBuffer()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (bias_quant): BiasQuantProxyFromInjector(\n",
       "      (_zero_hw_sentinel): StatelessBuffer()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_layers = 1\n",
    "\n",
    "if num_layers == 1:\n",
    "    class MNISTLinearQuantModel(ImageClassificationBase):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.fc1 = qnn.QuantLinear(in_features=28*28, out_features=10, bias=False, \n",
    "                                    weight_bit_width=weight_bit_width,\n",
    "                                    return_quant_tensor=True)\n",
    "\n",
    "        def forward(self, xb):\n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            out = self.fc1(xb)\n",
    "\n",
    "            return out\n",
    "\n",
    "elif num_layers == 2:\n",
    "    class MNISTLinearQuantModel(ImageClassificationBase):\n",
    "        \"\"\"\n",
    "        2 linear layers + 1 square activations\n",
    "        \"\"\"\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.fc1 = qnn.QuantLinear(in_features=28*28, out_features=128, bias=False, \n",
    "                                    weight_bit_width=weight_bit_width,\n",
    "                                    return_quant_tensor=True)\n",
    "            \n",
    "            self.fc2 = qnn.QuantLinear(in_features=128, out_features=10, bias=False, \n",
    "                                    weight_bit_width=weight_bit_width, \n",
    "                                    return_quant_tensor=True)\n",
    "\n",
    "        def forward(self, xb):\n",
    "            # import ipdb; ipdb.set_trace()\n",
    "            out = self.fc1(xb)\n",
    "            out = out * out  # first square\n",
    "            # out = out.reshape(out.shape[0], -1)\n",
    "            out = self.fc2(out)\n",
    "\n",
    "            return out\n",
    "else:\n",
    "    raise ValueError(\"num_layers not supported\")\n",
    "\n",
    "model = to_device(MNISTLinearQuantModel(), torch.device('cpu'))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader) -> Dict:\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "\n",
    "def fit(epochs, lr, model, \n",
    "        train_loader, val_loader, test_loader, \n",
    "        file_path, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    high_acc = 0.95\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "\n",
    "        if epoch >= 2:\n",
    "            eval_dict = evaluate(model, test_loader)\n",
    "            print(str(epoch) + \"\\t\" + str(eval_dict))\n",
    "            if eval_dict['val_acc'] > high_acc:\n",
    "                high_acc = eval_dict['val_acc']\n",
    "                torch.save(model.state_dict(), file_path)\n",
    "                print(f\"Saved into {file_path.relative_to(project_path)}\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dk/miniconda3/envs/syft/lib/python3.12/site-packages/torch/_tensor.py:1419: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1921.)\n",
      "  return super().rename(names)\n",
      "/tmp/ipykernel_132705/3874017255.py:2: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:294.)\n",
      "  _, preds = torch.max(outputs, dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], val_loss: 0.3799, val_acc: 0.8825\n",
      "Epoch [2], val_loss: 0.3975, val_acc: 0.8849\n",
      "Epoch [3], val_loss: 0.3700, val_acc: 0.8912\n",
      "2\t{'val_loss': 0.3395191729068756, 'val_acc': 0.902965784072876}\n",
      "Epoch [4], val_loss: 0.3895, val_acc: 0.8906\n",
      "3\t{'val_loss': 0.3802065849304199, 'val_acc': 0.8905254602432251}\n",
      "Epoch [5], val_loss: 0.3863, val_acc: 0.8910\n",
      "4\t{'val_loss': 0.3664878010749817, 'val_acc': 0.8991839289665222}\n",
      "Epoch [6], val_loss: 0.3929, val_acc: 0.8894\n",
      "5\t{'val_loss': 0.3606981635093689, 'val_acc': 0.8968949317932129}\n",
      "Epoch [7], val_loss: 0.4368, val_acc: 0.8811\n",
      "6\t{'val_loss': 0.4141297936439514, 'val_acc': 0.8851512670516968}\n",
      "Epoch [8], val_loss: 0.3921, val_acc: 0.8869\n",
      "7\t{'val_loss': 0.3566090166568756, 'val_acc': 0.9008758068084717}\n",
      "Epoch [9], val_loss: 0.3977, val_acc: 0.8954\n",
      "8\t{'val_loss': 0.3578961491584778, 'val_acc': 0.9045581221580505}\n",
      "Epoch [10], val_loss: 0.4656, val_acc: 0.8643\n",
      "9\t{'val_loss': 0.4245924651622772, 'val_acc': 0.8747014403343201}\n"
     ]
    }
   ],
   "source": [
    "history = [evaluate(model, val_loader)]\n",
    "history += fit(epochs=10, lr=0.001, model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader,\n",
    "            file_path=save_weight_path,\n",
    "            opt_func=torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final evaluation on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_131064/3874017255.py:2: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:294.)\n",
      "  _, preds = torch.max(outputs, dim=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': 0.39765068888664246, 'val_acc': 0.8913216590881348}\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  0 ...  0  0  0]\n",
      " [ 0 -1  1 ...  0  0  0]\n",
      " [-1  0  0 ...  0  0 -1]\n",
      " ...\n",
      " [-1  0  0 ...  0  0 -1]\n",
      " [-1  1  0 ...  1  1  0]\n",
      " [-1  0  0 ...  1  0  0]]\n"
     ]
    }
   ],
   "source": [
    "int_fc1 = np.array(model.fc1.quant_weight().int().tolist())\n",
    "print(int_fc1)\n",
    "\n",
    "if num_layers == 2:\n",
    "    int_fc2 = np.array(model.fc2.quant_weight().int().tolist())\n",
    "    print(int_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
